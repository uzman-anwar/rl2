[32;1mConfigured folder ./icrl/wandb/run-20201217_092415-173sfsh3/files for saving[0m
[32;1mName: HCWithPos-v0_tk_0.01_s_27_sid_0[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/linuxubuntu/Desktop/shehryar-usman/rl_codebase/.rl/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
----------------------------------
| eval/               |          |
|    best_mean_reward | 125      |
|    mean_ep_length   | 422      |
|    mean_reward      | 125      |
| infos/              |          |
|    cost             | 0        |
|    reward_ctrl      | -0.308   |
|    reward_run       | 0.563    |
| rollout/            |          |
|    adjusted_reward  | 0.309    |
|    ep_len_mean      | 500      |
|    ep_rew_mean      | 138      |
| time/               |          |
|    fps              | 1677     |
|    iterations       | 1        |
|    time_elapsed     | 6        |
|    total_timesteps  | 10240    |
----------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 125         |
|    mean_ep_length       | 339         |
|    mean_reward          | 107         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.301      |
|    reward_run           | 0.65        |
| rollout/                |             |
|    adjusted_reward      | 0.306       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 144         |
| time/                   |             |
|    fps                  | 1687        |
|    iterations           | 2           |
|    time_elapsed         | 12          |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.017028455 |
|    average_cost         | 0.014934244 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -19.7       |
|    cost_value_loss      | 5.58        |
|    entropy_loss         | -8.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 10          |
|    nu                   | 1.06        |
|    nu_loss              | -0.0149     |
|    policy_gradient_loss | -0.00774    |
|    reward_explained_... | -0.867      |
|    reward_value_loss    | 0.249       |
|    std                  | 0.998       |
|    total_cost           | 152.92665   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 125          |
|    mean_ep_length       | 381          |
|    mean_reward          | 123          |
| infos/                  |              |
|    cost                 | 0.00135      |
|    reward_ctrl          | -0.307       |
|    reward_run           | 0.828        |
| rollout/                |              |
|    adjusted_reward      | 0.317        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 147          |
| time/                   |              |
|    fps                  | 1335         |
|    iterations           | 3            |
|    time_elapsed         | 23           |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.011372453  |
|    average_cost         | 0.0066512814 |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.742        |
|    cost_value_loss      | 0.0448       |
|    entropy_loss         | -8.49        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0608       |
|    n_updates            | 20           |
|    nu                   | 1.13         |
|    nu_loss              | -0.00708     |
|    policy_gradient_loss | -0.00595     |
|    reward_explained_... | -5.58        |
|    reward_value_loss    | 0.0743       |
|    std                  | 0.993        |
|    total_cost           | 68.10912     |
------------------------------------------
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 156         |
|    mean_ep_length       | 430         |
|    mean_reward          | 156         |
| infos/                  |             |
|    cost                 | 0.0177      |
|    reward_ctrl          | -0.303      |
|    reward_run           | 0.576       |
| rollout/                |             |
|    adjusted_reward      | 0.344       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 153         |
| time/                   |             |
|    fps                  | 1196        |
|    iterations           | 4           |
|    time_elapsed         | 34          |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.014430866 |
|    average_cost         | 0.005977829 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.884       |
|    cost_value_loss      | 0.0346      |
|    entropy_loss         | -8.44       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0377      |
|    n_updates            | 30          |
|    nu                   | 1.19        |
|    nu_loss              | -0.00673    |
|    policy_gradient_loss | -0.00597    |
|    reward_explained_... | -2.76       |
|    reward_value_loss    | 0.0864      |
|    std                  | 0.986       |
|    total_cost           | 61.212967   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 156          |
|    mean_ep_length       | 383          |
|    mean_reward          | 122          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.3         |
|    reward_run           | 0.713        |
| rollout/                |              |
|    adjusted_reward      | 0.366        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 159          |
| time/                   |              |
|    fps                  | 1128         |
|    iterations           | 5            |
|    time_elapsed         | 45           |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 0.014085245  |
|    average_cost         | 0.0048189396 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.843        |
|    cost_value_loss      | 0.0322       |
|    entropy_loss         | -8.4         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0505       |
|    n_updates            | 40           |
|    nu                   | 1.25         |
|    nu_loss              | -0.00572     |
|    policy_gradient_loss | -0.0061      |
|    reward_explained_... | -0.33        |
|    reward_value_loss    | 0.088        |
|    std                  | 0.98         |
|    total_cost           | 49.345943    |
------------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 156         |
|    mean_ep_length       | 283         |
|    mean_reward          | 111         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.315      |
|    reward_run           | 0.761       |
| rollout/                |             |
|    adjusted_reward      | 0.398       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 172         |
| time/                   |             |
|    fps                  | 1101        |
|    iterations           | 6           |
|    time_elapsed         | 55          |
|    total_timesteps      | 61440       |
| train/                  |             |
|    approx_kl            | 0.011775821 |
|    average_cost         | 0.007507113 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.879       |
|    cost_value_loss      | 0.0548      |
|    entropy_loss         | -8.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0537      |
|    n_updates            | 50          |
|    nu                   | 1.31        |
|    nu_loss              | -0.00937    |
|    policy_gradient_loss | -0.00556    |
|    reward_explained_... | 0.0866      |
|    reward_value_loss    | 0.0902      |
|    std                  | 0.976       |
|    total_cost           | 76.87283    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 162         |
|    mean_ep_length       | 384         |
|    mean_reward          | 162         |
| infos/                  |             |
|    cost                 | 0.00899     |
|    reward_ctrl          | -0.297      |
|    reward_run           | 0.635       |
| rollout/                |             |
|    adjusted_reward      | 0.416       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 184         |
| time/                   |             |
|    fps                  | 1078        |
|    iterations           | 7           |
|    time_elapsed         | 66          |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.011690436 |
|    average_cost         | 0.007722267 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.89        |
|    cost_value_loss      | 0.0564      |
|    entropy_loss         | -8.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0594      |
|    n_updates            | 60          |
|    nu                   | 1.38        |
|    nu_loss              | -0.0101     |
|    policy_gradient_loss | -0.00628    |
|    reward_explained_... | 0.0664      |
|    reward_value_loss    | 0.103       |
|    std                  | 0.972       |
|    total_cost           | 79.07601    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 194          |
|    mean_ep_length       | 384          |
|    mean_reward          | 194          |
| infos/                  |              |
|    cost                 | 0.017        |
|    reward_ctrl          | -0.296       |
|    reward_run           | 0.885        |
| rollout/                |              |
|    adjusted_reward      | 0.457        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 200          |
| time/                   |              |
|    fps                  | 1055         |
|    iterations           | 8            |
|    time_elapsed         | 77           |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.013543919  |
|    average_cost         | 0.0042703683 |
|    clip_fraction        | 0.157        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.9          |
|    cost_value_loss      | 0.051        |
|    entropy_loss         | -8.28        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0627       |
|    n_updates            | 70           |
|    nu                   | 1.45         |
|    nu_loss              | -0.0059      |
|    policy_gradient_loss | -0.00696     |
|    reward_explained_... | 0.302        |
|    reward_value_loss    | 0.104        |
|    std                  | 0.96         |
|    total_cost           | 43.72857     |
------------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 194         |
|    mean_ep_length       | 384         |
|    mean_reward          | 188         |
| infos/                  |             |
|    cost                 | 0.00835     |
|    reward_ctrl          | -0.307      |
|    reward_run           | 0.743       |
| rollout/                |             |
|    adjusted_reward      | 0.472       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 212         |
| time/                   |             |
|    fps                  | 1041        |
|    iterations           | 9           |
|    time_elapsed         | 88          |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.009434281 |
|    average_cost         | 0.010238285 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.898       |
|    cost_value_loss      | 0.0818      |
|    entropy_loss         | -8.23       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0708      |
|    n_updates            | 80          |
|    nu                   | 1.52        |
|    nu_loss              | -0.0148     |
|    policy_gradient_loss | -0.00578    |
|    reward_explained_... | 0.369       |
|    reward_value_loss    | 0.127       |
|    std                  | 0.953       |
|    total_cost           | 104.84004   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 201         |
|    mean_ep_length       | 383         |
|    mean_reward          | 201         |
| infos/                  |             |
|    cost                 | 0.00834     |
|    reward_ctrl          | -0.311      |
|    reward_run           | 0.783       |
| rollout/                |             |
|    adjusted_reward      | 0.527       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 227         |
| time/                   |             |
|    fps                  | 1027        |
|    iterations           | 10          |
|    time_elapsed         | 99          |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.011990418 |
|    average_cost         | 0.006695767 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.911       |
|    cost_value_loss      | 0.0401      |
|    entropy_loss         | -8.17       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0708      |
|    n_updates            | 90          |
|    nu                   | 1.59        |
|    nu_loss              | -0.0102     |
|    policy_gradient_loss | -0.00649    |
|    reward_explained_... | 0.411       |
|    reward_value_loss    | 0.11        |
|    std                  | 0.943       |
|    total_cost           | 68.56465    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 201         |
|    mean_ep_length       | 342         |
|    mean_reward          | 183         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.318      |
|    reward_run           | 0.924       |
| rollout/                |             |
|    adjusted_reward      | 0.527       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 242         |
| time/                   |             |
|    fps                  | 1020        |
|    iterations           | 11          |
|    time_elapsed         | 110         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.011625718 |
|    average_cost         | 0.005936905 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.916       |
|    cost_value_loss      | 0.0591      |
|    entropy_loss         | -8.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 100         |
|    nu                   | 1.67        |
|    nu_loss              | -0.00946    |
|    policy_gradient_loss | -0.00582    |
|    reward_explained_... | 0.242       |
|    reward_value_loss    | 0.129       |
|    std                  | 0.938       |
|    total_cost           | 60.793907   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 227          |
|    mean_ep_length       | 402          |
|    mean_reward          | 227          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.318       |
|    reward_run           | 0.876        |
| rollout/                |              |
|    adjusted_reward      | 0.578        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 257          |
| time/                   |              |
|    fps                  | 1009         |
|    iterations           | 12           |
|    time_elapsed         | 121          |
|    total_timesteps      | 122880       |
| train/                  |              |
|    approx_kl            | 0.011182997  |
|    average_cost         | 0.0062578255 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.883        |
|    cost_value_loss      | 0.0682       |
|    entropy_loss         | -8.1         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0641       |
|    n_updates            | 110          |
|    nu                   | 1.75         |
|    nu_loss              | -0.0104      |
|    policy_gradient_loss | -0.00515     |
|    reward_explained_... | 0.442        |
|    reward_value_loss    | 0.15         |
|    std                  | 0.933        |
|    total_cost           | 64.08013     |
------------------------------------------
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 264         |
|    mean_ep_length       | 426         |
|    mean_reward          | 264         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.314      |
|    reward_run           | 0.978       |
| rollout/                |             |
|    adjusted_reward      | 0.587       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 270         |
| time/                   |             |
|    fps                  | 1002        |
|    iterations           | 13          |
|    time_elapsed         | 132         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.014591925 |
|    average_cost         | 0.008506032 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.917       |
|    cost_value_loss      | 0.071       |
|    entropy_loss         | -8.05       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0721      |
|    n_updates            | 120         |
|    nu                   | 1.83        |
|    nu_loss              | -0.0148     |
|    policy_gradient_loss | -0.00602    |
|    reward_explained_... | 0.455       |
|    reward_value_loss    | 0.121       |
|    std                  | 0.924       |
|    total_cost           | 87.10177    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 264         |
|    mean_ep_length       | 395         |
|    mean_reward          | 236         |
| infos/                  |             |
|    cost                 | 0.00807     |
|    reward_ctrl          | -0.294      |
|    reward_run           | 1.02        |
| rollout/                |             |
|    adjusted_reward      | 0.618       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 284         |
| time/                   |             |
|    fps                  | 998         |
|    iterations           | 14          |
|    time_elapsed         | 143         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.010071134 |
|    average_cost         | 0.009046687 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.928       |
|    cost_value_loss      | 0.0592      |
|    entropy_loss         | -7.99       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0503      |
|    n_updates            | 130         |
|    nu                   | 1.91        |
|    nu_loss              | -0.0165     |
|    policy_gradient_loss | -0.00545    |
|    reward_explained_... | 0.59        |
|    reward_value_loss    | 0.113       |
|    std                  | 0.915       |
|    total_cost           | 92.63808    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 264         |
|    mean_ep_length       | 378         |
|    mean_reward          | 226         |
| infos/                  |             |
|    cost                 | 0.014       |
|    reward_ctrl          | -0.316      |
|    reward_run           | 1.11        |
| rollout/                |             |
|    adjusted_reward      | 0.646       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 296         |
| time/                   |             |
|    fps                  | 993         |
|    iterations           | 15          |
|    time_elapsed         | 154         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.01253857  |
|    average_cost         | 0.006155522 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.927       |
|    cost_value_loss      | 0.0481      |
|    entropy_loss         | -7.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0945      |
|    n_updates            | 140         |
|    nu                   | 1.99        |
|    nu_loss              | -0.0117     |
|    policy_gradient_loss | -0.00586    |
|    reward_explained_... | 0.398       |
|    reward_value_loss    | 0.145       |
|    std                  | 0.905       |
|    total_cost           | 63.032547   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 264         |
|    mean_ep_length       | 371         |
|    mean_reward          | 256         |
| infos/                  |             |
|    cost                 | 0.00787     |
|    reward_ctrl          | -0.315      |
|    reward_run           | 1.17        |
| rollout/                |             |
|    adjusted_reward      | 0.688       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 312         |
| time/                   |             |
|    fps                  | 989         |
|    iterations           | 16          |
|    time_elapsed         | 165         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.010258427 |
|    average_cost         | 0.011679655 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.935       |
|    cost_value_loss      | 0.0779      |
|    entropy_loss         | -7.86       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 150         |
|    nu                   | 2.08        |
|    nu_loss              | -0.0233     |
|    policy_gradient_loss | -0.00566    |
|    reward_explained_... | 0.399       |
|    reward_value_loss    | 0.145       |
|    std                  | 0.895       |
|    total_cost           | 119.59966   |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 282         |
|    mean_ep_length       | 404         |
|    mean_reward          | 282         |
| infos/                  |             |
|    cost                 | 0.0138      |
|    reward_ctrl          | -0.305      |
|    reward_run           | 1.18        |
| rollout/                |             |
|    adjusted_reward      | 0.704       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 326         |
| time/                   |             |
|    fps                  | 999         |
|    iterations           | 17          |
|    time_elapsed         | 174         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.015016735 |
|    average_cost         | 0.005737017 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.903       |
|    cost_value_loss      | 0.0812      |
|    entropy_loss         | -7.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 160         |
|    nu                   | 2.17        |
|    nu_loss              | -0.0119     |
|    policy_gradient_loss | -0.0037     |
|    reward_explained_... | 0.187       |
|    reward_value_loss    | 0.176       |
|    std                  | 0.89        |
|    total_cost           | 58.74705    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 282         |
|    mean_ep_length       | 331         |
|    mean_reward          | 247         |
| infos/                  |             |
|    cost                 | 0.00776     |
|    reward_ctrl          | -0.31       |
|    reward_run           | 1.23        |
| rollout/                |             |
|    adjusted_reward      | 0.735       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 340         |
| time/                   |             |
|    fps                  | 1007        |
|    iterations           | 18          |
|    time_elapsed         | 182         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.014398785 |
|    average_cost         | 0.006435004 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.898       |
|    cost_value_loss      | 0.0757      |
|    entropy_loss         | -7.79       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0742      |
|    n_updates            | 170         |
|    nu                   | 2.25        |
|    nu_loss              | -0.0139     |
|    policy_gradient_loss | -0.00532    |
|    reward_explained_... | 0.399       |
|    reward_value_loss    | 0.137       |
|    std                  | 0.885       |
|    total_cost           | 65.89444    |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 282         |
|    mean_ep_length       | 286         |
|    mean_reward          | 218         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.313      |
|    reward_run           | 1.11        |
| rollout/                |             |
|    adjusted_reward      | 0.769       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 355         |
| time/                   |             |
|    fps                  | 1008        |
|    iterations           | 19          |
|    time_elapsed         | 192         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.012772923 |
|    average_cost         | 0.009505394 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.917       |
|    cost_value_loss      | 0.0607      |
|    entropy_loss         | -7.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0769      |
|    n_updates            | 180         |
|    nu                   | 2.34        |
|    nu_loss              | -0.0214     |
|    policy_gradient_loss | -0.0049     |
|    reward_explained_... | 0.5         |
|    reward_value_loss    | 0.144       |
|    std                  | 0.879       |
|    total_cost           | 97.335236   |
-----------------------------------------
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 282          |
|    mean_ep_length       | 321          |
|    mean_reward          | 249          |
| infos/                  |              |
|    cost                 | 0.023        |
|    reward_ctrl          | -0.319       |
|    reward_run           | 0.93         |
| rollout/                |              |
|    adjusted_reward      | 0.828        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 370          |
| time/                   |              |
|    fps                  | 1007         |
|    iterations           | 20           |
|    time_elapsed         | 203          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.014855024  |
|    average_cost         | 0.0077161207 |
|    clip_fraction        | 0.163        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.919        |
|    cost_value_loss      | 0.0557       |
|    entropy_loss         | -7.7         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.145        |
|    n_updates            | 190          |
|    nu                   | 2.44         |
|    nu_loss              | -0.0181      |
|    policy_gradient_loss | -0.00468     |
|    reward_explained_... | 0.559        |
|    reward_value_loss    | 0.143        |
|    std                  | 0.872        |
|    total_cost           | 79.01308     |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 282          |
|    mean_ep_length       | 345          |
|    mean_reward          | 274          |
| infos/                  |              |
|    cost                 | 0.00457      |
|    reward_ctrl          | -0.301       |
|    reward_run           | 0.943        |
| rollout/                |              |
|    adjusted_reward      | 0.817        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 388          |
| time/                   |              |
|    fps                  | 1015         |
|    iterations           | 21           |
|    time_elapsed         | 211          |
|    total_timesteps      | 215040       |
| train/                  |              |
|    approx_kl            | 0.015305085  |
|    average_cost         | 0.0071402444 |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.923        |
|    cost_value_loss      | 0.0397       |
|    entropy_loss         | -7.64        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0307       |
|    n_updates            | 200          |
|    nu                   | 2.53         |
|    nu_loss              | -0.0174      |
|    policy_gradient_loss | -0.00365     |
|    reward_explained_... | 0.683        |
|    reward_value_loss    | 0.143        |
|    std                  | 0.863        |
|    total_cost           | 73.116104    |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 331         |
|    mean_ep_length       | 382         |
|    mean_reward          | 331         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.315      |
|    reward_run           | 0.934       |
| rollout/                |             |
|    adjusted_reward      | 0.852       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 403         |
| time/                   |             |
|    fps                  | 1024        |
|    iterations           | 22          |
|    time_elapsed         | 219         |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.015309605 |
|    average_cost         | 0.00824943  |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.896       |
|    cost_value_loss      | 0.0791      |
|    entropy_loss         | -7.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0818      |
|    n_updates            | 210         |
|    nu                   | 2.62        |
|    nu_loss              | -0.0209     |
|    policy_gradient_loss | -0.00432    |
|    reward_explained_... | 0.46        |
|    reward_value_loss    | 0.159       |
|    std                  | 0.854       |
|    total_cost           | 84.47416    |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 331          |
|    mean_ep_length       | 326          |
|    mean_reward          | 275          |
| infos/                  |              |
|    cost                 | 0.00763      |
|    reward_ctrl          | -0.329       |
|    reward_run           | 1.14         |
| rollout/                |              |
|    adjusted_reward      | 0.864        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 415          |
| time/                   |              |
|    fps                  | 1036         |
|    iterations           | 23           |
|    time_elapsed         | 227          |
|    total_timesteps      | 235520       |
| train/                  |              |
|    approx_kl            | 0.015388554  |
|    average_cost         | 0.0074457587 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.886        |
|    cost_value_loss      | 0.0771       |
|    entropy_loss         | -7.52        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0464       |
|    n_updates            | 220          |
|    nu                   | 2.72         |
|    nu_loss              | -0.0195      |
|    policy_gradient_loss | -0.00225     |
|    reward_explained_... | 0.663        |
|    reward_value_loss    | 0.119        |
|    std                  | 0.845        |
|    total_cost           | 76.24457     |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 331         |
|    mean_ep_length       | 328         |
|    mean_reward          | 287         |
| infos/                  |             |
|    cost                 | 0.00765     |
|    reward_ctrl          | -0.328      |
|    reward_run           | 1.3         |
| rollout/                |             |
|    adjusted_reward      | 0.872       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 426         |
| time/                   |             |
|    fps                  | 1043        |
|    iterations           | 24          |
|    time_elapsed         | 235         |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.015546469 |
|    average_cost         | 0.007492388 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.903       |
|    cost_value_loss      | 0.0441      |
|    entropy_loss         | -7.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0373      |
|    n_updates            | 230         |
|    nu                   | 2.82        |
|    nu_loss              | -0.0204     |
|    policy_gradient_loss | -0.003      |
|    reward_explained_... | 0.766       |
|    reward_value_loss    | 0.133       |
|    std                  | 0.841       |
|    total_cost           | 76.72205    |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 331         |
|    mean_ep_length       | 305         |
|    mean_reward          | 280         |
| infos/                  |             |
|    cost                 | 0.00765     |
|    reward_ctrl          | -0.346      |
|    reward_run           | 1.34        |
| rollout/                |             |
|    adjusted_reward      | 0.896       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 434         |
| time/                   |             |
|    fps                  | 1045        |
|    iterations           | 25          |
|    time_elapsed         | 244         |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.01744293  |
|    average_cost         | 0.006336519 |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.897       |
|    cost_value_loss      | 0.0592      |
|    entropy_loss         | -7.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0754      |
|    n_updates            | 240         |
|    nu                   | 2.91        |
|    nu_loss              | -0.0178     |
|    policy_gradient_loss | -0.00444    |
|    reward_explained_... | 0.43        |
|    reward_value_loss    | 0.149       |
|    std                  | 0.832       |
|    total_cost           | 64.885956   |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 331         |
|    mean_ep_length       | 217         |
|    mean_reward          | 186         |
| infos/                  |             |
|    cost                 | 0.00768     |
|    reward_ctrl          | -0.322      |
|    reward_run           | 0.956       |
| rollout/                |             |
|    adjusted_reward      | 0.908       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 442         |
| time/                   |             |
|    fps                  | 1053        |
|    iterations           | 26          |
|    time_elapsed         | 252         |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.01692214  |
|    average_cost         | 0.006631124 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.912       |
|    cost_value_loss      | 0.0483      |
|    entropy_loss         | -7.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 250         |
|    nu                   | 3.01        |
|    nu_loss              | -0.0193     |
|    policy_gradient_loss | -0.00403    |
|    reward_explained_... | 0.404       |
|    reward_value_loss    | 0.165       |
|    std                  | 0.826       |
|    total_cost           | 67.90271    |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 331          |
|    mean_ep_length       | 312          |
|    mean_reward          | 297          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.346       |
|    reward_run           | 1.37         |
| rollout/                |              |
|    adjusted_reward      | 0.907        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 449          |
| time/                   |              |
|    fps                  | 1071         |
|    iterations           | 27           |
|    time_elapsed         | 258          |
|    total_timesteps      | 276480       |
| train/                  |              |
|    approx_kl            | 0.01570015   |
|    average_cost         | 0.0047358004 |
|    clip_fraction        | 0.17         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.919        |
|    cost_value_loss      | 0.0359       |
|    entropy_loss         | -7.35        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.116        |
|    n_updates            | 260          |
|    nu                   | 3.1          |
|    nu_loss              | -0.0143      |
|    policy_gradient_loss | -0.00195     |
|    reward_explained_... | 0.405        |
|    reward_value_loss    | 0.167        |
|    std                  | 0.824        |
|    total_cost           | 48.4946      |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 418         |
|    mean_ep_length       | 437         |
|    mean_reward          | 418         |
| infos/                  |             |
|    cost                 | 0.00615     |
|    reward_ctrl          | -0.344      |
|    reward_run           | 1.28        |
| rollout/                |             |
|    adjusted_reward      | 0.92        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 455         |
| time/                   |             |
|    fps                  | 1074        |
|    iterations           | 28          |
|    time_elapsed         | 266         |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.015509871 |
|    average_cost         | 0.009034017 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.919       |
|    cost_value_loss      | 0.0709      |
|    entropy_loss         | -7.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 270         |
|    nu                   | 3.2         |
|    nu_loss              | -0.028      |
|    policy_gradient_loss | -0.00263    |
|    reward_explained_... | 0.183       |
|    reward_value_loss    | 0.15        |
|    std                  | 0.82        |
|    total_cost           | 92.50833    |
-----------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 418         |
|    mean_ep_length       | 430         |
|    mean_reward          | 387         |
| infos/                  |             |
|    cost                 | 0.0231      |
|    reward_ctrl          | -0.315      |
|    reward_run           | 1.33        |
| rollout/                |             |
|    adjusted_reward      | 0.944       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 462         |
| time/                   |             |
|    fps                  | 1070        |
|    iterations           | 29          |
|    time_elapsed         | 277         |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.01608982  |
|    average_cost         | 0.004360474 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.918       |
|    cost_value_loss      | 0.036       |
|    entropy_loss         | -7.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0604      |
|    n_updates            | 280         |
|    nu                   | 3.3         |
|    nu_loss              | -0.014      |
|    policy_gradient_loss | -0.00433    |
|    reward_explained_... | 0.564       |
|    reward_value_loss    | 0.12        |
|    std                  | 0.815       |
|    total_cost           | 44.651257   |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 418         |
|    mean_ep_length       | 271         |
|    mean_reward          | 247         |
| infos/                  |             |
|    cost                 | 0.0155      |
|    reward_ctrl          | -0.336      |
|    reward_run           | 1.24        |
| rollout/                |             |
|    adjusted_reward      | 0.949       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 469         |
| time/                   |             |
|    fps                  | 1074        |
|    iterations           | 30          |
|    time_elapsed         | 285         |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.019763622 |
|    average_cost         | 0.006332466 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.931       |
|    cost_value_loss      | 0.0457      |
|    entropy_loss         | -7.27       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0626      |
|    n_updates            | 290         |
|    nu                   | 3.39        |
|    nu_loss              | -0.0209     |
|    policy_gradient_loss | -0.00312    |
|    reward_explained_... | 0.251       |
|    reward_value_loss    | 0.14        |
|    std                  | 0.813       |
|    total_cost           | 64.84445    |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 418          |
|    mean_ep_length       | 387          |
|    mean_reward          | 346          |
| infos/                  |              |
|    cost                 | 0.00776      |
|    reward_ctrl          | -0.34        |
|    reward_run           | 1.19         |
| rollout/                |              |
|    adjusted_reward      | 0.939        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 473          |
| time/                   |              |
|    fps                  | 1083         |
|    iterations           | 31           |
|    time_elapsed         | 292          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.016938813  |
|    average_cost         | 0.0065527125 |
|    clip_fraction        | 0.174        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.92         |
|    cost_value_loss      | 0.0508       |
|    entropy_loss         | -7.26        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0712       |
|    n_updates            | 300          |
|    nu                   | 3.49         |
|    nu_loss              | -0.0222      |
|    policy_gradient_loss | -0.00167     |
|    reward_explained_... | 0.68         |
|    reward_value_loss    | 0.106        |
|    std                  | 0.814        |
|    total_cost           | 67.09978     |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 418          |
|    mean_ep_length       | 363          |
|    mean_reward          | 327          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.349       |
|    reward_run           | 1.67         |
| rollout/                |              |
|    adjusted_reward      | 0.979        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 478          |
| time/                   |              |
|    fps                  | 1092         |
|    iterations           | 32           |
|    time_elapsed         | 299          |
|    total_timesteps      | 327680       |
| train/                  |              |
|    approx_kl            | 0.015644094  |
|    average_cost         | 0.0043691685 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.913        |
|    cost_value_loss      | 0.0472       |
|    entropy_loss         | -7.25        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.169        |
|    n_updates            | 310          |
|    nu                   | 3.58         |
|    nu_loss              | -0.0152      |
|    policy_gradient_loss | -0.00145     |
|    reward_explained_... | 0.517        |
|    reward_value_loss    | 0.13         |
|    std                  | 0.81         |
|    total_cost           | 44.740288    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 467          |
|    mean_ep_length       | 500          |
|    mean_reward          | 467          |
| infos/                  |              |
|    cost                 | 0.000779     |
|    reward_ctrl          | -0.335       |
|    reward_run           | 1.4          |
| rollout/                |              |
|    adjusted_reward      | 0.953        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 483          |
| time/                   |              |
|    fps                  | 1096         |
|    iterations           | 33           |
|    time_elapsed         | 308          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.015438059  |
|    average_cost         | 0.0029291087 |
|    clip_fraction        | 0.156        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.878        |
|    cost_value_loss      | 0.0397       |
|    entropy_loss         | -7.21        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0784       |
|    n_updates            | 320          |
|    nu                   | 3.67         |
|    nu_loss              | -0.0105      |
|    policy_gradient_loss | -0.00115     |
|    reward_explained_... | 0.332        |
|    reward_value_loss    | 0.152        |
|    std                  | 0.803        |
|    total_cost           | 29.994074    |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 467          |
|    mean_ep_length       | 413          |
|    mean_reward          | 407          |
| infos/                  |              |
|    cost                 | 0.00777      |
|    reward_ctrl          | -0.317       |
|    reward_run           | 1.39         |
| rollout/                |              |
|    adjusted_reward      | 0.961        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 484          |
| time/                   |              |
|    fps                  | 1098         |
|    iterations           | 34           |
|    time_elapsed         | 316          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.01594911   |
|    average_cost         | 0.0072156424 |
|    clip_fraction        | 0.198        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.921        |
|    cost_value_loss      | 0.0417       |
|    entropy_loss         | -7.15        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0999       |
|    n_updates            | 330          |
|    nu                   | 3.76         |
|    nu_loss              | -0.0265      |
|    policy_gradient_loss | -0.00277     |
|    reward_explained_... | 0.692        |
|    reward_value_loss    | 0.141        |
|    std                  | 0.794        |
|    total_cost           | 73.888176    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 467          |
|    mean_ep_length       | 376          |
|    mean_reward          | 367          |
| infos/                  |              |
|    cost                 | 0.0156       |
|    reward_ctrl          | -0.342       |
|    reward_run           | 1.54         |
| rollout/                |              |
|    adjusted_reward      | 0.95         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 486          |
| time/                   |              |
|    fps                  | 1105         |
|    iterations           | 35           |
|    time_elapsed         | 324          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.015885523  |
|    average_cost         | 0.0071690967 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.931        |
|    cost_value_loss      | 0.0416       |
|    entropy_loss         | -7.13        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0943       |
|    n_updates            | 340          |
|    nu                   | 3.85         |
|    nu_loss              | -0.027       |
|    policy_gradient_loss | -0.00177     |
|    reward_explained_... | 0.617        |
|    reward_value_loss    | 0.124        |
|    std                  | 0.796        |
|    total_cost           | 73.41155     |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 467          |
|    mean_ep_length       | 448          |
|    mean_reward          | 433          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.345       |
|    reward_run           | 1.39         |
| rollout/                |              |
|    adjusted_reward      | 0.967        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 490          |
| time/                   |              |
|    fps                  | 1104         |
|    iterations           | 36           |
|    time_elapsed         | 333          |
|    total_timesteps      | 368640       |
| train/                  |              |
|    approx_kl            | 0.018622722  |
|    average_cost         | 0.0049804784 |
|    clip_fraction        | 0.222        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.936        |
|    cost_value_loss      | 0.0338       |
|    entropy_loss         | -7.13        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.141        |
|    n_updates            | 350          |
|    nu                   | 3.95         |
|    nu_loss              | -0.0192      |
|    policy_gradient_loss | -0.00326     |
|    reward_explained_... | 0.264        |
|    reward_value_loss    | 0.144        |
|    std                  | 0.795        |
|    total_cost           | 51.0001      |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 467          |
|    mean_ep_length       | 405          |
|    mean_reward          | 389          |
| infos/                  |              |
|    cost                 | 0.00901      |
|    reward_ctrl          | -0.349       |
|    reward_run           | 1.54         |
| rollout/                |              |
|    adjusted_reward      | 0.988        |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 490          |
| time/                   |              |
|    fps                  | 1104         |
|    iterations           | 37           |
|    time_elapsed         | 342          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.016889853  |
|    average_cost         | 0.0064069135 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.933        |
|    cost_value_loss      | 0.0291       |
|    entropy_loss         | -7.09        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0716       |
|    n_updates            | 360          |
|    nu                   | 4.04         |
|    nu_loss              | -0.0253      |
|    policy_gradient_loss | -0.00271     |
|    reward_explained_... | 0.645        |
|    reward_value_loss    | 0.125        |
|    std                  | 0.787        |
|    total_cost           | 65.6068      |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 498         |
|    mean_ep_length       | 500         |
|    mean_reward          | 498         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.33       |
|    reward_run           | 1.42        |
| rollout/                |             |
|    adjusted_reward      | 0.983       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 493         |
| time/                   |             |
|    fps                  | 1108        |
|    iterations           | 38          |
|    time_elapsed         | 350         |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.015212627 |
|    average_cost         | 0.001703351 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.872       |
|    cost_value_loss      | 0.033       |
|    entropy_loss         | -7.06       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0817      |
|    n_updates            | 370         |
|    nu                   | 4.13        |
|    nu_loss              | -0.00688    |
|    policy_gradient_loss | -0.00128    |
|    reward_explained_... | 0.139       |
|    reward_value_loss    | 0.141       |
|    std                  | 0.785       |
|    total_cost           | 17.442314   |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 498         |
|    mean_ep_length       | 444         |
|    mean_reward          | 451         |
| infos/                  |             |
|    cost                 | 0.0079      |
|    reward_ctrl          | -0.364      |
|    reward_run           | 1.12        |
| rollout/                |             |
|    adjusted_reward      | 0.991       |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 495         |
| time/                   |             |
|    fps                  | 1114        |
|    iterations           | 39          |
|    time_elapsed         | 358         |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.017262533 |
|    average_cost         | 0.005926384 |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.91        |
|    cost_value_loss      | 0.0466      |
|    entropy_loss         | -7.05       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0819      |
|    n_updates            | 380         |
|    nu                   | 4.22        |
|    nu_loss              | -0.0245     |
|    policy_gradient_loss | -0.000554   |
|    reward_explained_... | 0.491       |
|    reward_value_loss    | 0.134       |
|    std                  | 0.785       |
|    total_cost           | 60.686172   |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 503          |
|    mean_ep_length       | 500          |
|    mean_reward          | 503          |
| infos/                  |              |
|    cost                 | 0.0158       |
|    reward_ctrl          | -0.351       |
|    reward_run           | 1.44         |
| rollout/                |              |
|    adjusted_reward      | 1            |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 499          |
| time/                   |              |
|    fps                  | 1122         |
|    iterations           | 40           |
|    time_elapsed         | 364          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.016233545  |
|    average_cost         | 0.0009791828 |
|    clip_fraction        | 0.181        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.95         |
|    cost_value_loss      | 0.00589      |
|    entropy_loss         | -7.07        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0127       |
|    n_updates            | 390          |
|    nu                   | 4.3          |
|    nu_loss              | -0.00413     |
|    policy_gradient_loss | -0.00203     |
|    reward_explained_... | 0.113        |
|    reward_value_loss    | 0.142        |
|    std                  | 0.788        |
|    total_cost           | 10.026832    |
------------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 503          |
|    mean_ep_length       | 439          |
|    mean_reward          | 469          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.35        |
|    reward_run           | 1.85         |
| rollout/                |              |
|    adjusted_reward      | 1.03         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 503          |
| time/                   |              |
|    fps                  | 1122         |
|    iterations           | 41           |
|    time_elapsed         | 374          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.016200662  |
|    average_cost         | 0.0043255775 |
|    clip_fraction        | 0.226        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.938        |
|    cost_value_loss      | 0.0287       |
|    entropy_loss         | -7.03        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0283       |
|    n_updates            | 400          |
|    nu                   | 4.38         |
|    nu_loss              | -0.0186      |
|    policy_gradient_loss | -0.00349     |
|    reward_explained_... | 0.256        |
|    reward_value_loss    | 0.129        |
|    std                  | 0.781        |
|    total_cost           | 44.293915    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 505          |
|    mean_ep_length       | 468          |
|    mean_reward          | 505          |
| infos/                  |              |
|    cost                 | 0.0016       |
|    reward_ctrl          | -0.326       |
|    reward_run           | 1.11         |
| rollout/                |              |
|    adjusted_reward      | 1.03         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 510          |
| time/                   |              |
|    fps                  | 1132         |
|    iterations           | 42           |
|    time_elapsed         | 379          |
|    total_timesteps      | 430080       |
| train/                  |              |
|    approx_kl            | 0.015810315  |
|    average_cost         | 0.0039413674 |
|    clip_fraction        | 0.168        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.927        |
|    cost_value_loss      | 0.0306       |
|    entropy_loss         | -7.01        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0797       |
|    n_updates            | 410          |
|    nu                   | 4.46         |
|    nu_loss              | -0.0173      |
|    policy_gradient_loss | -0.00114     |
|    reward_explained_... | 0.327        |
|    reward_value_loss    | 0.118        |
|    std                  | 0.78         |
|    total_cost           | 40.3596      |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 505          |
|    mean_ep_length       | 431          |
|    mean_reward          | 457          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.345       |
|    reward_run           | 1.36         |
| rollout/                |              |
|    adjusted_reward      | 1.05         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 514          |
| time/                   |              |
|    fps                  | 1133         |
|    iterations           | 43           |
|    time_elapsed         | 388          |
|    total_timesteps      | 440320       |
| train/                  |              |
|    approx_kl            | 0.015743837  |
|    average_cost         | 0.0013788904 |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.944        |
|    cost_value_loss      | 0.0107       |
|    entropy_loss         | -7.01        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0582       |
|    n_updates            | 420          |
|    nu                   | 4.53         |
|    nu_loss              | -0.00615     |
|    policy_gradient_loss | -0.00211     |
|    reward_explained_... | 0.146        |
|    reward_value_loss    | 0.141        |
|    std                  | 0.78         |
|    total_cost           | 14.119838    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 522          |
|    mean_ep_length       | 500          |
|    mean_reward          | 522          |
| infos/                  |              |
|    cost                 | 0.00811      |
|    reward_ctrl          | -0.35        |
|    reward_run           | 1.4          |
| rollout/                |              |
|    adjusted_reward      | 1.07         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 523          |
| time/                   |              |
|    fps                  | 1135         |
|    iterations           | 44           |
|    time_elapsed         | 396          |
|    total_timesteps      | 450560       |
| train/                  |              |
|    approx_kl            | 0.015041965  |
|    average_cost         | 0.0011067021 |
|    clip_fraction        | 0.157        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.889        |
|    cost_value_loss      | 0.0127       |
|    entropy_loss         | -6.99        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0512       |
|    n_updates            | 430          |
|    nu                   | 4.6          |
|    nu_loss              | -0.00502     |
|    policy_gradient_loss | -0.000529    |
|    reward_explained_... | 0.602        |
|    reward_value_loss    | 0.128        |
|    std                  | 0.777        |
|    total_cost           | 11.33263     |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 538          |
|    mean_ep_length       | 494          |
|    mean_reward          | 538          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.362       |
|    reward_run           | 1.39         |
| rollout/                |              |
|    adjusted_reward      | 1.09         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 530          |
| time/                   |              |
|    fps                  | 1133         |
|    iterations           | 45           |
|    time_elapsed         | 406          |
|    total_timesteps      | 460800       |
| train/                  |              |
|    approx_kl            | 0.016716348  |
|    average_cost         | 0.0028685075 |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.896        |
|    cost_value_loss      | 0.0317       |
|    entropy_loss         | -6.98        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0728       |
|    n_updates            | 440          |
|    nu                   | 4.67         |
|    nu_loss              | -0.0132      |
|    policy_gradient_loss | -0.00168     |
|    reward_explained_... | 0.682        |
|    reward_value_loss    | 0.102        |
|    std                  | 0.775        |
|    total_cost           | 29.373518    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 549          |
|    mean_ep_length       | 500          |
|    mean_reward          | 549          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.341       |
|    reward_run           | 1.66         |
| rollout/                |              |
|    adjusted_reward      | 1.11         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 535          |
| time/                   |              |
|    fps                  | 1133         |
|    iterations           | 46           |
|    time_elapsed         | 415          |
|    total_timesteps      | 471040       |
| train/                  |              |
|    approx_kl            | 0.016325872  |
|    average_cost         | 0.0017769041 |
|    clip_fraction        | 0.209        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.9          |
|    cost_value_loss      | 0.0149       |
|    entropy_loss         | -6.95        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0365       |
|    n_updates            | 450          |
|    nu                   | 4.74         |
|    nu_loss              | -0.0083      |
|    policy_gradient_loss | -0.00178     |
|    reward_explained_... | 0.596        |
|    reward_value_loss    | 0.118        |
|    std                  | 0.771        |
|    total_cost           | 18.195498    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 549         |
|    mean_ep_length       | 477         |
|    mean_reward          | 517         |
| infos/                  |             |
|    cost                 | 0.00829     |
|    reward_ctrl          | -0.351      |
|    reward_run           | 1.29        |
| rollout/                |             |
|    adjusted_reward      | 1.08        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 542         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 47          |
|    time_elapsed         | 423         |
|    total_timesteps      | 481280      |
| train/                  |             |
|    approx_kl            | 0.015199417 |
|    average_cost         | 0.000863236 |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.905       |
|    cost_value_loss      | 0.00579     |
|    entropy_loss         | -6.92       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0398      |
|    n_updates            | 460         |
|    nu                   | 4.8         |
|    nu_loss              | -0.00409    |
|    policy_gradient_loss | -0.00132    |
|    reward_explained_... | 0.401       |
|    reward_value_loss    | 0.119       |
|    std                  | 0.767       |
|    total_cost           | 8.839537    |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 552          |
|    mean_ep_length       | 500          |
|    mean_reward          | 552          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.357       |
|    reward_run           | 1.34         |
| rollout/                |              |
|    adjusted_reward      | 1.07         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 545          |
| time/                   |              |
|    fps                  | 1134         |
|    iterations           | 48           |
|    time_elapsed         | 433          |
|    total_timesteps      | 491520       |
| train/                  |              |
|    approx_kl            | 0.015350926  |
|    average_cost         | 0.0016561983 |
|    clip_fraction        | 0.185        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.942        |
|    cost_value_loss      | 0.011        |
|    entropy_loss         | -6.91        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0173       |
|    n_updates            | 470          |
|    nu                   | 4.86         |
|    nu_loss              | -0.00795     |
|    policy_gradient_loss | -0.00144     |
|    reward_explained_... | 0.222        |
|    reward_value_loss    | 0.132        |
|    std                  | 0.766        |
|    total_cost           | 16.95947     |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 552          |
|    mean_ep_length       | 407          |
|    mean_reward          | 456          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.353       |
|    reward_run           | 1.47         |
| rollout/                |              |
|    adjusted_reward      | 1.07         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 545          |
| time/                   |              |
|    fps                  | 1138         |
|    iterations           | 49           |
|    time_elapsed         | 440          |
|    total_timesteps      | 501760       |
| train/                  |              |
|    approx_kl            | 0.0156673    |
|    average_cost         | 0.0041160407 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.935        |
|    cost_value_loss      | 0.027        |
|    entropy_loss         | -6.89        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0691       |
|    n_updates            | 480          |
|    nu                   | 4.92         |
|    nu_loss              | -0.02        |
|    policy_gradient_loss | -0.000527    |
|    reward_explained_... | 0.482        |
|    reward_value_loss    | 0.125        |
|    std                  | 0.762        |
|    total_cost           | 42.14826     |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 597          |
|    mean_ep_length       | 500          |
|    mean_reward          | 597          |
| infos/                  |              |
|    cost                 | 0.00837      |
|    reward_ctrl          | -0.351       |
|    reward_run           | 1.46         |
| rollout/                |              |
|    adjusted_reward      | 1.11         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 550          |
| time/                   |              |
|    fps                  | 1141         |
|    iterations           | 50           |
|    time_elapsed         | 448          |
|    total_timesteps      | 512000       |
| train/                  |              |
|    approx_kl            | 0.015825745  |
|    average_cost         | 0.0040618237 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.953        |
|    cost_value_loss      | 0.0261       |
|    entropy_loss         | -6.87        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0255       |
|    n_updates            | 490          |
|    nu                   | 4.98         |
|    nu_loss              | -0.02        |
|    policy_gradient_loss | -0.000517    |
|    reward_explained_... | 0.528        |
|    reward_value_loss    | 0.116        |
|    std                  | 0.761        |
|    total_cost           | 41.593075    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 597          |
|    mean_ep_length       | 466          |
|    mean_reward          | 515          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.35        |
|    reward_run           | 1.39         |
| rollout/                |              |
|    adjusted_reward      | 1.12         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 552          |
| time/                   |              |
|    fps                  | 1143         |
|    iterations           | 51           |
|    time_elapsed         | 456          |
|    total_timesteps      | 522240       |
| train/                  |              |
|    approx_kl            | 0.016016075  |
|    average_cost         | 0.0014676596 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.971        |
|    cost_value_loss      | 0.00535      |
|    entropy_loss         | -6.85        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0609       |
|    n_updates            | 500          |
|    nu                   | 5.04         |
|    nu_loss              | -0.00731     |
|    policy_gradient_loss | -0.00129     |
|    reward_explained_... | 0.524        |
|    reward_value_loss    | 0.109        |
|    std                  | 0.76         |
|    total_cost           | 15.028834    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 597           |
|    mean_ep_length       | 500           |
|    mean_reward          | 578           |
| infos/                  |               |
|    cost                 | 0.00849       |
|    reward_ctrl          | -0.344        |
|    reward_run           | 1.34          |
| rollout/                |               |
|    adjusted_reward      | 1.11          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 553           |
| time/                   |               |
|    fps                  | 1143          |
|    iterations           | 52            |
|    time_elapsed         | 465           |
|    total_timesteps      | 532480        |
| train/                  |               |
|    approx_kl            | 0.017627476   |
|    average_cost         | 0.00040850596 |
|    clip_fraction        | 0.182         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.864         |
|    cost_value_loss      | 0.0068        |
|    entropy_loss         | -6.82         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.056         |
|    n_updates            | 510           |
|    nu                   | 5.09          |
|    nu_loss              | -0.00206      |
|    policy_gradient_loss | -0.00139      |
|    reward_explained_... | 0.647         |
|    reward_value_loss    | 0.105         |
|    std                  | 0.754         |
|    total_cost           | 4.183101      |
-------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 607          |
|    mean_ep_length       | 500          |
|    mean_reward          | 607          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.368       |
|    reward_run           | 1.4          |
| rollout/                |              |
|    adjusted_reward      | 1.17         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 562          |
| time/                   |              |
|    fps                  | 1144         |
|    iterations           | 53           |
|    time_elapsed         | 474          |
|    total_timesteps      | 542720       |
| train/                  |              |
|    approx_kl            | 0.018381676  |
|    average_cost         | 0.0018838074 |
|    clip_fraction        | 0.191        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.922        |
|    cost_value_loss      | 0.015        |
|    entropy_loss         | -6.81        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0887       |
|    n_updates            | 520          |
|    nu                   | 5.15         |
|    nu_loss              | -0.00959     |
|    policy_gradient_loss | -0.000711    |
|    reward_explained_... | 0.476        |
|    reward_value_loss    | 0.131        |
|    std                  | 0.755        |
|    total_cost           | 19.290188    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 607          |
|    mean_ep_length       | 500          |
|    mean_reward          | 590          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.35        |
|    reward_run           | 1.68         |
| rollout/                |              |
|    adjusted_reward      | 1.14         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 566          |
| time/                   |              |
|    fps                  | 1145         |
|    iterations           | 54           |
|    time_elapsed         | 482          |
|    total_timesteps      | 552960       |
| train/                  |              |
|    approx_kl            | 0.015006365  |
|    average_cost         | 0.0002818975 |
|    clip_fraction        | 0.186        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.847        |
|    cost_value_loss      | 0.00621      |
|    entropy_loss         | -6.8         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0799       |
|    n_updates            | 530          |
|    nu                   | 5.19         |
|    nu_loss              | -0.00145     |
|    policy_gradient_loss | -0.00128     |
|    reward_explained_... | 0.558        |
|    reward_value_loss    | 0.115        |
|    std                  | 0.752        |
|    total_cost           | 2.8866303    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 607         |
|    mean_ep_length       | 500         |
|    mean_reward          | 582         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.36       |
|    reward_run           | 1.84        |
| rollout/                |             |
|    adjusted_reward      | 1.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 571         |
| time/                   |             |
|    fps                  | 1147        |
|    iterations           | 55          |
|    time_elapsed         | 490         |
|    total_timesteps      | 563200      |
| train/                  |             |
|    approx_kl            | 0.015082901 |
|    average_cost         | 0.000791559 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.874       |
|    cost_value_loss      | 0.00602     |
|    entropy_loss         | -6.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 540         |
|    nu                   | 5.24        |
|    nu_loss              | -0.00411    |
|    policy_gradient_loss | -0.000212   |
|    reward_explained_... | 0.419       |
|    reward_value_loss    | 0.128       |
|    std                  | 0.749       |
|    total_cost           | 8.105564    |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 607           |
|    mean_ep_length       | 500           |
|    mean_reward          | 578           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.366        |
|    reward_run           | 1.89          |
| rollout/                |               |
|    adjusted_reward      | 1.16          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 575           |
| time/                   |               |
|    fps                  | 1146          |
|    iterations           | 56            |
|    time_elapsed         | 500           |
|    total_timesteps      | 573440        |
| train/                  |               |
|    approx_kl            | 0.016003307   |
|    average_cost         | 0.00059057237 |
|    clip_fraction        | 0.183         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.955         |
|    cost_value_loss      | 0.00242       |
|    entropy_loss         | -6.77         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0859        |
|    n_updates            | 550           |
|    nu                   | 5.28          |
|    nu_loss              | -0.00309      |
|    policy_gradient_loss | -0.00158      |
|    reward_explained_... | 0.702         |
|    reward_value_loss    | 0.0952        |
|    std                  | 0.75          |
|    total_cost           | 6.047461      |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 631          |
|    mean_ep_length       | 500          |
|    mean_reward          | 631          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.345       |
|    reward_run           | 1.68         |
| rollout/                |              |
|    adjusted_reward      | 1.2          |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 583          |
| time/                   |              |
|    fps                  | 1144         |
|    iterations           | 57           |
|    time_elapsed         | 509          |
|    total_timesteps      | 583680       |
| train/                  |              |
|    approx_kl            | 0.015982157  |
|    average_cost         | 0.0006000359 |
|    clip_fraction        | 0.18         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.907        |
|    cost_value_loss      | 0.00604      |
|    entropy_loss         | -6.76        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0387       |
|    n_updates            | 560          |
|    nu                   | 5.32         |
|    nu_loss              | -0.00317     |
|    policy_gradient_loss | -0.00189     |
|    reward_explained_... | 0.821        |
|    reward_value_loss    | 0.0822       |
|    std                  | 0.748        |
|    total_cost           | 6.1443677    |
------------------------------------------
Early stopping at step 7 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 631         |
|    mean_ep_length       | 422         |
|    mean_reward          | 535         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.384      |
|    reward_run           | 1.39        |
| rollout/                |             |
|    adjusted_reward      | 1.21        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 586         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 58          |
|    time_elapsed         | 520         |
|    total_timesteps      | 593920      |
| train/                  |             |
|    approx_kl            | 0.015661718 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.748       |
|    cost_value_loss      | 0.00103     |
|    entropy_loss         | -6.71       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0514      |
|    n_updates            | 570         |
|    nu                   | 5.36        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00302    |
|    reward_explained_... | 0.739       |
|    reward_value_loss    | 0.105       |
|    std                  | 0.741       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 631          |
|    mean_ep_length       | 381          |
|    mean_reward          | 421          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.364       |
|    reward_run           | 1.83         |
| rollout/                |              |
|    adjusted_reward      | 1.2          |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 592          |
| time/                   |              |
|    fps                  | 1145         |
|    iterations           | 59           |
|    time_elapsed         | 527          |
|    total_timesteps      | 604160       |
| train/                  |              |
|    approx_kl            | 0.01525226   |
|    average_cost         | 0.0008660442 |
|    clip_fraction        | 0.188        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.936        |
|    cost_value_loss      | 0.00354      |
|    entropy_loss         | -6.67        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0218       |
|    n_updates            | 580          |
|    nu                   | 5.39         |
|    nu_loss              | -0.00464     |
|    policy_gradient_loss | -0.00154     |
|    reward_explained_... | 0.758        |
|    reward_value_loss    | 0.0996       |
|    std                  | 0.737        |
|    total_cost           | 8.868293     |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 631          |
|    mean_ep_length       | 411          |
|    mean_reward          | 524          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.366       |
|    reward_run           | 1.69         |
| rollout/                |              |
|    adjusted_reward      | 1.21         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 597          |
| time/                   |              |
|    fps                  | 1144         |
|    iterations           | 60           |
|    time_elapsed         | 536          |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0154953655 |
|    average_cost         | 0.000495652  |
|    clip_fraction        | 0.182        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.882        |
|    cost_value_loss      | 0.00363      |
|    entropy_loss         | -6.64        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.029        |
|    n_updates            | 590          |
|    nu                   | 5.42         |
|    nu_loss              | -0.00267     |
|    policy_gradient_loss | -0.00167     |
|    reward_explained_... | 0.689        |
|    reward_value_loss    | 0.0991       |
|    std                  | 0.734        |
|    total_cost           | 5.0754766    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 631           |
|    mean_ep_length       | 433           |
|    mean_reward          | 512           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.371        |
|    reward_run           | 1.53          |
| rollout/                |               |
|    adjusted_reward      | 1.25          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 606           |
| time/                   |               |
|    fps                  | 1151          |
|    iterations           | 61            |
|    time_elapsed         | 542           |
|    total_timesteps      | 624640        |
| train/                  |               |
|    approx_kl            | 0.015178153   |
|    average_cost         | 0.00066595885 |
|    clip_fraction        | 0.195         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.931         |
|    cost_value_loss      | 0.00168       |
|    entropy_loss         | -6.61         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0328        |
|    n_updates            | 600           |
|    nu                   | 5.45          |
|    nu_loss              | -0.00361      |
|    policy_gradient_loss | -0.00121      |
|    reward_explained_... | 0.707         |
|    reward_value_loss    | 0.113         |
|    std                  | 0.729         |
|    total_cost           | 6.819419      |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 631          |
|    mean_ep_length       | 500          |
|    mean_reward          | 605          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.375       |
|    reward_run           | 1.47         |
| rollout/                |              |
|    adjusted_reward      | 1.22         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 609          |
| time/                   |              |
|    fps                  | 1151         |
|    iterations           | 62           |
|    time_elapsed         | 551          |
|    total_timesteps      | 634880       |
| train/                  |              |
|    approx_kl            | 0.016977388  |
|    average_cost         | 0.0017985359 |
|    clip_fraction        | 0.203        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.952        |
|    cost_value_loss      | 0.0134       |
|    entropy_loss         | -6.57        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0342       |
|    n_updates            | 610          |
|    nu                   | 5.48         |
|    nu_loss              | -0.00981     |
|    policy_gradient_loss | -0.0014      |
|    reward_explained_... | 0.721        |
|    reward_value_loss    | 0.103        |
|    std                  | 0.725        |
|    total_cost           | 18.417007    |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 631         |
|    mean_ep_length       | 500         |
|    mean_reward          | 630         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.36       |
|    reward_run           | 1.47        |
| rollout/                |             |
|    adjusted_reward      | 1.23        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 612         |
| time/                   |             |
|    fps                  | 1150        |
|    iterations           | 63          |
|    time_elapsed         | 560         |
|    total_timesteps      | 645120      |
| train/                  |             |
|    approx_kl            | 0.015629878 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.757       |
|    cost_value_loss      | 0.000729    |
|    entropy_loss         | -6.55       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0797      |
|    n_updates            | 620         |
|    nu                   | 5.51        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00111    |
|    reward_explained_... | 0.678       |
|    reward_value_loss    | 0.108       |
|    std                  | 0.721       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 643           |
|    mean_ep_length       | 500           |
|    mean_reward          | 643           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.37         |
|    reward_run           | 1.83          |
| rollout/                |               |
|    adjusted_reward      | 1.26          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 617           |
| time/                   |               |
|    fps                  | 1150          |
|    iterations           | 64            |
|    time_elapsed         | 569           |
|    total_timesteps      | 655360        |
| train/                  |               |
|    approx_kl            | 0.015985388   |
|    average_cost         | 0.00023212555 |
|    clip_fraction        | 0.179         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.888         |
|    cost_value_loss      | 0.00127       |
|    entropy_loss         | -6.52         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0564        |
|    n_updates            | 630           |
|    nu                   | 5.54          |
|    nu_loss              | -0.00128      |
|    policy_gradient_loss | -0.000905     |
|    reward_explained_... | 0.652         |
|    reward_value_loss    | 0.135         |
|    std                  | 0.719         |
|    total_cost           | 2.3769655     |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 658        |
|    mean_ep_length       | 500        |
|    mean_reward          | 658        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.356     |
|    reward_run           | 1.4        |
| rollout/                |            |
|    adjusted_reward      | 1.27       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 622        |
| time/                   |            |
|    fps                  | 1147       |
|    iterations           | 65         |
|    time_elapsed         | 579        |
|    total_timesteps      | 665600     |
| train/                  |            |
|    approx_kl            | 0.01509878 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.397      |
|    cost_value_loss      | 8.09e-05   |
|    entropy_loss         | -6.51      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.164      |
|    n_updates            | 640        |
|    nu                   | 5.56       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.00163   |
|    reward_explained_... | 0.768      |
|    reward_value_loss    | 0.0968     |
|    std                  | 0.718      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 658          |
|    mean_ep_length       | 500          |
|    mean_reward          | 647          |
| infos/                  |              |
|    cost                 | 0.0093       |
|    reward_ctrl          | -0.379       |
|    reward_run           | 1.47         |
| rollout/                |              |
|    adjusted_reward      | 1.24         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 623          |
| time/                   |              |
|    fps                  | 1148         |
|    iterations           | 66           |
|    time_elapsed         | 588          |
|    total_timesteps      | 675840       |
| train/                  |              |
|    approx_kl            | 0.016285023  |
|    average_cost         | 0.0018169426 |
|    clip_fraction        | 0.188        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.956        |
|    cost_value_loss      | 0.00562      |
|    entropy_loss         | -6.5         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0351       |
|    n_updates            | 650          |
|    nu                   | 5.59         |
|    nu_loss              | -0.0101      |
|    policy_gradient_loss | -0.00116     |
|    reward_explained_... | 0.823        |
|    reward_value_loss    | 0.0912       |
|    std                  | 0.718        |
|    total_cost           | 18.605492    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 658         |
|    mean_ep_length       | 465         |
|    mean_reward          | 559         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.373      |
|    reward_run           | 1.51        |
| rollout/                |             |
|    adjusted_reward      | 1.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 626         |
| time/                   |             |
|    fps                  | 1149        |
|    iterations           | 67          |
|    time_elapsed         | 596         |
|    total_timesteps      | 686080      |
| train/                  |             |
|    approx_kl            | 0.015961293 |
|    average_cost         | 0.001106619 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.762       |
|    cost_value_loss      | 0.00944     |
|    entropy_loss         | -6.5        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0382      |
|    n_updates            | 660         |
|    nu                   | 5.61        |
|    nu_loss              | -0.00618    |
|    policy_gradient_loss | -0.000293   |
|    reward_explained_... | 0.693       |
|    reward_value_loss    | 0.127       |
|    std                  | 0.717       |
|    total_cost           | 11.331779   |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 658          |
|    mean_ep_length       | 500          |
|    mean_reward          | 624          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.361       |
|    reward_run           | 1.93         |
| rollout/                |              |
|    adjusted_reward      | 1.31         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 631          |
| time/                   |              |
|    fps                  | 1150         |
|    iterations           | 68           |
|    time_elapsed         | 605          |
|    total_timesteps      | 696320       |
| train/                  |              |
|    approx_kl            | 0.015674466  |
|    average_cost         | 0.0015068697 |
|    clip_fraction        | 0.157        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.938        |
|    cost_value_loss      | 0.00259      |
|    entropy_loss         | -6.49        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0722       |
|    n_updates            | 670          |
|    nu                   | 5.64         |
|    nu_loss              | -0.00846     |
|    policy_gradient_loss | -0.000267    |
|    reward_explained_... | 0.79         |
|    reward_value_loss    | 0.0894       |
|    std                  | 0.715        |
|    total_cost           | 15.430346    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 678           |
|    mean_ep_length       | 500           |
|    mean_reward          | 678           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.361        |
|    reward_run           | 1.67          |
| rollout/                |               |
|    adjusted_reward      | 1.29          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 635           |
| time/                   |               |
|    fps                  | 1151          |
|    iterations           | 69            |
|    time_elapsed         | 613           |
|    total_timesteps      | 706560        |
| train/                  |               |
|    approx_kl            | 0.01681134    |
|    average_cost         | 0.00017333314 |
|    clip_fraction        | 0.172         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.848         |
|    cost_value_loss      | 0.00549       |
|    entropy_loss         | -6.46         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0222        |
|    n_updates            | 680           |
|    nu                   | 5.66          |
|    nu_loss              | -0.000977     |
|    policy_gradient_loss | -0.000394     |
|    reward_explained_... | 0.807         |
|    reward_value_loss    | 0.0916        |
|    std                  | 0.712         |
|    total_cost           | 1.7749314     |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 678          |
|    mean_ep_length       | 500          |
|    mean_reward          | 651          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.379       |
|    reward_run           | 1.67         |
| rollout/                |              |
|    adjusted_reward      | 1.3          |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 638          |
| time/                   |              |
|    fps                  | 1150         |
|    iterations           | 70           |
|    time_elapsed         | 623          |
|    total_timesteps      | 716800       |
| train/                  |              |
|    approx_kl            | 0.017871717  |
|    average_cost         | 0.0011475792 |
|    clip_fraction        | 0.19         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.946        |
|    cost_value_loss      | 0.00255      |
|    entropy_loss         | -6.43        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0477       |
|    n_updates            | 690          |
|    nu                   | 5.69         |
|    nu_loss              | -0.0065      |
|    policy_gradient_loss | -0.00187     |
|    reward_explained_... | 0.829        |
|    reward_value_loss    | 0.0983       |
|    std                  | 0.708        |
|    total_cost           | 11.751211    |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 678         |
|    mean_ep_length       | 500         |
|    mean_reward          | 636         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.38       |
|    reward_run           | 1.75        |
| rollout/                |             |
|    adjusted_reward      | 1.32        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 646         |
| time/                   |             |
|    fps                  | 1149        |
|    iterations           | 71          |
|    time_elapsed         | 632         |
|    total_timesteps      | 727040      |
| train/                  |             |
|    approx_kl            | 0.015487751 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.628       |
|    cost_value_loss      | 0.000104    |
|    entropy_loss         | -6.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 700         |
|    nu                   | 5.71        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00115    |
|    reward_explained_... | 0.826       |
|    reward_value_loss    | 0.0946      |
|    std                  | 0.705       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 6 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 683         |
|    mean_ep_length       | 500         |
|    mean_reward          | 683         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.375      |
|    reward_run           | 1.68        |
| rollout/                |             |
|    adjusted_reward      | 1.36        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 654         |
| time/                   |             |
|    fps                  | 1146        |
|    iterations           | 72          |
|    time_elapsed         | 643         |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.017369656 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.482       |
|    cost_value_loss      | 7.79e-05    |
|    entropy_loss         | -6.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0629      |
|    n_updates            | 710         |
|    nu                   | 5.73        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00213    |
|    reward_explained_... | 0.823       |
|    reward_value_loss    | 0.0885      |
|    std                  | 0.701       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 9 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 683           |
|    mean_ep_length       | 500           |
|    mean_reward          | 670           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.364        |
|    reward_run           | 1.24          |
| rollout/                |               |
|    adjusted_reward      | 1.35          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 661           |
| time/                   |               |
|    fps                  | 1140          |
|    iterations           | 73            |
|    time_elapsed         | 655           |
|    total_timesteps      | 747520        |
| train/                  |               |
|    approx_kl            | 0.01660508    |
|    average_cost         | 0.00020610579 |
|    clip_fraction        | 0.223         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.662         |
|    cost_value_loss      | 0.00107       |
|    entropy_loss         | -6.32         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0388        |
|    n_updates            | 720           |
|    nu                   | 5.75          |
|    nu_loss              | -0.00118      |
|    policy_gradient_loss | -0.0031       |
|    reward_explained_... | 0.815         |
|    reward_value_loss    | 0.114         |
|    std                  | 0.695         |
|    total_cost           | 2.1105232     |
-------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 683       |
|    mean_ep_length       | 500       |
|    mean_reward          | 673       |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.382    |
|    reward_run           | 1.87      |
| rollout/                |           |
|    adjusted_reward      | 1.38      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 671       |
| time/                   |           |
|    fps                  | 1138      |
|    iterations           | 74        |
|    time_elapsed         | 665       |
|    total_timesteps      | 757760    |
| train/                  |           |
|    approx_kl            | 0.0178708 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.193     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.439     |
|    cost_value_loss      | 0.000293  |
|    entropy_loss         | -6.29     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0646    |
|    n_updates            | 730       |
|    nu                   | 5.76      |
|    nu_loss              | -0        |
|    policy_gradient_loss | -0.00176  |
|    reward_explained_... | 0.799     |
|    reward_value_loss    | 0.138     |
|    std                  | 0.694     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
Violated constraint in the test environment, terminating the episode.
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 683          |
|    mean_ep_length       | 411          |
|    mean_reward          | 566          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.388       |
|    reward_run           | 1.82         |
| rollout/                |              |
|    adjusted_reward      | 1.4          |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 679          |
| time/                   |              |
|    fps                  | 1140         |
|    iterations           | 75           |
|    time_elapsed         | 673          |
|    total_timesteps      | 768000       |
| train/                  |              |
|    approx_kl            | 0.015443513  |
|    average_cost         | 0.0002083671 |
|    clip_fraction        | 0.165        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.602        |
|    cost_value_loss      | 0.000882     |
|    entropy_loss         | -6.27        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0246       |
|    n_updates            | 740          |
|    nu                   | 5.78         |
|    nu_loss              | -0.0012      |
|    policy_gradient_loss | -0.000216    |
|    reward_explained_... | 0.854        |
|    reward_value_loss    | 0.0898       |
|    std                  | 0.689        |
|    total_cost           | 2.1336792    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 760         |
|    mean_ep_length       | 500         |
|    mean_reward          | 760         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 1.94        |
| rollout/                |             |
|    adjusted_reward      | 1.41        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 687         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 76          |
|    time_elapsed         | 682         |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.015383055 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.59        |
|    cost_value_loss      | 0.000265    |
|    entropy_loss         | -6.24       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0278      |
|    n_updates            | 750         |
|    nu                   | 5.79        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000829   |
|    reward_explained_... | 0.833       |
|    reward_value_loss    | 0.106       |
|    std                  | 0.688       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 760          |
|    mean_ep_length       | 500          |
|    mean_reward          | 698          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.39        |
|    reward_run           | 1.57         |
| rollout/                |              |
|    adjusted_reward      | 1.42         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 693          |
| time/                   |              |
|    fps                  | 1139         |
|    iterations           | 77           |
|    time_elapsed         | 692          |
|    total_timesteps      | 788480       |
| train/                  |              |
|    approx_kl            | 0.0151483435 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.188        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.525        |
|    cost_value_loss      | 5.86e-05     |
|    entropy_loss         | -6.21        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.022        |
|    n_updates            | 760          |
|    nu                   | 5.81         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0016      |
|    reward_explained_... | 0.865        |
|    reward_value_loss    | 0.0886       |
|    std                  | 0.684        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 760         |
|    mean_ep_length       | 500         |
|    mean_reward          | 740         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.388      |
|    reward_run           | 1.97        |
| rollout/                |             |
|    adjusted_reward      | 1.47        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 703         |
| time/                   |             |
|    fps                  | 1136        |
|    iterations           | 78          |
|    time_elapsed         | 702         |
|    total_timesteps      | 798720      |
| train/                  |             |
|    approx_kl            | 0.015811391 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.199       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.549       |
|    cost_value_loss      | 6.17e-05    |
|    entropy_loss         | -6.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0814      |
|    n_updates            | 770         |
|    nu                   | 5.82        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00157    |
|    reward_explained_... | 0.804       |
|    reward_value_loss    | 0.117       |
|    std                  | 0.683       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 760        |
|    mean_ep_length       | 500        |
|    mean_reward          | 718        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.393     |
|    reward_run           | 1.87       |
| rollout/                |            |
|    adjusted_reward      | 1.42       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 707        |
| time/                   |            |
|    fps                  | 1136       |
|    iterations           | 79         |
|    time_elapsed         | 711        |
|    total_timesteps      | 808960     |
| train/                  |            |
|    approx_kl            | 0.01690196 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.386      |
|    cost_value_loss      | 3.13e-05   |
|    entropy_loss         | -6.17      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 780        |
|    nu                   | 5.83       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000603  |
|    reward_explained_... | 0.864      |
|    reward_value_loss    | 0.0971     |
|    std                  | 0.677      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 760         |
|    mean_ep_length       | 500         |
|    mean_reward          | 691         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.388      |
|    reward_run           | 1.62        |
| rollout/                |             |
|    adjusted_reward      | 1.46        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 713         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 80          |
|    time_elapsed         | 721         |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.016105032 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.846       |
|    cost_value_loss      | 0.000182    |
|    entropy_loss         | -6.13       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0313      |
|    n_updates            | 790         |
|    nu                   | 5.84        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00152    |
|    reward_explained_... | 0.831       |
|    reward_value_loss    | 0.0957      |
|    std                  | 0.674       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 760         |
|    mean_ep_length       | 500         |
|    mean_reward          | 750         |
| infos/                  |             |
|    cost                 | 0.0101      |
|    reward_ctrl          | -0.376      |
|    reward_run           | 1.61        |
| rollout/                |             |
|    adjusted_reward      | 1.45        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 719         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 81          |
|    time_elapsed         | 730         |
|    total_timesteps      | 829440      |
| train/                  |             |
|    approx_kl            | 0.017116513 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.529       |
|    cost_value_loss      | 6.59e-05    |
|    entropy_loss         | -6.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0544      |
|    n_updates            | 800         |
|    nu                   | 5.85        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00107    |
|    reward_explained_... | 0.832       |
|    reward_value_loss    | 0.11        |
|    std                  | 0.672       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 762          |
|    mean_ep_length       | 500          |
|    mean_reward          | 762          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.37        |
|    reward_run           | 1.68         |
| rollout/                |              |
|    adjusted_reward      | 1.45         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 723          |
| time/                   |              |
|    fps                  | 1136         |
|    iterations           | 82           |
|    time_elapsed         | 739          |
|    total_timesteps      | 839680       |
| train/                  |              |
|    approx_kl            | 0.020942789  |
|    average_cost         | 0.0004953119 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.417        |
|    cost_value_loss      | 0.0015       |
|    entropy_loss         | -6.09        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0861       |
|    n_updates            | 810          |
|    nu                   | 5.86         |
|    nu_loss              | -0.0029      |
|    policy_gradient_loss | -0.000617    |
|    reward_explained_... | 0.806        |
|    reward_value_loss    | 0.109        |
|    std                  | 0.671        |
|    total_cost           | 5.071994     |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 762          |
|    mean_ep_length       | 500          |
|    mean_reward          | 693          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.363       |
|    reward_run           | 1.55         |
| rollout/                |              |
|    adjusted_reward      | 1.46         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 722          |
| time/                   |              |
|    fps                  | 1136         |
|    iterations           | 83           |
|    time_elapsed         | 747          |
|    total_timesteps      | 849920       |
| train/                  |              |
|    approx_kl            | 0.016962484  |
|    average_cost         | 0.0014650805 |
|    clip_fraction        | 0.192        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.847        |
|    cost_value_loss      | 0.00224      |
|    entropy_loss         | -6.08        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0734       |
|    n_updates            | 820          |
|    nu                   | 5.87         |
|    nu_loss              | -0.00858     |
|    policy_gradient_loss | -0.000265    |
|    reward_explained_... | 0.837        |
|    reward_value_loss    | 0.0995       |
|    std                  | 0.671        |
|    total_cost           | 15.002424    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 762           |
|    mean_ep_length       | 500           |
|    mean_reward          | 759           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.38         |
|    reward_run           | 1.7           |
| rollout/                |               |
|    adjusted_reward      | 1.43          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 724           |
| time/                   |               |
|    fps                  | 1137          |
|    iterations           | 84            |
|    time_elapsed         | 756           |
|    total_timesteps      | 860160        |
| train/                  |               |
|    approx_kl            | 0.015948666   |
|    average_cost         | 4.9720104e-05 |
|    clip_fraction        | 0.17          |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.827         |
|    cost_value_loss      | 0.000664      |
|    entropy_loss         | -6.07         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.102         |
|    n_updates            | 830           |
|    nu                   | 5.88          |
|    nu_loss              | -0.000292     |
|    policy_gradient_loss | -0.000409     |
|    reward_explained_... | 0.877         |
|    reward_value_loss    | 0.0911        |
|    std                  | 0.668         |
|    total_cost           | 0.5091339     |
-------------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 789         |
|    mean_ep_length       | 500         |
|    mean_reward          | 789         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.38       |
|    reward_run           | 1.98        |
| rollout/                |             |
|    adjusted_reward      | 1.51        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 730         |
| time/                   |             |
|    fps                  | 1138        |
|    iterations           | 85          |
|    time_elapsed         | 764         |
|    total_timesteps      | 870400      |
| train/                  |             |
|    approx_kl            | 0.01616234  |
|    average_cost         | 0.001618322 |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.815       |
|    cost_value_loss      | 0.00562     |
|    entropy_loss         | -6.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0468      |
|    n_updates            | 840         |
|    nu                   | 5.89        |
|    nu_loss              | -0.00951    |
|    policy_gradient_loss | 0.000884    |
|    reward_explained_... | 0.847       |
|    reward_value_loss    | 0.101       |
|    std                  | 0.669       |
|    total_cost           | 16.571617   |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 789         |
|    mean_ep_length       | 500         |
|    mean_reward          | 680         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.389      |
|    reward_run           | 2.18        |
| rollout/                |             |
|    adjusted_reward      | 1.54        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 738         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 86          |
|    time_elapsed         | 772         |
|    total_timesteps      | 880640      |
| train/                  |             |
|    approx_kl            | 0.015137154 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.454       |
|    cost_value_loss      | 0.000112    |
|    entropy_loss         | -6.06       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0355      |
|    n_updates            | 850         |
|    nu                   | 5.9         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000876   |
|    reward_explained_... | 0.889       |
|    reward_value_loss    | 0.0856      |
|    std                  | 0.668       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 789          |
|    mean_ep_length       | 500          |
|    mean_reward          | 719          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.405       |
|    reward_run           | 2.14         |
| rollout/                |              |
|    adjusted_reward      | 1.58         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 751          |
| time/                   |              |
|    fps                  | 1140         |
|    iterations           | 87           |
|    time_elapsed         | 781          |
|    total_timesteps      | 890880       |
| train/                  |              |
|    approx_kl            | 0.019322481  |
|    average_cost         | 0.0009224371 |
|    clip_fraction        | 0.201        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.889        |
|    cost_value_loss      | 0.00346      |
|    entropy_loss         | -6.05        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.106        |
|    n_updates            | 860          |
|    nu                   | 5.92         |
|    nu_loss              | -0.00544     |
|    policy_gradient_loss | -0.00114     |
|    reward_explained_... | 0.82         |
|    reward_value_loss    | 0.118        |
|    std                  | 0.666        |
|    total_cost           | 9.445756     |
------------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 789         |
|    mean_ep_length       | 476         |
|    mean_reward          | 748         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 1.91        |
| rollout/                |             |
|    adjusted_reward      | 1.59        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 763         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 88          |
|    time_elapsed         | 789         |
|    total_timesteps      | 901120      |
| train/                  |             |
|    approx_kl            | 0.015421224 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.622       |
|    cost_value_loss      | 8.22e-05    |
|    entropy_loss         | -6.04       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0262      |
|    n_updates            | 870         |
|    nu                   | 5.93        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00118    |
|    reward_explained_... | 0.841       |
|    reward_value_loss    | 0.105       |
|    std                  | 0.665       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 820         |
|    mean_ep_length       | 500         |
|    mean_reward          | 820         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 2.02        |
| rollout/                |             |
|    adjusted_reward      | 1.55        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 775         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 89          |
|    time_elapsed         | 799         |
|    total_timesteps      | 911360      |
| train/                  |             |
|    approx_kl            | 0.016133655 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.768       |
|    cost_value_loss      | 0.000255    |
|    entropy_loss         | -6.04       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0633      |
|    n_updates            | 880         |
|    nu                   | 5.94        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00117    |
|    reward_explained_... | 0.897       |
|    reward_value_loss    | 0.0869      |
|    std                  | 0.665       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 820        |
|    mean_ep_length       | 500        |
|    mean_reward          | 790        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.394     |
|    reward_run           | 1.75       |
| rollout/                |            |
|    adjusted_reward      | 1.57       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 778        |
| time/                   |            |
|    fps                  | 1140       |
|    iterations           | 90         |
|    time_elapsed         | 808        |
|    total_timesteps      | 921600     |
| train/                  |            |
|    approx_kl            | 0.01760066 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.57       |
|    cost_value_loss      | 2.34e-05   |
|    entropy_loss         | -6.02      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0895     |
|    n_updates            | 890        |
|    nu                   | 5.95       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.00094   |
|    reward_explained_... | 0.877      |
|    reward_value_loss    | 0.102      |
|    std                  | 0.663      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 820         |
|    mean_ep_length       | 500         |
|    mean_reward          | 814         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.374      |
|    reward_run           | 2.15        |
| rollout/                |             |
|    adjusted_reward      | 1.57        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 785         |
| time/                   |             |
|    fps                  | 1139        |
|    iterations           | 91          |
|    time_elapsed         | 818         |
|    total_timesteps      | 931840      |
| train/                  |             |
|    approx_kl            | 0.015714664 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.193       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.837       |
|    cost_value_loss      | 0.000105    |
|    entropy_loss         | -5.99       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0657      |
|    n_updates            | 900         |
|    nu                   | 5.96        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00167    |
|    reward_explained_... | 0.865       |
|    reward_value_loss    | 0.1         |
|    std                  | 0.659       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 820         |
|    mean_ep_length       | 500         |
|    mean_reward          | 760         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.374      |
|    reward_run           | 1.79        |
| rollout/                |             |
|    adjusted_reward      | 1.53        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 777         |
| time/                   |             |
|    fps                  | 1138        |
|    iterations           | 92          |
|    time_elapsed         | 827         |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.016171996 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.895       |
|    cost_value_loss      | 5.72e-05    |
|    entropy_loss         | -5.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0296      |
|    n_updates            | 910         |
|    nu                   | 5.96        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00122    |
|    reward_explained_... | 0.865       |
|    reward_value_loss    | 0.113       |
|    std                  | 0.654       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 820           |
|    mean_ep_length       | 421           |
|    mean_reward          | 610           |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.4          |
|    reward_run           | 1.67          |
| rollout/                |               |
|    adjusted_reward      | 1.62          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 784           |
| time/                   |               |
|    fps                  | 1139          |
|    iterations           | 93            |
|    time_elapsed         | 835           |
|    total_timesteps      | 952320        |
| train/                  |               |
|    approx_kl            | 0.01750188    |
|    average_cost         | 9.8700555e-05 |
|    clip_fraction        | 0.187         |
|    clip_range           | 0.2           |
|    cost_explained_va... | 0.885         |
|    cost_value_loss      | 0.000464      |
|    entropy_loss         | -5.92         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0219        |
|    n_updates            | 920           |
|    nu                   | 5.97          |
|    nu_loss              | -0.000589     |
|    policy_gradient_loss | -0.000835     |
|    reward_explained_... | 0.853         |
|    reward_value_loss    | 0.105         |
|    std                  | 0.653         |
|    total_cost           | 1.0106937     |
-------------------------------------------
Early stopping at step 8 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 843         |
|    mean_ep_length       | 500         |
|    mean_reward          | 843         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.363      |
|    reward_run           | 1.46        |
| rollout/                |             |
|    adjusted_reward      | 1.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 788         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 94          |
|    time_elapsed         | 847         |
|    total_timesteps      | 962560      |
| train/                  |             |
|    approx_kl            | 0.015843386 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.348       |
|    cost_value_loss      | 1.46e-05    |
|    entropy_loss         | -5.9        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0984      |
|    n_updates            | 930         |
|    nu                   | 5.98        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.0024     |
|    reward_explained_... | 0.867       |
|    reward_value_loss    | 0.0961      |
|    std                  | 0.651       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 843         |
|    mean_ep_length       | 500         |
|    mean_reward          | 760         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.406      |
|    reward_run           | 2.08        |
| rollout/                |             |
|    adjusted_reward      | 1.63        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 796         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 95          |
|    time_elapsed         | 857         |
|    total_timesteps      | 972800      |
| train/                  |             |
|    approx_kl            | 0.016089242 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.224       |
|    cost_value_loss      | 1.51e-05    |
|    entropy_loss         | -5.89       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0591      |
|    n_updates            | 940         |
|    nu                   | 5.99        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000814   |
|    reward_explained_... | 0.789       |
|    reward_value_loss    | 0.154       |
|    std                  | 0.649       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 843         |
|    mean_ep_length       | 500         |
|    mean_reward          | 754         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 2.27        |
| rollout/                |             |
|    adjusted_reward      | 1.58        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 795         |
| time/                   |             |
|    fps                  | 1133        |
|    iterations           | 96          |
|    time_elapsed         | 867         |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.017531807 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.839       |
|    cost_value_loss      | 2.36e-05    |
|    entropy_loss         | -5.87       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.029       |
|    n_updates            | 950         |
|    nu                   | 5.99        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00146    |
|    reward_explained_... | 0.868       |
|    reward_value_loss    | 0.0969      |
|    std                  | 0.647       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 843        |
|    mean_ep_length       | 500        |
|    mean_reward          | 738        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.403     |
|    reward_run           | 1.78       |
| rollout/                |            |
|    adjusted_reward      | 1.71       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 813        |
| time/                   |            |
|    fps                  | 1132       |
|    iterations           | 97         |
|    time_elapsed         | 876        |
|    total_timesteps      | 993280     |
| train/                  |            |
|    approx_kl            | 0.01522488 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.205      |
|    cost_value_loss      | 8.87e-06   |
|    entropy_loss         | -5.87      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0461     |
|    n_updates            | 960        |
|    nu                   | 6          |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.00111   |
|    reward_explained_... | 0.813      |
|    reward_value_loss    | 0.127      |
|    std                  | 0.648      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 929         |
|    mean_ep_length       | 500         |
|    mean_reward          | 929         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.39        |
| rollout/                |             |
|    adjusted_reward      | 1.76        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 824         |
| time/                   |             |
|    fps                  | 1131        |
|    iterations           | 98          |
|    time_elapsed         | 886         |
|    total_timesteps      | 1003520     |
| train/                  |             |
|    approx_kl            | 0.016768897 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.819       |
|    cost_value_loss      | 5.68e-05    |
|    entropy_loss         | -5.87       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.019       |
|    n_updates            | 970         |
|    nu                   | 6           |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.0016     |
|    reward_explained_... | 0.886       |
|    reward_value_loss    | 0.0806      |
|    std                  | 0.645       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 929         |
|    mean_ep_length       | 499         |
|    mean_reward          | 683         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.42       |
|    reward_run           | 2.68        |
| rollout/                |             |
|    adjusted_reward      | 1.68        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 832         |
| time/                   |             |
|    fps                  | 1130        |
|    iterations           | 99          |
|    time_elapsed         | 897         |
|    total_timesteps      | 1013760     |
| train/                  |             |
|    approx_kl            | 0.015273482 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.915       |
|    cost_value_loss      | 1.38e-05    |
|    entropy_loss         | -5.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0349      |
|    n_updates            | 980         |
|    nu                   | 6           |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00162    |
|    reward_explained_... | 0.856       |
|    reward_value_loss    | 0.114       |
|    std                  | 0.644       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 929         |
|    mean_ep_length       | 500         |
|    mean_reward          | 801         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 1.98        |
| rollout/                |             |
|    adjusted_reward      | 1.63        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 831         |
| time/                   |             |
|    fps                  | 1130        |
|    iterations           | 100         |
|    time_elapsed         | 905         |
|    total_timesteps      | 1024000     |
| train/                  |             |
|    approx_kl            | 0.015438372 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.829       |
|    cost_value_loss      | 8.37e-06    |
|    entropy_loss         | -5.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0428      |
|    n_updates            | 990         |
|    nu                   | 6.01        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000703   |
|    reward_explained_... | 0.843       |
|    reward_value_loss    | 0.119       |
|    std                  | 0.644       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 929          |
|    mean_ep_length       | 500          |
|    mean_reward          | 883          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.392       |
|    reward_run           | 1.93         |
| rollout/                |              |
|    adjusted_reward      | 1.75         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 844          |
| time/                   |              |
|    fps                  | 1130         |
|    iterations           | 101          |
|    time_elapsed         | 914          |
|    total_timesteps      | 1034240      |
| train/                  |              |
|    approx_kl            | 0.017149627  |
|    average_cost         | 0.0007935634 |
|    clip_fraction        | 0.202        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.104        |
|    cost_value_loss      | 0.00194      |
|    entropy_loss         | -5.82        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0543       |
|    n_updates            | 1000         |
|    nu                   | 6.01         |
|    nu_loss              | -0.00477     |
|    policy_gradient_loss | -8.28e-05    |
|    reward_explained_... | 0.827        |
|    reward_value_loss    | 0.118        |
|    std                  | 0.64         |
|    total_cost           | 8.126089     |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 965         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 1.74        |
| rollout/                |             |
|    adjusted_reward      | 1.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 838         |
| time/                   |             |
|    fps                  | 1131        |
|    iterations           | 102         |
|    time_elapsed         | 923         |
|    total_timesteps      | 1044480     |
| train/                  |             |
|    approx_kl            | 0.017839625 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.85        |
|    cost_value_loss      | 4.88e-05    |
|    entropy_loss         | -5.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0427      |
|    n_updates            | 1010        |
|    nu                   | 6.02        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000754   |
|    reward_explained_... | 0.881       |
|    reward_value_loss    | 0.105       |
|    std                  | 0.641       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 965        |
|    mean_ep_length       | 500        |
|    mean_reward          | 856        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.388     |
|    reward_run           | 2.22       |
| rollout/                |            |
|    adjusted_reward      | 1.81       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 840        |
| time/                   |            |
|    fps                  | 1131       |
|    iterations           | 103        |
|    time_elapsed         | 932        |
|    total_timesteps      | 1054720    |
| train/                  |            |
|    approx_kl            | 0.01686881 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.826      |
|    cost_value_loss      | 4.32e-05   |
|    entropy_loss         | -5.81      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0227     |
|    n_updates            | 1020       |
|    nu                   | 6.02       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000728  |
|    reward_explained_... | 0.885      |
|    reward_value_loss    | 0.0927     |
|    std                  | 0.64       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 819         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.383      |
|    reward_run           | 1.69        |
| rollout/                |             |
|    adjusted_reward      | 1.77        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 849         |
| time/                   |             |
|    fps                  | 1132        |
|    iterations           | 104         |
|    time_elapsed         | 940         |
|    total_timesteps      | 1064960     |
| train/                  |             |
|    approx_kl            | 0.015070406 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.466       |
|    cost_value_loss      | 1.11e-05    |
|    entropy_loss         | -5.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0338      |
|    n_updates            | 1030        |
|    nu                   | 6.03        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000303   |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.0892      |
|    std                  | 0.636       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 853         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 1.94        |
| rollout/                |             |
|    adjusted_reward      | 1.64        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 858         |
| time/                   |             |
|    fps                  | 1130        |
|    iterations           | 105         |
|    time_elapsed         | 951         |
|    total_timesteps      | 1075200     |
| train/                  |             |
|    approx_kl            | 0.017521644 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.355       |
|    cost_value_loss      | 7.18e-06    |
|    entropy_loss         | -5.75       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0369      |
|    n_updates            | 1040        |
|    nu                   | 6.03        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00167    |
|    reward_explained_... | 0.896       |
|    reward_value_loss    | 0.0863      |
|    std                  | 0.634       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 965          |
|    mean_ep_length       | 500          |
|    mean_reward          | 720          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.415       |
|    reward_run           | 2.2          |
| rollout/                |              |
|    adjusted_reward      | 1.74         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 852          |
| time/                   |              |
|    fps                  | 1130         |
|    iterations           | 106          |
|    time_elapsed         | 960          |
|    total_timesteps      | 1085440      |
| train/                  |              |
|    approx_kl            | 0.0150595065 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.195        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.627        |
|    cost_value_loss      | 1.97e-05     |
|    entropy_loss         | -5.73        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0254       |
|    n_updates            | 1050         |
|    nu                   | 6.03         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00103     |
|    reward_explained_... | 0.835        |
|    reward_value_loss    | 0.115        |
|    std                  | 0.631        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 909         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 2.75        |
| rollout/                |             |
|    adjusted_reward      | 1.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 873         |
| time/                   |             |
|    fps                  | 1131        |
|    iterations           | 107         |
|    time_elapsed         | 968         |
|    total_timesteps      | 1095680     |
| train/                  |             |
|    approx_kl            | 0.015385633 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.896       |
|    cost_value_loss      | 3.05e-05    |
|    entropy_loss         | -5.72       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0443      |
|    n_updates            | 1060        |
|    nu                   | 6.04        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000178   |
|    reward_explained_... | 0.861       |
|    reward_value_loss    | 0.114       |
|    std                  | 0.63        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 800         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.41        |
| rollout/                |             |
|    adjusted_reward      | 1.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 872         |
| time/                   |             |
|    fps                  | 1131        |
|    iterations           | 108         |
|    time_elapsed         | 977         |
|    total_timesteps      | 1105920     |
| train/                  |             |
|    approx_kl            | 0.015788855 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.835       |
|    cost_value_loss      | 2.11e-05    |
|    entropy_loss         | -5.71       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0213      |
|    n_updates            | 1070        |
|    nu                   | 6.04        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000127   |
|    reward_explained_... | 0.906       |
|    reward_value_loss    | 0.0894      |
|    std                  | 0.627       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 965         |
|    mean_ep_length       | 500         |
|    mean_reward          | 882         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.61        |
| rollout/                |             |
|    adjusted_reward      | 1.81        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 874         |
| time/                   |             |
|    fps                  | 1130        |
|    iterations           | 109         |
|    time_elapsed         | 987         |
|    total_timesteps      | 1116160     |
| train/                  |             |
|    approx_kl            | 0.016524803 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.857       |
|    cost_value_loss      | 8.51e-06    |
|    entropy_loss         | -5.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0401      |
|    n_updates            | 1080        |
|    nu                   | 6.04        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00154    |
|    reward_explained_... | 0.915       |
|    reward_value_loss    | 0.0788      |
|    std                  | 0.624       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1.02e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 1.02e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    reward_ctrl          | -0.401        |
|    reward_run           | 2.51          |
| rollout/                |               |
|    adjusted_reward      | 1.77          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 887           |
| time/                   |               |
|    fps                  | 1131          |
|    iterations           | 110           |
|    time_elapsed         | 995           |
|    total_timesteps      | 1126400       |
| train/                  |               |
|    approx_kl            | 0.016157476   |
|    average_cost         | 0.00023668687 |
|    clip_fraction        | 0.186         |
|    clip_range           | 0.2           |
|    cost_explained_va... | -3.66         |
|    cost_value_loss      | 0.00104       |
|    entropy_loss         | -5.64         |
|    learning_rate        | 0.0003        |
|    loss                 | 0.0416        |
|    n_updates            | 1090          |
|    nu                   | 6.04          |
|    nu_loss              | -0.00143      |
|    policy_gradient_loss | -0.000136     |
|    reward_explained_... | 0.902         |
|    reward_value_loss    | 0.0936        |
|    std                  | 0.62          |
|    total_cost           | 2.4236736     |
-------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 921         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.389      |
|    reward_run           | 2.2         |
| rollout/                |             |
|    adjusted_reward      | 1.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 898         |
| time/                   |             |
|    fps                  | 1131        |
|    iterations           | 111         |
|    time_elapsed         | 1004        |
|    total_timesteps      | 1136640     |
| train/                  |             |
|    approx_kl            | 0.016450005 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.635       |
|    cost_value_loss      | 7.39e-06    |
|    entropy_loss         | -5.61       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0403      |
|    n_updates            | 1100        |
|    nu                   | 6.05        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000565   |
|    reward_explained_... | 0.87        |
|    reward_value_loss    | 0.11        |
|    std                  | 0.617       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 900         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.02        |
| rollout/                |             |
|    adjusted_reward      | 1.86        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 911         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 112         |
|    time_elapsed         | 1010        |
|    total_timesteps      | 1146880     |
| train/                  |             |
|    approx_kl            | 0.015715864 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.682       |
|    cost_value_loss      | 2.98e-06    |
|    entropy_loss         | -5.59       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0476      |
|    n_updates            | 1110        |
|    nu                   | 6.05        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000776   |
|    reward_explained_... | 0.811       |
|    reward_value_loss    | 0.127       |
|    std                  | 0.617       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 980         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 2.4         |
| rollout/                |             |
|    adjusted_reward      | 1.91        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 918         |
| time/                   |             |
|    fps                  | 1135        |
|    iterations           | 113         |
|    time_elapsed         | 1018        |
|    total_timesteps      | 1157120     |
| train/                  |             |
|    approx_kl            | 0.015006687 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.172       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.525       |
|    cost_value_loss      | 6.06e-06    |
|    entropy_loss         | -5.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0396      |
|    n_updates            | 1120        |
|    nu                   | 6.05        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -8.04e-05   |
|    reward_explained_... | 0.871       |
|    reward_value_loss    | 0.092       |
|    std                  | 0.614       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 972         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.393      |
|    reward_run           | 2.38        |
| rollout/                |             |
|    adjusted_reward      | 1.83        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 926         |
| time/                   |             |
|    fps                  | 1136        |
|    iterations           | 114         |
|    time_elapsed         | 1027        |
|    total_timesteps      | 1167360     |
| train/                  |             |
|    approx_kl            | 0.018187407 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.512       |
|    cost_value_loss      | 3.51e-06    |
|    entropy_loss         | -5.55       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0793      |
|    n_updates            | 1130        |
|    nu                   | 6.05        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000662   |
|    reward_explained_... | 0.865       |
|    reward_value_loss    | 0.113       |
|    std                  | 0.613       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.02e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 942        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.388     |
|    reward_run           | 1.37       |
| rollout/                |            |
|    adjusted_reward      | 1.85       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 927        |
| time/                   |            |
|    fps                  | 1136       |
|    iterations           | 115        |
|    time_elapsed         | 1036       |
|    total_timesteps      | 1177600    |
| train/                  |            |
|    approx_kl            | 0.01557402 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.513      |
|    cost_value_loss      | 5.1e-06    |
|    entropy_loss         | -5.55      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0391     |
|    n_updates            | 1140       |
|    nu                   | 6.05       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000208  |
|    reward_explained_... | 0.86       |
|    reward_value_loss    | 0.11       |
|    std                  | 0.613      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 916         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 2.43        |
| rollout/                |             |
|    adjusted_reward      | 1.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 940         |
| time/                   |             |
|    fps                  | 1137        |
|    iterations           | 116         |
|    time_elapsed         | 1044        |
|    total_timesteps      | 1187840     |
| train/                  |             |
|    approx_kl            | 0.017871935 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.455       |
|    cost_value_loss      | 6.51e-06    |
|    entropy_loss         | -5.54       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0632      |
|    n_updates            | 1150        |
|    nu                   | 6.06        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000501   |
|    reward_explained_... | 0.77        |
|    reward_value_loss    | 0.155       |
|    std                  | 0.611       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 895         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 2.28        |
| rollout/                |             |
|    adjusted_reward      | 1.83        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 936         |
| time/                   |             |
|    fps                  | 1140        |
|    iterations           | 117         |
|    time_elapsed         | 1050        |
|    total_timesteps      | 1198080     |
| train/                  |             |
|    approx_kl            | 0.015834132 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.551       |
|    cost_value_loss      | 5.27e-06    |
|    entropy_loss         | -5.53       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0404      |
|    n_updates            | 1160        |
|    nu                   | 6.06        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00011    |
|    reward_explained_... | 0.89        |
|    reward_value_loss    | 0.0945      |
|    std                  | 0.61        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.02e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 989          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.401       |
|    reward_run           | 2.71         |
| rollout/                |              |
|    adjusted_reward      | 1.86         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 932          |
| time/                   |              |
|    fps                  | 1142         |
|    iterations           | 118          |
|    time_elapsed         | 1057         |
|    total_timesteps      | 1208320      |
| train/                  |              |
|    approx_kl            | 0.017542329  |
|    average_cost         | 0.0011357629 |
|    clip_fraction        | 0.237        |
|    clip_range           | 0.2          |
|    cost_explained_va... | -1.28        |
|    cost_value_loss      | 0.00464      |
|    entropy_loss         | -5.53        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0812       |
|    n_updates            | 1170         |
|    nu                   | 6.06         |
|    nu_loss              | -0.00688     |
|    policy_gradient_loss | 0.00136      |
|    reward_explained_... | 0.891        |
|    reward_value_loss    | 0.102        |
|    std                  | 0.61         |
|    total_cost           | 11.630212    |
------------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 786         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 2.17        |
| rollout/                |             |
|    adjusted_reward      | 1.88        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 937         |
| time/                   |             |
|    fps                  | 1142        |
|    iterations           | 119         |
|    time_elapsed         | 1066        |
|    total_timesteps      | 1218560     |
| train/                  |             |
|    approx_kl            | 0.015332577 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.424       |
|    cost_value_loss      | 2.66e-05    |
|    entropy_loss         | -5.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0412      |
|    n_updates            | 1180        |
|    nu                   | 6.07        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000768   |
|    reward_explained_... | 0.902       |
|    reward_value_loss    | 0.0909      |
|    std                  | 0.607       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 869         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.388      |
|    reward_run           | 2.44        |
| rollout/                |             |
|    adjusted_reward      | 2.02        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 953         |
| time/                   |             |
|    fps                  | 1142        |
|    iterations           | 120         |
|    time_elapsed         | 1075        |
|    total_timesteps      | 1228800     |
| train/                  |             |
|    approx_kl            | 0.015268093 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.616       |
|    cost_value_loss      | 5.96e-06    |
|    entropy_loss         | -5.49       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 1190        |
|    nu                   | 6.07        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000558   |
|    reward_explained_... | 0.868       |
|    reward_value_loss    | 0.0972      |
|    std                  | 0.606       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 869         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.404      |
|    reward_run           | 2.52        |
| rollout/                |             |
|    adjusted_reward      | 1.84        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 938         |
| time/                   |             |
|    fps                  | 1142        |
|    iterations           | 121         |
|    time_elapsed         | 1084        |
|    total_timesteps      | 1239040     |
| train/                  |             |
|    approx_kl            | 0.018168185 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.74        |
|    cost_value_loss      | 1.25e-05    |
|    entropy_loss         | -5.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0391      |
|    n_updates            | 1200        |
|    nu                   | 6.07        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000926   |
|    reward_explained_... | 0.901       |
|    reward_value_loss    | 0.0957      |
|    std                  | 0.606       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.02e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 991          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.392       |
|    reward_run           | 2.32         |
| rollout/                |              |
|    adjusted_reward      | 1.97         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 952          |
| time/                   |              |
|    fps                  | 1143         |
|    iterations           | 122          |
|    time_elapsed         | 1092         |
|    total_timesteps      | 1249280      |
| train/                  |              |
|    approx_kl            | 0.01815541   |
|    average_cost         | 0.0011557408 |
|    clip_fraction        | 0.21         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.641        |
|    cost_value_loss      | 0.003        |
|    entropy_loss         | -5.48        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0981       |
|    n_updates            | 1210         |
|    nu                   | 6.08         |
|    nu_loss              | -0.00702     |
|    policy_gradient_loss | 7.99e-05     |
|    reward_explained_... | 0.882        |
|    reward_value_loss    | 0.101        |
|    std                  | 0.605        |
|    total_cost           | 11.834785    |
------------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.02e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 988         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 2.12        |
| rollout/                |             |
|    adjusted_reward      | 1.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 954         |
| time/                   |             |
|    fps                  | 1142        |
|    iterations           | 123         |
|    time_elapsed         | 1102        |
|    total_timesteps      | 1259520     |
| train/                  |             |
|    approx_kl            | 0.015437472 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.688       |
|    cost_value_loss      | 6.82e-05    |
|    entropy_loss         | -5.45       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0632      |
|    n_updates            | 1220        |
|    nu                   | 6.08        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00144    |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.093       |
|    std                  | 0.602       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.02e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 860          |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.371       |
|    reward_run           | 2.21         |
| rollout/                |              |
|    adjusted_reward      | 1.85         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 940          |
| time/                   |              |
|    fps                  | 1142         |
|    iterations           | 124          |
|    time_elapsed         | 1111         |
|    total_timesteps      | 1269760      |
| train/                  |              |
|    approx_kl            | 0.02386902   |
|    average_cost         | 0.0020421804 |
|    clip_fraction        | 0.24         |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.853        |
|    cost_value_loss      | 0.00663      |
|    entropy_loss         | -5.44        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0451       |
|    n_updates            | 1230         |
|    nu                   | 6.1          |
|    nu_loss              | -0.0124      |
|    policy_gradient_loss | -0.000391    |
|    reward_explained_... | 0.876        |
|    reward_value_loss    | 0.114        |
|    std                  | 0.601        |
|    total_cost           | 20.911928    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.08e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.381      |
|    reward_run           | 2.32        |
| rollout/                |             |
|    adjusted_reward      | 2.07        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 953         |
| time/                   |             |
|    fps                  | 1143        |
|    iterations           | 125         |
|    time_elapsed         | 1119        |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.015767032 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.762       |
|    cost_value_loss      | 0.00189     |
|    entropy_loss         | -5.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0629      |
|    n_updates            | 1240        |
|    nu                   | 6.11        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000191   |
|    reward_explained_... | 0.82        |
|    reward_value_loss    | 0.127       |
|    std                  | 0.599       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.08e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.04e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.401      |
|    reward_run           | 2.17        |
| rollout/                |             |
|    adjusted_reward      | 2.04        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 976         |
| time/                   |             |
|    fps                  | 1144        |
|    iterations           | 126         |
|    time_elapsed         | 1127        |
|    total_timesteps      | 1290240     |
| train/                  |             |
|    approx_kl            | 0.015410599 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.168       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.729       |
|    cost_value_loss      | 9.44e-05    |
|    entropy_loss         | -5.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.045       |
|    n_updates            | 1250        |
|    nu                   | 6.11        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 5.11e-05    |
|    reward_explained_... | 0.874       |
|    reward_value_loss    | 0.108       |
|    std                  | 0.6         |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.08e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 899         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.402      |
|    reward_run           | 2.7         |
| rollout/                |             |
|    adjusted_reward      | 2.04        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 977         |
| time/                   |             |
|    fps                  | 1144        |
|    iterations           | 127         |
|    time_elapsed         | 1136        |
|    total_timesteps      | 1300480     |
| train/                  |             |
|    approx_kl            | 0.015033868 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.243       |
|    cost_value_loss      | 1.04e-05    |
|    entropy_loss         | -5.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0391      |
|    n_updates            | 1260        |
|    nu                   | 6.12        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00075    |
|    reward_explained_... | 0.851       |
|    reward_value_loss    | 0.0957      |
|    std                  | 0.598       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.08e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 929         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 2.15        |
| rollout/                |             |
|    adjusted_reward      | 1.9         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 989         |
| time/                   |             |
|    fps                  | 1145        |
|    iterations           | 128         |
|    time_elapsed         | 1144        |
|    total_timesteps      | 1310720     |
| train/                  |             |
|    approx_kl            | 0.015805446 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.236       |
|    cost_value_loss      | 1.38e-05    |
|    entropy_loss         | -5.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0366      |
|    n_updates            | 1270        |
|    nu                   | 6.13        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -9.69e-05   |
|    reward_explained_... | 0.895       |
|    reward_value_loss    | 0.0975      |
|    std                  | 0.596       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.1e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 2.49        |
| rollout/                |             |
|    adjusted_reward      | 1.91        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 995         |
| time/                   |             |
|    fps                  | 1146        |
|    iterations           | 129         |
|    time_elapsed         | 1152        |
|    total_timesteps      | 1320960     |
| train/                  |             |
|    approx_kl            | 0.015775532 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.7         |
|    cost_value_loss      | 2.9e-05     |
|    entropy_loss         | -5.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0803      |
|    n_updates            | 1280        |
|    nu                   | 6.14        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -2.86e-05   |
|    reward_explained_... | 0.864       |
|    reward_value_loss    | 0.111       |
|    std                  | 0.595       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.1e+03    |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.03e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.397     |
|    reward_run           | 2.68       |
| rollout/                |            |
|    adjusted_reward      | 2.01       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 984        |
| time/                   |            |
|    fps                  | 1145       |
|    iterations           | 130        |
|    time_elapsed         | 1161       |
|    total_timesteps      | 1331200    |
| train/                  |            |
|    approx_kl            | 0.01614796 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.697      |
|    cost_value_loss      | 2.03e-05   |
|    entropy_loss         | -5.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.03       |
|    n_updates            | 1290       |
|    nu                   | 6.14       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000624  |
|    reward_explained_... | 0.888      |
|    reward_value_loss    | 0.0962     |
|    std                  | 0.595      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 1.07e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.401       |
|    reward_run           | 2.4          |
| rollout/                |              |
|    adjusted_reward      | 2.15         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 994          |
| time/                   |              |
|    fps                  | 1146         |
|    iterations           | 131          |
|    time_elapsed         | 1169         |
|    total_timesteps      | 1341440      |
| train/                  |              |
|    approx_kl            | 0.016105423  |
|    average_cost         | 0.0002735427 |
|    clip_fraction        | 0.176        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.737        |
|    cost_value_loss      | 0.00461      |
|    entropy_loss         | -5.37        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.037        |
|    n_updates            | 1300         |
|    nu                   | 6.15         |
|    nu_loss              | -0.00168     |
|    policy_gradient_loss | 0.000573     |
|    reward_explained_... | 0.888        |
|    reward_value_loss    | 0.0987       |
|    std                  | 0.595        |
|    total_cost           | 2.8010774    |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.01e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 2.01        |
| rollout/                |             |
|    adjusted_reward      | 2.02        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 997         |
| time/                   |             |
|    fps                  | 1147        |
|    iterations           | 132         |
|    time_elapsed         | 1177        |
|    total_timesteps      | 1351680     |
| train/                  |             |
|    approx_kl            | 0.015511459 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.376       |
|    cost_value_loss      | 2.65e-05    |
|    entropy_loss         | -5.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 1310        |
|    nu                   | 6.15        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 8.67e-05    |
|    reward_explained_... | 0.91        |
|    reward_value_loss    | 0.0697      |
|    std                  | 0.594       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 955         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 2.68        |
| rollout/                |             |
|    adjusted_reward      | 2.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.01e+03    |
| time/                   |             |
|    fps                  | 1148        |
|    iterations           | 133         |
|    time_elapsed         | 1186        |
|    total_timesteps      | 1361920     |
| train/                  |             |
|    approx_kl            | 0.016460832 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.184       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.604       |
|    cost_value_loss      | 2.58e-05    |
|    entropy_loss         | -5.37       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0376      |
|    n_updates            | 1320        |
|    nu                   | 6.16        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000216   |
|    reward_explained_... | 0.883       |
|    reward_value_loss    | 0.0932      |
|    std                  | 0.594       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.1e+03    |
|    mean_ep_length       | 500        |
|    mean_reward          | 983        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.386     |
|    reward_run           | 2.47       |
| rollout/                |            |
|    adjusted_reward      | 2.07       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.03e+03   |
| time/                   |            |
|    fps                  | 1147       |
|    iterations           | 134        |
|    time_elapsed         | 1195       |
|    total_timesteps      | 1372160    |
| train/                  |            |
|    approx_kl            | 0.01617818 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.705      |
|    cost_value_loss      | 9.73e-06   |
|    entropy_loss         | -5.36      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0291     |
|    n_updates            | 1330       |
|    nu                   | 6.16       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000886  |
|    reward_explained_... | 0.904      |
|    reward_value_loss    | 0.0715     |
|    std                  | 0.593      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.12e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.379      |
|    reward_run           | 2.3         |
| rollout/                |             |
|    adjusted_reward      | 1.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 1148        |
|    iterations           | 135         |
|    time_elapsed         | 1204        |
|    total_timesteps      | 1382400     |
| train/                  |             |
|    approx_kl            | 0.018293604 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.743       |
|    cost_value_loss      | 0.000385    |
|    entropy_loss         | -5.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.027       |
|    n_updates            | 1340        |
|    nu                   | 6.17        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000101   |
|    reward_explained_... | 0.915       |
|    reward_value_loss    | 0.0887      |
|    std                  | 0.59        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 857         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 1.75        |
| rollout/                |             |
|    adjusted_reward      | 2.1         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.03e+03    |
| time/                   |             |
|    fps                  | 1148        |
|    iterations           | 136         |
|    time_elapsed         | 1213        |
|    total_timesteps      | 1392640     |
| train/                  |             |
|    approx_kl            | 0.016550226 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.627       |
|    cost_value_loss      | 1.5e-05     |
|    entropy_loss         | -5.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0455      |
|    n_updates            | 1350        |
|    nu                   | 6.17        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000283   |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.0895      |
|    std                  | 0.588       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 975         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.401      |
|    reward_run           | 2.63        |
| rollout/                |             |
|    adjusted_reward      | 2.17        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 1148        |
|    iterations           | 137         |
|    time_elapsed         | 1221        |
|    total_timesteps      | 1402880     |
| train/                  |             |
|    approx_kl            | 0.015325436 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.188       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.499       |
|    cost_value_loss      | 1.02e-05    |
|    entropy_loss         | -5.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0358      |
|    n_updates            | 1360        |
|    nu                   | 6.17        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000163   |
|    reward_explained_... | 0.89        |
|    reward_value_loss    | 0.085       |
|    std                  | 0.588       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 902         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.387      |
|    reward_run           | 2.6         |
| rollout/                |             |
|    adjusted_reward      | 2.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 138         |
|    time_elapsed         | 1226        |
|    total_timesteps      | 1413120     |
| train/                  |             |
|    approx_kl            | 0.018338872 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.631       |
|    cost_value_loss      | 7.2e-06     |
|    entropy_loss         | -5.29       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0201      |
|    n_updates            | 1370        |
|    nu                   | 6.18        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000186    |
|    reward_explained_... | 0.922       |
|    reward_value_loss    | 0.0672      |
|    std                  | 0.586       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 997         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 2.76        |
| rollout/                |             |
|    adjusted_reward      | 2.13        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.05e+03    |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 139         |
|    time_elapsed         | 1235        |
|    total_timesteps      | 1423360     |
| train/                  |             |
|    approx_kl            | 0.016623737 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.66        |
|    cost_value_loss      | 1.92e-05    |
|    entropy_loss         | -5.28       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 1380        |
|    nu                   | 6.18        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 9.39e-05    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0786      |
|    std                  | 0.584       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.12e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.11e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 2.75        |
| rollout/                |             |
|    adjusted_reward      | 2.21        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 1151        |
|    iterations           | 140         |
|    time_elapsed         | 1244        |
|    total_timesteps      | 1433600     |
| train/                  |             |
|    approx_kl            | 0.015400526 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.846       |
|    cost_value_loss      | 5.54e-05    |
|    entropy_loss         | -5.24       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 1390        |
|    nu                   | 6.18        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000992   |
|    reward_explained_... | 0.895       |
|    reward_value_loss    | 0.0943      |
|    std                  | 0.581       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.22e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.392      |
|    reward_run           | 2.37        |
| rollout/                |             |
|    adjusted_reward      | 2.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 1151        |
|    iterations           | 141         |
|    time_elapsed         | 1253        |
|    total_timesteps      | 1443840     |
| train/                  |             |
|    approx_kl            | 0.015835099 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.267       |
|    cost_value_loss      | 6.41e-06    |
|    entropy_loss         | -5.21       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0195      |
|    n_updates            | 1400        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000238   |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0799      |
|    std                  | 0.579       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 883         |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.385      |
|    reward_run           | 2.33        |
| rollout/                |             |
|    adjusted_reward      | 2.19        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 1151        |
|    iterations           | 142         |
|    time_elapsed         | 1262        |
|    total_timesteps      | 1454080     |
| train/                  |             |
|    approx_kl            | 0.015688073 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.288       |
|    cost_value_loss      | 7.98e-06    |
|    entropy_loss         | -5.2        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0312      |
|    n_updates            | 1410        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00106    |
|    reward_explained_... | 0.897       |
|    reward_value_loss    | 0.087       |
|    std                  | 0.579       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.22e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 890        |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.389     |
|    reward_run           | 3.25       |
| rollout/                |            |
|    adjusted_reward      | 2.25       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.09e+03   |
| time/                   |            |
|    fps                  | 1152       |
|    iterations           | 143        |
|    time_elapsed         | 1270       |
|    total_timesteps      | 1464320    |
| train/                  |            |
|    approx_kl            | 0.01552296 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.38       |
|    cost_value_loss      | 6.81e-06   |
|    entropy_loss         | -5.2       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 1420       |
|    nu                   | 6.19       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000174  |
|    reward_explained_... | 0.875      |
|    reward_value_loss    | 0.0915     |
|    std                  | 0.578      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.13e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.372      |
|    reward_run           | 2.73        |
| rollout/                |             |
|    adjusted_reward      | 2.08        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.09e+03    |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 144         |
|    time_elapsed         | 1279        |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.017245641 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.497       |
|    cost_value_loss      | 5.17e-06    |
|    entropy_loss         | -5.18       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0393      |
|    n_updates            | 1430        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000707   |
|    reward_explained_... | 0.91        |
|    reward_value_loss    | 0.081       |
|    std                  | 0.575       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.04e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.377      |
|    reward_run           | 2.5         |
| rollout/                |             |
|    adjusted_reward      | 2.28        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.09e+03    |
| time/                   |             |
|    fps                  | 1151        |
|    iterations           | 145         |
|    time_elapsed         | 1288        |
|    total_timesteps      | 1484800     |
| train/                  |             |
|    approx_kl            | 0.018126708 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.607       |
|    cost_value_loss      | 6.52e-06    |
|    entropy_loss         | -5.17       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0179      |
|    n_updates            | 1440        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00101    |
|    reward_explained_... | 0.896       |
|    reward_value_loss    | 0.0846      |
|    std                  | 0.575       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 4 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.13e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.372      |
|    reward_run           | 1.71        |
| rollout/                |             |
|    adjusted_reward      | 2.31        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.11e+03    |
| time/                   |             |
|    fps                  | 1151        |
|    iterations           | 146         |
|    time_elapsed         | 1298        |
|    total_timesteps      | 1495040     |
| train/                  |             |
|    approx_kl            | 0.015770834 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.464       |
|    cost_value_loss      | 2.47e-06    |
|    entropy_loss         | -5.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0382      |
|    n_updates            | 1450        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00137    |
|    reward_explained_... | 0.892       |
|    reward_value_loss    | 0.0843      |
|    std                  | 0.572       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.12e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 2.27        |
| rollout/                |             |
|    adjusted_reward      | 2.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.1e+03     |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 147         |
|    time_elapsed         | 1306        |
|    total_timesteps      | 1505280     |
| train/                  |             |
|    approx_kl            | 0.016142337 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.145       |
|    cost_value_loss      | 1.82e-06    |
|    entropy_loss         | -5.12       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0281      |
|    n_updates            | 1460        |
|    nu                   | 6.19        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 7.77e-07    |
|    reward_explained_... | 0.921       |
|    reward_value_loss    | 0.0784      |
|    std                  | 0.569       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.402      |
|    reward_run           | 2.75        |
| rollout/                |             |
|    adjusted_reward      | 2.33        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.11e+03    |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 148         |
|    time_elapsed         | 1315        |
|    total_timesteps      | 1515520     |
| train/                  |             |
|    approx_kl            | 0.015301351 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.897       |
|    cost_value_loss      | 5.01e-05    |
|    entropy_loss         | -5.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0311      |
|    n_updates            | 1470        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -7.05e-05   |
|    reward_explained_... | 0.905       |
|    reward_value_loss    | 0.0882      |
|    std                  | 0.568       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.82        |
| rollout/                |             |
|    adjusted_reward      | 2.08        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.11e+03    |
| time/                   |             |
|    fps                  | 1152        |
|    iterations           | 149         |
|    time_elapsed         | 1323        |
|    total_timesteps      | 1525760     |
| train/                  |             |
|    approx_kl            | 0.015359606 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.858       |
|    cost_value_loss      | 2.13e-05    |
|    entropy_loss         | -5.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0537      |
|    n_updates            | 1480        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00034    |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0655      |
|    std                  | 0.567       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.03e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.8         |
| rollout/                |             |
|    adjusted_reward      | 2           |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 1155        |
|    iterations           | 150         |
|    time_elapsed         | 1329        |
|    total_timesteps      | 1536000     |
| train/                  |             |
|    approx_kl            | 0.015226973 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.392       |
|    cost_value_loss      | 4.18e-06    |
|    entropy_loss         | -5.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0803      |
|    n_updates            | 1490        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000241    |
|    reward_explained_... | 0.856       |
|    reward_value_loss    | 0.124       |
|    std                  | 0.565       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.22e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.24        |
| rollout/                |             |
|    adjusted_reward      | 2.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.06e+03    |
| time/                   |             |
|    fps                  | 1158        |
|    iterations           | 151         |
|    time_elapsed         | 1334        |
|    total_timesteps      | 1546240     |
| train/                  |             |
|    approx_kl            | 0.015172182 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.849       |
|    cost_value_loss      | 8.36e-06    |
|    entropy_loss         | -5.04       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 1500        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000217   |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0828      |
|    std                  | 0.563       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.23e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.386      |
|    reward_run           | 2.65        |
| rollout/                |             |
|    adjusted_reward      | 2.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.07e+03    |
| time/                   |             |
|    fps                  | 1159        |
|    iterations           | 152         |
|    time_elapsed         | 1342        |
|    total_timesteps      | 1556480     |
| train/                  |             |
|    approx_kl            | 0.015377002 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.514       |
|    cost_value_loss      | 3.77e-06    |
|    entropy_loss         | -5.03       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.033       |
|    n_updates            | 1510        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000658   |
|    reward_explained_... | 0.903       |
|    reward_value_loss    | 0.0886      |
|    std                  | 0.56        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 5 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.384      |
|    reward_run           | 1.76        |
| rollout/                |             |
|    adjusted_reward      | 2.03        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.04e+03    |
| time/                   |             |
|    fps                  | 1158        |
|    iterations           | 153         |
|    time_elapsed         | 1352        |
|    total_timesteps      | 1566720     |
| train/                  |             |
|    approx_kl            | 0.018239517 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.479       |
|    cost_value_loss      | 2.93e-06    |
|    entropy_loss         | -5.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0224      |
|    n_updates            | 1520        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00166    |
|    reward_explained_... | 0.899       |
|    reward_value_loss    | 0.0864      |
|    std                  | 0.562       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.21e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.406      |
|    reward_run           | 2.39        |
| rollout/                |             |
|    adjusted_reward      | 2.23        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.05e+03    |
| time/                   |             |
|    fps                  | 1158        |
|    iterations           | 154         |
|    time_elapsed         | 1361        |
|    total_timesteps      | 1576960     |
| train/                  |             |
|    approx_kl            | 0.015519333 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.43        |
|    cost_value_loss      | 5.44e-06    |
|    entropy_loss         | -5.03       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0538      |
|    n_updates            | 1530        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000105   |
|    reward_explained_... | 0.891       |
|    reward_value_loss    | 0.0918      |
|    std                  | 0.562       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.23e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 1.12e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    reward_ctrl          | -0.396       |
|    reward_run           | 2.14         |
| rollout/                |              |
|    adjusted_reward      | 2.22         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 1.08e+03     |
| time/                   |              |
|    fps                  | 1159         |
|    iterations           | 155          |
|    time_elapsed         | 1368         |
|    total_timesteps      | 1587200      |
| train/                  |              |
|    approx_kl            | 0.0154043045 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.193        |
|    clip_range           | 0.2          |
|    cost_explained_va... | 0.55         |
|    cost_value_loss      | 1.98e-06     |
|    entropy_loss         | -5.02        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.061        |
|    n_updates            | 1540         |
|    nu                   | 6.2          |
|    nu_loss              | -0           |
|    policy_gradient_loss | 0.00109      |
|    reward_explained_... | 0.885        |
|    reward_value_loss    | 0.101        |
|    std                  | 0.562        |
|    total_cost           | 0.0          |
------------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.06e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.399     |
|    reward_run           | 2.91       |
| rollout/                |            |
|    adjusted_reward      | 2.28       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.09e+03   |
| time/                   |            |
|    fps                  | 1160       |
|    iterations           | 156        |
|    time_elapsed         | 1376       |
|    total_timesteps      | 1597440    |
| train/                  |            |
|    approx_kl            | 0.01653721 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.63       |
|    cost_value_loss      | 8.13e-06   |
|    entropy_loss         | -5.02      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0247     |
|    n_updates            | 1550       |
|    nu                   | 6.2        |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000277  |
|    reward_explained_... | 0.893      |
|    reward_value_loss    | 0.081      |
|    std                  | 0.561      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.09e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.401     |
|    reward_run           | 2.62       |
| rollout/                |            |
|    adjusted_reward      | 2.15       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.1e+03    |
| time/                   |            |
|    fps                  | 1160       |
|    iterations           | 157        |
|    time_elapsed         | 1385       |
|    total_timesteps      | 1607680    |
| train/                  |            |
|    approx_kl            | 0.01611108 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.883      |
|    cost_value_loss      | 7.56e-06   |
|    entropy_loss         | -5.01      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0145     |
|    n_updates            | 1560       |
|    nu                   | 6.2        |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000487  |
|    reward_explained_... | 0.923      |
|    reward_value_loss    | 0.0695     |
|    std                  | 0.559      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.09e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.391      |
|    reward_run           | 2.6         |
| rollout/                |             |
|    adjusted_reward      | 2.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.1e+03     |
| time/                   |             |
|    fps                  | 1160        |
|    iterations           | 158         |
|    time_elapsed         | 1394        |
|    total_timesteps      | 1617920     |
| train/                  |             |
|    approx_kl            | 0.017569885 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.194       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.687       |
|    cost_value_loss      | 2.4e-06     |
|    entropy_loss         | -4.99       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0694      |
|    n_updates            | 1570        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 8.48e-06    |
|    reward_explained_... | 0.857       |
|    reward_value_loss    | 0.105       |
|    std                  | 0.558       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.399      |
|    reward_run           | 2.65        |
| rollout/                |             |
|    adjusted_reward      | 2.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 1160        |
|    iterations           | 159         |
|    time_elapsed         | 1402        |
|    total_timesteps      | 1628160     |
| train/                  |             |
|    approx_kl            | 0.018176641 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.585       |
|    cost_value_loss      | 1.66e-06    |
|    entropy_loss         | -4.97       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0486      |
|    n_updates            | 1580        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000152   |
|    reward_explained_... | 0.857       |
|    reward_value_loss    | 0.0989      |
|    std                  | 0.556       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.1e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.38       |
|    reward_run           | 2.05        |
| rollout/                |             |
|    adjusted_reward      | 2.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 1160        |
|    iterations           | 160         |
|    time_elapsed         | 1411        |
|    total_timesteps      | 1638400     |
| train/                  |             |
|    approx_kl            | 0.019078832 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.531       |
|    cost_value_loss      | 4.38e-06    |
|    entropy_loss         | -4.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 1590        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000133    |
|    reward_explained_... | 0.859       |
|    reward_value_loss    | 0.11        |
|    std                  | 0.553       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.15e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 3.3         |
| rollout/                |             |
|    adjusted_reward      | 2.3         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.08e+03    |
| time/                   |             |
|    fps                  | 1161        |
|    iterations           | 161         |
|    time_elapsed         | 1419        |
|    total_timesteps      | 1648640     |
| train/                  |             |
|    approx_kl            | 0.017177474 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.482       |
|    cost_value_loss      | 3.18e-06    |
|    entropy_loss         | -4.92       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0198      |
|    n_updates            | 1600        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 3.39e-06    |
|    reward_explained_... | 0.911       |
|    reward_value_loss    | 0.0715      |
|    std                  | 0.551       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.388      |
|    reward_run           | 2.58        |
| rollout/                |             |
|    adjusted_reward      | 2.27        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.09e+03    |
| time/                   |             |
|    fps                  | 1161        |
|    iterations           | 162         |
|    time_elapsed         | 1428        |
|    total_timesteps      | 1658880     |
| train/                  |             |
|    approx_kl            | 0.015617532 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.308       |
|    cost_value_loss      | 1.48e-06    |
|    entropy_loss         | -4.89       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0198      |
|    n_updates            | 1610        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000714   |
|    reward_explained_... | 0.878       |
|    reward_value_loss    | 0.0878      |
|    std                  | 0.548       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.01e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.77        |
| rollout/                |             |
|    adjusted_reward      | 2.29        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.1e+03     |
| time/                   |             |
|    fps                  | 1161        |
|    iterations           | 163         |
|    time_elapsed         | 1436        |
|    total_timesteps      | 1669120     |
| train/                  |             |
|    approx_kl            | 0.016517797 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.325       |
|    cost_value_loss      | 1.74e-06    |
|    entropy_loss         | -4.86       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 1620        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00021    |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0691      |
|    std                  | 0.546       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.12e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.386      |
|    reward_run           | 2.39        |
| rollout/                |             |
|    adjusted_reward      | 2.29        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.13e+03    |
| time/                   |             |
|    fps                  | 1161        |
|    iterations           | 164         |
|    time_elapsed         | 1445        |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.015119356 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.203       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.725       |
|    cost_value_loss      | 3.31e-06    |
|    entropy_loss         | -4.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0224      |
|    n_updates            | 1630        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000345   |
|    reward_explained_... | 0.886       |
|    reward_value_loss    | 0.0959      |
|    std                  | 0.545       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.2e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.384      |
|    reward_run           | 2.16        |
| rollout/                |             |
|    adjusted_reward      | 2.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.13e+03    |
| time/                   |             |
|    fps                  | 1162        |
|    iterations           | 165         |
|    time_elapsed         | 1453        |
|    total_timesteps      | 1689600     |
| train/                  |             |
|    approx_kl            | 0.016636122 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.73        |
|    cost_value_loss      | 4.89e-06    |
|    entropy_loss         | -4.83       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0272      |
|    n_updates            | 1640        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -3.94e-05   |
|    reward_explained_... | 0.901       |
|    reward_value_loss    | 0.0887      |
|    std                  | 0.543       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.03e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.39      |
|    reward_run           | 2.29       |
| rollout/                |            |
|    adjusted_reward      | 2.31       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.14e+03   |
| time/                   |            |
|    fps                  | 1163       |
|    iterations           | 166        |
|    time_elapsed         | 1461       |
|    total_timesteps      | 1699840    |
| train/                  |            |
|    approx_kl            | 0.01538603 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.452      |
|    cost_value_loss      | 6.07e-06   |
|    entropy_loss         | -4.82      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0245     |
|    n_updates            | 1650       |
|    nu                   | 6.2        |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000567   |
|    reward_explained_... | 0.892      |
|    reward_value_loss    | 0.101      |
|    std                  | 0.543      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.09e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.386      |
|    reward_run           | 1.6         |
| rollout/                |             |
|    adjusted_reward      | 2.35        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.15e+03    |
| time/                   |             |
|    fps                  | 1163        |
|    iterations           | 167         |
|    time_elapsed         | 1469        |
|    total_timesteps      | 1710080     |
| train/                  |             |
|    approx_kl            | 0.018788233 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.545       |
|    cost_value_loss      | 2.41e-06    |
|    entropy_loss         | -4.79       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0465      |
|    n_updates            | 1660        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000152    |
|    reward_explained_... | 0.911       |
|    reward_value_loss    | 0.0751      |
|    std                  | 0.54        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.23e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.397      |
|    reward_run           | 3.25        |
| rollout/                |             |
|    adjusted_reward      | 2.33        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.15e+03    |
| time/                   |             |
|    fps                  | 1165        |
|    iterations           | 168         |
|    time_elapsed         | 1475        |
|    total_timesteps      | 1720320     |
| train/                  |             |
|    approx_kl            | 0.016950732 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.654       |
|    cost_value_loss      | 1.88e-06    |
|    entropy_loss         | -4.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0229      |
|    n_updates            | 1670        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | -4.27e-05   |
|    reward_explained_... | 0.903       |
|    reward_value_loss    | 0.0873      |
|    std                  | 0.539       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.06e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.387      |
|    reward_run           | 3.13        |
| rollout/                |             |
|    adjusted_reward      | 2.24        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.15e+03    |
| time/                   |             |
|    fps                  | 1168        |
|    iterations           | 169         |
|    time_elapsed         | 1481        |
|    total_timesteps      | 1730560     |
| train/                  |             |
|    approx_kl            | 0.016272934 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.847       |
|    cost_value_loss      | 3.36e-06    |
|    entropy_loss         | -4.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 1680        |
|    nu                   | 6.2         |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000255    |
|    reward_explained_... | 0.887       |
|    reward_value_loss    | 0.0884      |
|    std                  | 0.538       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.01e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.392      |
|    reward_run           | 3.13        |
| rollout/                |             |
|    adjusted_reward      | 2.35        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.16e+03    |
| time/                   |             |
|    fps                  | 1168        |
|    iterations           | 170         |
|    time_elapsed         | 1489        |
|    total_timesteps      | 1740800     |
| train/                  |             |
|    approx_kl            | 0.016024496 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.61        |
|    cost_value_loss      | 1.7e-06     |
|    entropy_loss         | -4.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0341      |
|    n_updates            | 1690        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 8.83e-06    |
|    reward_explained_... | 0.875       |
|    reward_value_loss    | 0.0946      |
|    std                  | 0.536       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.09e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.389     |
|    reward_run           | 3.04       |
| rollout/                |            |
|    adjusted_reward      | 2.38       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.16e+03   |
| time/                   |            |
|    fps                  | 1169       |
|    iterations           | 171        |
|    time_elapsed         | 1497       |
|    total_timesteps      | 1751040    |
| train/                  |            |
|    approx_kl            | 0.01571691 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.807      |
|    cost_value_loss      | 2.11e-05   |
|    entropy_loss         | -4.72      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0468     |
|    n_updates            | 1700       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000589   |
|    reward_explained_... | 0.927      |
|    reward_value_loss    | 0.0742     |
|    std                  | 0.535      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.15e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.378      |
|    reward_run           | 2.64        |
| rollout/                |             |
|    adjusted_reward      | 2.44        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.17e+03    |
| time/                   |             |
|    fps                  | 1169        |
|    iterations           | 172         |
|    time_elapsed         | 1505        |
|    total_timesteps      | 1761280     |
| train/                  |             |
|    approx_kl            | 0.018240498 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.622       |
|    cost_value_loss      | 9.62e-07    |
|    entropy_loss         | -4.71       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0333      |
|    n_updates            | 1710        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 7.34e-05    |
|    reward_explained_... | 0.885       |
|    reward_value_loss    | 0.107       |
|    std                  | 0.534       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.22e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.401      |
|    reward_run           | 3.37        |
| rollout/                |             |
|    adjusted_reward      | 2.53        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.19e+03    |
| time/                   |             |
|    fps                  | 1169        |
|    iterations           | 173         |
|    time_elapsed         | 1515        |
|    total_timesteps      | 1771520     |
| train/                  |             |
|    approx_kl            | 0.016669914 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.607       |
|    cost_value_loss      | 1.37e-06    |
|    entropy_loss         | -4.71       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.033       |
|    n_updates            | 1720        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000749   |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0679      |
|    std                  | 0.533       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.24e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.24e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.71        |
| rollout/                |             |
|    adjusted_reward      | 2.38        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.21e+03    |
| time/                   |             |
|    fps                  | 1169        |
|    iterations           | 174         |
|    time_elapsed         | 1523        |
|    total_timesteps      | 1781760     |
| train/                  |             |
|    approx_kl            | 0.017786006 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.419       |
|    cost_value_loss      | 8.38e-07    |
|    entropy_loss         | -4.7        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0303      |
|    n_updates            | 1730        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -8.23e-05   |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0665      |
|    std                  | 0.532       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 3 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.25e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.25e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.386      |
|    reward_run           | 2.35        |
| rollout/                |             |
|    adjusted_reward      | 2.46        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.21e+03    |
| time/                   |             |
|    fps                  | 1168        |
|    iterations           | 175         |
|    time_elapsed         | 1533        |
|    total_timesteps      | 1792000     |
| train/                  |             |
|    approx_kl            | 0.017818606 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.523       |
|    cost_value_loss      | 1.68e-06    |
|    entropy_loss         | -4.68       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0404      |
|    n_updates            | 1740        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00103    |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0761      |
|    std                  | 0.531       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.3e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.38       |
|    reward_run           | 2.94        |
| rollout/                |             |
|    adjusted_reward      | 2.49        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.22e+03    |
| time/                   |             |
|    fps                  | 1168        |
|    iterations           | 176         |
|    time_elapsed         | 1542        |
|    total_timesteps      | 1802240     |
| train/                  |             |
|    approx_kl            | 0.016313132 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.762       |
|    cost_value_loss      | 1.73e-06    |
|    entropy_loss         | -4.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0369      |
|    n_updates            | 1750        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000476   |
|    reward_explained_... | 0.895       |
|    reward_value_loss    | 0.1         |
|    std                  | 0.529       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.3e+03    |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.26e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.391     |
|    reward_run           | 2.21       |
| rollout/                |            |
|    adjusted_reward      | 2.44       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.22e+03   |
| time/                   |            |
|    fps                  | 1168       |
|    iterations           | 177        |
|    time_elapsed         | 1550       |
|    total_timesteps      | 1812480    |
| train/                  |            |
|    approx_kl            | 0.01719165 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.48       |
|    cost_value_loss      | 9.14e-07   |
|    entropy_loss         | -4.65      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 1760       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.001      |
|    reward_explained_... | 0.891      |
|    reward_value_loss    | 0.0953     |
|    std                  | 0.529      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.25e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.399      |
|    reward_run           | 2.74        |
| rollout/                |             |
|    adjusted_reward      | 2.45        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.22e+03    |
| time/                   |             |
|    fps                  | 1169        |
|    iterations           | 178         |
|    time_elapsed         | 1558        |
|    total_timesteps      | 1822720     |
| train/                  |             |
|    approx_kl            | 0.015818885 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.386       |
|    cost_value_loss      | 6.87e-07    |
|    entropy_loss         | -4.65       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0801      |
|    n_updates            | 1770        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -6.43e-05   |
|    reward_explained_... | 0.902       |
|    reward_value_loss    | 0.091       |
|    std                  | 0.528       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.25e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 3.54        |
| rollout/                |             |
|    adjusted_reward      | 2.5         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.23e+03    |
| time/                   |             |
|    fps                  | 1169        |
|    iterations           | 179         |
|    time_elapsed         | 1567        |
|    total_timesteps      | 1832960     |
| train/                  |             |
|    approx_kl            | 0.015626928 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.393       |
|    cost_value_loss      | 1.08e-06    |
|    entropy_loss         | -4.64       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0185      |
|    n_updates            | 1780        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000155    |
|    reward_explained_... | 0.916       |
|    reward_value_loss    | 0.0836      |
|    std                  | 0.528       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.12e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.393      |
|    reward_run           | 2.59        |
| rollout/                |             |
|    adjusted_reward      | 2.49        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.24e+03    |
| time/                   |             |
|    fps                  | 1170        |
|    iterations           | 180         |
|    time_elapsed         | 1575        |
|    total_timesteps      | 1843200     |
| train/                  |             |
|    approx_kl            | 0.015439665 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.165       |
|    cost_value_loss      | 1.13e-06    |
|    entropy_loss         | -4.64       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0243      |
|    n_updates            | 1790        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00125     |
|    reward_explained_... | 0.912       |
|    reward_value_loss    | 0.0853      |
|    std                  | 0.527       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.14e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 3.2         |
| rollout/                |             |
|    adjusted_reward      | 2.52        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.24e+03    |
| time/                   |             |
|    fps                  | 1170        |
|    iterations           | 181         |
|    time_elapsed         | 1583        |
|    total_timesteps      | 1853440     |
| train/                  |             |
|    approx_kl            | 0.015870491 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.178       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.565       |
|    cost_value_loss      | 1.9e-06     |
|    entropy_loss         | -4.63       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0221      |
|    n_updates            | 1800        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00101     |
|    reward_explained_... | 0.922       |
|    reward_value_loss    | 0.0794      |
|    std                  | 0.527       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.3e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.17e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 3.53        |
| rollout/                |             |
|    adjusted_reward      | 2.66        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.26e+03    |
| time/                   |             |
|    fps                  | 1170        |
|    iterations           | 182         |
|    time_elapsed         | 1591        |
|    total_timesteps      | 1863680     |
| train/                  |             |
|    approx_kl            | 0.016020473 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.729       |
|    cost_value_loss      | 1.67e-06    |
|    entropy_loss         | -4.62       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0511      |
|    n_updates            | 1810        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000133    |
|    reward_explained_... | 0.874       |
|    reward_value_loss    | 0.103       |
|    std                  | 0.527       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.41e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.41e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.386     |
|    reward_run           | 3.35       |
| rollout/                |            |
|    adjusted_reward      | 2.62       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.27e+03   |
| time/                   |            |
|    fps                  | 1171       |
|    iterations           | 183        |
|    time_elapsed         | 1600       |
|    total_timesteps      | 1873920    |
| train/                  |            |
|    approx_kl            | 0.01558722 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.656      |
|    cost_value_loss      | 1.12e-06   |
|    entropy_loss         | -4.62      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0212     |
|    n_updates            | 1820       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000136   |
|    reward_explained_... | 0.922      |
|    reward_value_loss    | 0.0721     |
|    std                  | 0.526      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.41e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.36e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.399     |
|    reward_run           | 2.64       |
| rollout/                |            |
|    adjusted_reward      | 2.55       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.28e+03   |
| time/                   |            |
|    fps                  | 1171       |
|    iterations           | 184        |
|    time_elapsed         | 1607       |
|    total_timesteps      | 1884160    |
| train/                  |            |
|    approx_kl            | 0.01642829 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.498      |
|    cost_value_loss      | 6.44e-07   |
|    entropy_loss         | -4.61      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0267     |
|    n_updates            | 1830       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000953   |
|    reward_explained_... | 0.904      |
|    reward_value_loss    | 0.0839     |
|    std                  | 0.525      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.41e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.32e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 3.56        |
| rollout/                |             |
|    adjusted_reward      | 2.65        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.29e+03    |
| time/                   |             |
|    fps                  | 1172        |
|    iterations           | 185         |
|    time_elapsed         | 1615        |
|    total_timesteps      | 1894400     |
| train/                  |             |
|    approx_kl            | 0.015388086 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.445       |
|    cost_value_loss      | 7.39e-07    |
|    entropy_loss         | -4.6        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0241      |
|    n_updates            | 1840        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000985    |
|    reward_explained_... | 0.92        |
|    reward_value_loss    | 0.075       |
|    std                  | 0.525       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.41e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.3e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 2.49        |
| rollout/                |             |
|    adjusted_reward      | 2.7         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.31e+03    |
| time/                   |             |
|    fps                  | 1172        |
|    iterations           | 186         |
|    time_elapsed         | 1624        |
|    total_timesteps      | 1904640     |
| train/                  |             |
|    approx_kl            | 0.019190874 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.507       |
|    cost_value_loss      | 6.78e-07    |
|    entropy_loss         | -4.6        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0338      |
|    n_updates            | 1850        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000101    |
|    reward_explained_... | 0.904       |
|    reward_value_loss    | 0.0877      |
|    std                  | 0.524       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.41e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.4e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 2.88        |
| rollout/                |             |
|    adjusted_reward      | 2.47        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.3e+03     |
| time/                   |             |
|    fps                  | 1172        |
|    iterations           | 187         |
|    time_elapsed         | 1632        |
|    total_timesteps      | 1914880     |
| train/                  |             |
|    approx_kl            | 0.017144144 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.108       |
|    cost_value_loss      | 1.4e-06     |
|    entropy_loss         | -4.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0267      |
|    n_updates            | 1860        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00019     |
|    reward_explained_... | 0.931       |
|    reward_value_loss    | 0.0596      |
|    std                  | 0.523       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.41e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.39e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.413      |
|    reward_run           | 1.79        |
| rollout/                |             |
|    adjusted_reward      | 2.57        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.29e+03    |
| time/                   |             |
|    fps                  | 1172        |
|    iterations           | 188         |
|    time_elapsed         | 1641        |
|    total_timesteps      | 1925120     |
| train/                  |             |
|    approx_kl            | 0.021508673 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.298       |
|    cost_value_loss      | 2.5e-06     |
|    entropy_loss         | -4.56       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 1870        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000272    |
|    reward_explained_... | 0.911       |
|    reward_value_loss    | 0.071       |
|    std                  | 0.521       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.49e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 2.82        |
| rollout/                |             |
|    adjusted_reward      | 2.59        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.3e+03     |
| time/                   |             |
|    fps                  | 1173        |
|    iterations           | 189         |
|    time_elapsed         | 1649        |
|    total_timesteps      | 1935360     |
| train/                  |             |
|    approx_kl            | 0.015751997 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.684       |
|    cost_value_loss      | 1.08e-06    |
|    entropy_loss         | -4.55       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0526      |
|    n_updates            | 1880        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.891       |
|    reward_value_loss    | 0.104       |
|    std                  | 0.521       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.34e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.406     |
|    reward_run           | 2.73       |
| rollout/                |            |
|    adjusted_reward      | 2.54       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.29e+03   |
| time/                   |            |
|    fps                  | 1173       |
|    iterations           | 190        |
|    time_elapsed         | 1657       |
|    total_timesteps      | 1945600    |
| train/                  |            |
|    approx_kl            | 0.01589339 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.642      |
|    cost_value_loss      | 1.17e-06   |
|    entropy_loss         | -4.54      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0427     |
|    n_updates            | 1890       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000125   |
|    reward_explained_... | 0.93       |
|    reward_value_loss    | 0.0658     |
|    std                  | 0.52       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.48e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.64        |
| rollout/                |             |
|    adjusted_reward      | 2.71        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.29e+03    |
| time/                   |             |
|    fps                  | 1173        |
|    iterations           | 191         |
|    time_elapsed         | 1665        |
|    total_timesteps      | 1955840     |
| train/                  |             |
|    approx_kl            | 0.016977498 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.763       |
|    cost_value_loss      | 2.13e-06    |
|    entropy_loss         | -4.53       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0505      |
|    n_updates            | 1900        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000117    |
|    reward_explained_... | 0.925       |
|    reward_value_loss    | 0.0694      |
|    std                  | 0.518       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.414      |
|    reward_run           | 3.05        |
| rollout/                |             |
|    adjusted_reward      | 2.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.3e+03     |
| time/                   |             |
|    fps                  | 1174        |
|    iterations           | 192         |
|    time_elapsed         | 1674        |
|    total_timesteps      | 1966080     |
| train/                  |             |
|    approx_kl            | 0.018377608 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.24        |
|    cost_value_loss      | 6.71e-07    |
|    entropy_loss         | -4.52       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0599      |
|    n_updates            | 1910        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000241    |
|    reward_explained_... | 0.903       |
|    reward_value_loss    | 0.0788      |
|    std                  | 0.518       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.14e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.386     |
|    reward_run           | 3.55       |
| rollout/                |            |
|    adjusted_reward      | 2.59       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.3e+03    |
| time/                   |            |
|    fps                  | 1174       |
|    iterations           | 193        |
|    time_elapsed         | 1683       |
|    total_timesteps      | 1976320    |
| train/                  |            |
|    approx_kl            | 0.01824665 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.232      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.324      |
|    cost_value_loss      | 1.51e-06   |
|    entropy_loss         | -4.51      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0337     |
|    n_updates            | 1920       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | -0.000529  |
|    reward_explained_... | 0.912      |
|    reward_value_loss    | 0.0777     |
|    std                  | 0.517      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 1.49e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 1.35e+03  |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.405    |
|    reward_run           | 2.75      |
| rollout/                |           |
|    adjusted_reward      | 2.66      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 1.31e+03  |
| time/                   |           |
|    fps                  | 1174      |
|    iterations           | 194       |
|    time_elapsed         | 1692      |
|    total_timesteps      | 1986560   |
| train/                  |           |
|    approx_kl            | 0.0160415 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.209     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.719     |
|    cost_value_loss      | 1.26e-06  |
|    entropy_loss         | -4.5      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0225    |
|    n_updates            | 1930      |
|    nu                   | 6.21      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00014   |
|    reward_explained_... | 0.912     |
|    reward_value_loss    | 0.0732    |
|    std                  | 0.517     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.29e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 2.74        |
| rollout/                |             |
|    adjusted_reward      | 2.64        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.31e+03    |
| time/                   |             |
|    fps                  | 1174        |
|    iterations           | 195         |
|    time_elapsed         | 1700        |
|    total_timesteps      | 1996800     |
| train/                  |             |
|    approx_kl            | 0.015062243 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.585       |
|    cost_value_loss      | 1.74e-06    |
|    entropy_loss         | -4.49       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0377      |
|    n_updates            | 1940        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 6.15e-05    |
|    reward_explained_... | 0.915       |
|    reward_value_loss    | 0.0705      |
|    std                  | 0.515       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.23e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 2.08        |
| rollout/                |             |
|    adjusted_reward      | 2.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 1174        |
|    iterations           | 196         |
|    time_elapsed         | 1709        |
|    total_timesteps      | 2007040     |
| train/                  |             |
|    approx_kl            | 0.019800942 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.577       |
|    cost_value_loss      | 6.41e-06    |
|    entropy_loss         | -4.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0206      |
|    n_updates            | 1950        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000224    |
|    reward_explained_... | 0.93        |
|    reward_value_loss    | 0.0665      |
|    std                  | 0.515       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.42e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.402     |
|    reward_run           | 2.28       |
| rollout/                |            |
|    adjusted_reward      | 2.54       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.32e+03   |
| time/                   |            |
|    fps                  | 1174       |
|    iterations           | 197        |
|    time_elapsed         | 1717       |
|    total_timesteps      | 2017280    |
| train/                  |            |
|    approx_kl            | 0.01968973 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.211      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.669      |
|    cost_value_loss      | 2.77e-06   |
|    entropy_loss         | -4.48      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0199     |
|    n_updates            | 1960       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000164   |
|    reward_explained_... | 0.948      |
|    reward_value_loss    | 0.0528     |
|    std                  | 0.514      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.2e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.391      |
|    reward_run           | 2.82        |
| rollout/                |             |
|    adjusted_reward      | 2.57        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 1174        |
|    iterations           | 198         |
|    time_elapsed         | 1726        |
|    total_timesteps      | 2027520     |
| train/                  |             |
|    approx_kl            | 0.018528197 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.6         |
|    cost_value_loss      | 1.59e-06    |
|    entropy_loss         | -4.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0221      |
|    n_updates            | 1970        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000279    |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0909      |
|    std                  | 0.515       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.35e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.412     |
|    reward_run           | 3.01       |
| rollout/                |            |
|    adjusted_reward      | 2.62       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.31e+03   |
| time/                   |            |
|    fps                  | 1175       |
|    iterations           | 199        |
|    time_elapsed         | 1733       |
|    total_timesteps      | 2037760    |
| train/                  |            |
|    approx_kl            | 0.01681887 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.194      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.611      |
|    cost_value_loss      | 5.89e-07   |
|    entropy_loss         | -4.49      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0493     |
|    n_updates            | 1980       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00112    |
|    reward_explained_... | 0.89       |
|    reward_value_loss    | 0.108      |
|    std                  | 0.515      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.39e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 3.06        |
| rollout/                |             |
|    adjusted_reward      | 2.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 1175        |
|    iterations           | 200         |
|    time_elapsed         | 1742        |
|    total_timesteps      | 2048000     |
| train/                  |             |
|    approx_kl            | 0.016238442 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.487       |
|    cost_value_loss      | 7.48e-07    |
|    entropy_loss         | -4.47       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0435      |
|    n_updates            | 1990        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000257    |
|    reward_explained_... | 0.906       |
|    reward_value_loss    | 0.0754      |
|    std                  | 0.513       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.404      |
|    reward_run           | 3.27        |
| rollout/                |             |
|    adjusted_reward      | 2.67        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.3e+03     |
| time/                   |             |
|    fps                  | 1175        |
|    iterations           | 201         |
|    time_elapsed         | 1750        |
|    total_timesteps      | 2058240     |
| train/                  |             |
|    approx_kl            | 0.017998213 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.324       |
|    cost_value_loss      | 2.87e-06    |
|    entropy_loss         | -4.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0219      |
|    n_updates            | 2000        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00099     |
|    reward_explained_... | 0.923       |
|    reward_value_loss    | 0.0734      |
|    std                  | 0.513       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.2e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.94        |
| rollout/                |             |
|    adjusted_reward      | 2.56        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.3e+03     |
| time/                   |             |
|    fps                  | 1176        |
|    iterations           | 202         |
|    time_elapsed         | 1758        |
|    total_timesteps      | 2068480     |
| train/                  |             |
|    approx_kl            | 0.015782133 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.21        |
|    cost_value_loss      | 6.37e-07    |
|    entropy_loss         | -4.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0261      |
|    n_updates            | 2010        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 8.86e-05    |
|    reward_explained_... | 0.905       |
|    reward_value_loss    | 0.0842      |
|    std                  | 0.513       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.34e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 3           |
| rollout/                |             |
|    adjusted_reward      | 2.76        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 1176        |
|    iterations           | 203         |
|    time_elapsed         | 1766        |
|    total_timesteps      | 2078720     |
| train/                  |             |
|    approx_kl            | 0.018525362 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.665       |
|    cost_value_loss      | 1.86e-06    |
|    entropy_loss         | -4.45       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0339      |
|    n_updates            | 2020        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 6.19e-05    |
|    reward_explained_... | 0.925       |
|    reward_value_loss    | 0.0721      |
|    std                  | 0.511       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.16e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.389      |
|    reward_run           | 3.29        |
| rollout/                |             |
|    adjusted_reward      | 2.83        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 1176        |
|    iterations           | 204         |
|    time_elapsed         | 1775        |
|    total_timesteps      | 2088960     |
| train/                  |             |
|    approx_kl            | 0.015887741 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.4         |
|    cost_value_loss      | 7.18e-07    |
|    entropy_loss         | -4.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0338      |
|    n_updates            | 2030        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000185    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0674      |
|    std                  | 0.51        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.382      |
|    reward_run           | 2.51        |
| rollout/                |             |
|    adjusted_reward      | 2.6         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.34e+03    |
| time/                   |             |
|    fps                  | 1176        |
|    iterations           | 205         |
|    time_elapsed         | 1784        |
|    total_timesteps      | 2099200     |
| train/                  |             |
|    approx_kl            | 0.021025036 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.511       |
|    cost_value_loss      | 1.12e-06    |
|    entropy_loss         | -4.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0269      |
|    n_updates            | 2040        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000125    |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0554      |
|    std                  | 0.51        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.39e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.413      |
|    reward_run           | 3.52        |
| rollout/                |             |
|    adjusted_reward      | 2.51        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.32e+03    |
| time/                   |             |
|    fps                  | 1176        |
|    iterations           | 206         |
|    time_elapsed         | 1792        |
|    total_timesteps      | 2109440     |
| train/                  |             |
|    approx_kl            | 0.018625936 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.454       |
|    cost_value_loss      | 2.69e-06    |
|    entropy_loss         | -4.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0278      |
|    n_updates            | 2050        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000213    |
|    reward_explained_... | 0.94        |
|    reward_value_loss    | 0.0592      |
|    std                  | 0.507       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.29e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 3.97        |
| rollout/                |             |
|    adjusted_reward      | 2.66        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 1177        |
|    iterations           | 207         |
|    time_elapsed         | 1799        |
|    total_timesteps      | 2119680     |
| train/                  |             |
|    approx_kl            | 0.017257687 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.432       |
|    cost_value_loss      | 1.06e-06    |
|    entropy_loss         | -4.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0461      |
|    n_updates            | 2060        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -1.64e-05   |
|    reward_explained_... | 0.901       |
|    reward_value_loss    | 0.0921      |
|    std                  | 0.505       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.38       |
|    reward_run           | 2.28        |
| rollout/                |             |
|    adjusted_reward      | 2.55        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 1178        |
|    iterations           | 208         |
|    time_elapsed         | 1807        |
|    total_timesteps      | 2129920     |
| train/                  |             |
|    approx_kl            | 0.015753036 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.381       |
|    cost_value_loss      | 9.98e-07    |
|    entropy_loss         | -4.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0924      |
|    n_updates            | 2070        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00108     |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0813      |
|    std                  | 0.506       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.26e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.388     |
|    reward_run           | 2.07       |
| rollout/                |            |
|    adjusted_reward      | 2.74       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.3e+03    |
| time/                   |            |
|    fps                  | 1178       |
|    iterations           | 209        |
|    time_elapsed         | 1815       |
|    total_timesteps      | 2140160    |
| train/                  |            |
|    approx_kl            | 0.01751996 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.112      |
|    cost_value_loss      | 1.19e-06   |
|    entropy_loss         | -4.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0458     |
|    n_updates            | 2080       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000371   |
|    reward_explained_... | 0.923      |
|    reward_value_loss    | 0.0692     |
|    std                  | 0.506      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.43e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 3.65        |
| rollout/                |             |
|    adjusted_reward      | 2.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.33e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 210         |
|    time_elapsed         | 1823        |
|    total_timesteps      | 2150400     |
| train/                  |             |
|    approx_kl            | 0.016220009 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.807       |
|    cost_value_loss      | 2.09e-06    |
|    entropy_loss         | -4.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0363      |
|    n_updates            | 2090        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.918       |
|    reward_value_loss    | 0.0766      |
|    std                  | 0.506       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.49e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.42e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.389     |
|    reward_run           | 3.23       |
| rollout/                |            |
|    adjusted_reward      | 2.79       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.35e+03   |
| time/                   |            |
|    fps                  | 1179       |
|    iterations           | 211        |
|    time_elapsed         | 1832       |
|    total_timesteps      | 2160640    |
| train/                  |            |
|    approx_kl            | 0.01910587 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.235      |
|    cost_value_loss      | 8.32e-07   |
|    entropy_loss         | -4.38      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0247     |
|    n_updates            | 2100       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000192   |
|    reward_explained_... | 0.907      |
|    reward_value_loss    | 0.0803     |
|    std                  | 0.506      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.25e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.382      |
|    reward_run           | 2.99        |
| rollout/                |             |
|    adjusted_reward      | 2.82        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.37e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 212         |
|    time_elapsed         | 1840        |
|    total_timesteps      | 2170880     |
| train/                  |             |
|    approx_kl            | 0.016693188 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.271       |
|    cost_value_loss      | 1.63e-06    |
|    entropy_loss         | -4.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0301      |
|    n_updates            | 2110        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000265    |
|    reward_explained_... | 0.933       |
|    reward_value_loss    | 0.0671      |
|    std                  | 0.506       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.2e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 3.77        |
| rollout/                |             |
|    adjusted_reward      | 2.63        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.37e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 213         |
|    time_elapsed         | 1849        |
|    total_timesteps      | 2181120     |
| train/                  |             |
|    approx_kl            | 0.021138672 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.866       |
|    cost_value_loss      | 2.34e-06    |
|    entropy_loss         | -4.39       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.031       |
|    n_updates            | 2120        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0003      |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0673      |
|    std                  | 0.506       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.34e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.397      |
|    reward_run           | 3.46        |
| rollout/                |             |
|    adjusted_reward      | 2.81        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 214         |
|    time_elapsed         | 1858        |
|    total_timesteps      | 2191360     |
| train/                  |             |
|    approx_kl            | 0.018447947 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.735       |
|    cost_value_loss      | 1.03e-06    |
|    entropy_loss         | -4.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0914      |
|    n_updates            | 2130        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000281    |
|    reward_explained_... | 0.887       |
|    reward_value_loss    | 0.091       |
|    std                  | 0.506       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.49e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.23e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 3.01        |
| rollout/                |             |
|    adjusted_reward      | 2.72        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.37e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 215         |
|    time_elapsed         | 1866        |
|    total_timesteps      | 2201600     |
| train/                  |             |
|    approx_kl            | 0.016227415 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.405       |
|    cost_value_loss      | 1.76e-06    |
|    entropy_loss         | -4.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0168      |
|    n_updates            | 2140        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000311    |
|    reward_explained_... | 0.934       |
|    reward_value_loss    | 0.0628      |
|    std                  | 0.504       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.51e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 2.6         |
| rollout/                |             |
|    adjusted_reward      | 2.68        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.37e+03    |
| time/                   |             |
|    fps                  | 1179        |
|    iterations           | 216         |
|    time_elapsed         | 1874        |
|    total_timesteps      | 2211840     |
| train/                  |             |
|    approx_kl            | 0.017669605 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.509       |
|    cost_value_loss      | 2.69e-06    |
|    entropy_loss         | -4.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0419      |
|    n_updates            | 2150        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000178    |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.0916      |
|    std                  | 0.505       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.34e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 2.38        |
| rollout/                |             |
|    adjusted_reward      | 2.68        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.35e+03    |
| time/                   |             |
|    fps                  | 1180        |
|    iterations           | 217         |
|    time_elapsed         | 1882        |
|    total_timesteps      | 2222080     |
| train/                  |             |
|    approx_kl            | 0.015078738 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.826       |
|    cost_value_loss      | 1.54e-05    |
|    entropy_loss         | -4.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 2160        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000915    |
|    reward_explained_... | 0.918       |
|    reward_value_loss    | 0.0809      |
|    std                  | 0.504       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 2 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.37e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 3.17        |
| rollout/                |             |
|    adjusted_reward      | 2.91        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 1180        |
|    iterations           | 218         |
|    time_elapsed         | 1891        |
|    total_timesteps      | 2232320     |
| train/                  |             |
|    approx_kl            | 0.017392552 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.653       |
|    cost_value_loss      | 2.92e-06    |
|    entropy_loss         | -4.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.026       |
|    n_updates            | 2170        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.000436   |
|    reward_explained_... | 0.899       |
|    reward_value_loss    | 0.102       |
|    std                  | 0.502       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.42e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 2.03        |
| rollout/                |             |
|    adjusted_reward      | 2.65        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.36e+03    |
| time/                   |             |
|    fps                  | 1180        |
|    iterations           | 219         |
|    time_elapsed         | 1899        |
|    total_timesteps      | 2242560     |
| train/                  |             |
|    approx_kl            | 0.018878484 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.623       |
|    cost_value_loss      | 5.59e-07    |
|    entropy_loss         | -4.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0147      |
|    n_updates            | 2180        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00029     |
|    reward_explained_... | 0.924       |
|    reward_value_loss    | 0.0661      |
|    std                  | 0.5         |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.3e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 3.03        |
| rollout/                |             |
|    adjusted_reward      | 2.92        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 1181        |
|    iterations           | 220         |
|    time_elapsed         | 1907        |
|    total_timesteps      | 2252800     |
| train/                  |             |
|    approx_kl            | 0.019877251 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.406       |
|    cost_value_loss      | 1.4e-06     |
|    entropy_loss         | -4.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0184      |
|    n_updates            | 2190        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000222    |
|    reward_explained_... | 0.896       |
|    reward_value_loss    | 0.0982      |
|    std                  | 0.499       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.51e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.14e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.415     |
|    reward_run           | 3.63       |
| rollout/                |            |
|    adjusted_reward      | 2.69       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.39e+03   |
| time/                   |            |
|    fps                  | 1181       |
|    iterations           | 221        |
|    time_elapsed         | 1915       |
|    total_timesteps      | 2263040    |
| train/                  |            |
|    approx_kl            | 0.01603051 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.403      |
|    cost_value_loss      | 7.36e-07   |
|    entropy_loss         | -4.28      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0214     |
|    n_updates            | 2200       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000513   |
|    reward_explained_... | 0.928      |
|    reward_value_loss    | 0.0626     |
|    std                  | 0.496      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.31e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 4.05        |
| rollout/                |             |
|    adjusted_reward      | 2.79        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.4e+03     |
| time/                   |             |
|    fps                  | 1181        |
|    iterations           | 222         |
|    time_elapsed         | 1923        |
|    total_timesteps      | 2273280     |
| train/                  |             |
|    approx_kl            | 0.015572968 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.201       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.715       |
|    cost_value_loss      | 1.61e-06    |
|    entropy_loss         | -4.26       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 2210        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.912       |
|    reward_value_loss    | 0.0795      |
|    std                  | 0.495       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 3.36        |
| rollout/                |             |
|    adjusted_reward      | 2.77        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.38e+03    |
| time/                   |             |
|    fps                  | 1181        |
|    iterations           | 223         |
|    time_elapsed         | 1931        |
|    total_timesteps      | 2283520     |
| train/                  |             |
|    approx_kl            | 0.016448151 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.897       |
|    cost_value_loss      | 1.26e-05    |
|    entropy_loss         | -4.25       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0418      |
|    n_updates            | 2220        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00115     |
|    reward_explained_... | 0.934       |
|    reward_value_loss    | 0.0747      |
|    std                  | 0.495       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.51e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.45e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.395     |
|    reward_run           | 3.72       |
| rollout/                |            |
|    adjusted_reward      | 2.93       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.4e+03    |
| time/                   |            |
|    fps                  | 1182       |
|    iterations           | 224        |
|    time_elapsed         | 1939       |
|    total_timesteps      | 2293760    |
| train/                  |            |
|    approx_kl            | 0.01747311 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.484      |
|    cost_value_loss      | 1.31e-06   |
|    entropy_loss         | -4.24      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0312     |
|    n_updates            | 2230       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00109    |
|    reward_explained_... | 0.918      |
|    reward_value_loss    | 0.0752     |
|    std                  | 0.495      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.51e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.39e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.389     |
|    reward_run           | 3.5        |
| rollout/                |            |
|    adjusted_reward      | 2.85       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.4e+03    |
| time/                   |            |
|    fps                  | 1182       |
|    iterations           | 225        |
|    time_elapsed         | 1947       |
|    total_timesteps      | 2304000    |
| train/                  |            |
|    approx_kl            | 0.01742717 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.222      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.455      |
|    cost_value_loss      | 1.07e-06   |
|    entropy_loss         | -4.24      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0621     |
|    n_updates            | 2240       |
|    nu                   | 6.21       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 6.74e-05   |
|    reward_explained_... | 0.916      |
|    reward_value_loss    | 0.0714     |
|    std                  | 0.494      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.49e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.404      |
|    reward_run           | 3.34        |
| rollout/                |             |
|    adjusted_reward      | 2.82        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.41e+03    |
| time/                   |             |
|    fps                  | 1183        |
|    iterations           | 226         |
|    time_elapsed         | 1955        |
|    total_timesteps      | 2314240     |
| train/                  |             |
|    approx_kl            | 0.016548857 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.226       |
|    cost_value_loss      | 1.85e-06    |
|    entropy_loss         | -4.23       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0249      |
|    n_updates            | 2250        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000897    |
|    reward_explained_... | 0.896       |
|    reward_value_loss    | 0.0819      |
|    std                  | 0.493       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 3.27        |
| rollout/                |             |
|    adjusted_reward      | 2.99        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1184        |
|    iterations           | 227         |
|    time_elapsed         | 1963        |
|    total_timesteps      | 2324480     |
| train/                  |             |
|    approx_kl            | 0.015826853 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.477       |
|    cost_value_loss      | 1.74e-06    |
|    entropy_loss         | -4.22       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 2260        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000976    |
|    reward_explained_... | 0.9         |
|    reward_value_loss    | 0.0804      |
|    std                  | 0.492       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.51e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.51e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.402      |
|    reward_run           | 2.78        |
| rollout/                |             |
|    adjusted_reward      | 2.83        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.44e+03    |
| time/                   |             |
|    fps                  | 1184        |
|    iterations           | 228         |
|    time_elapsed         | 1971        |
|    total_timesteps      | 2334720     |
| train/                  |             |
|    approx_kl            | 0.015116343 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.324       |
|    cost_value_loss      | 5.48e-07    |
|    entropy_loss         | -4.22       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 2270        |
|    nu                   | 6.21        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000975    |
|    reward_explained_... | 0.942       |
|    reward_value_loss    | 0.0513      |
|    std                  | 0.493       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.63e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.393      |
|    reward_run           | 2.74        |
| rollout/                |             |
|    adjusted_reward      | 2.86        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.44e+03    |
| time/                   |             |
|    fps                  | 1184        |
|    iterations           | 229         |
|    time_elapsed         | 1979        |
|    total_timesteps      | 2344960     |
| train/                  |             |
|    approx_kl            | 0.02946207  |
|    average_cost         | 0.001941933 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -53.9       |
|    cost_value_loss      | 0.011       |
|    entropy_loss         | -4.22       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0304      |
|    n_updates            | 2280        |
|    nu                   | 6.21        |
|    nu_loss              | -0.0121     |
|    policy_gradient_loss | 0.00297     |
|    reward_explained_... | 0.921       |
|    reward_value_loss    | 0.0776      |
|    std                  | 0.494       |
|    total_cost           | 19.885393   |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.33e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.41       |
|    reward_run           | 2.66        |
| rollout/                |             |
|    adjusted_reward      | 2.82        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 230         |
|    time_elapsed         | 1987        |
|    total_timesteps      | 2355200     |
| train/                  |             |
|    approx_kl            | 0.015240511 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.379       |
|    cost_value_loss      | 1.31e-05    |
|    entropy_loss         | -4.23       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.079       |
|    n_updates            | 2290        |
|    nu                   | 6.22        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000332    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0725      |
|    std                  | 0.493       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.48e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 3.58        |
| rollout/                |             |
|    adjusted_reward      | 2.84        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 231         |
|    time_elapsed         | 1995        |
|    total_timesteps      | 2365440     |
| train/                  |             |
|    approx_kl            | 0.019337503 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.601       |
|    cost_value_loss      | 1.72e-05    |
|    entropy_loss         | -4.22       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0272      |
|    n_updates            | 2300        |
|    nu                   | 6.23        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 4.25e-05    |
|    reward_explained_... | 0.897       |
|    reward_value_loss    | 0.0798      |
|    std                  | 0.492       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 3.97        |
| rollout/                |             |
|    adjusted_reward      | 2.9         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.42e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 232         |
|    time_elapsed         | 2004        |
|    total_timesteps      | 2375680     |
| train/                  |             |
|    approx_kl            | 0.017870462 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.743       |
|    cost_value_loss      | 1.15e-05    |
|    entropy_loss         | -4.21       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0377      |
|    n_updates            | 2310        |
|    nu                   | 6.23        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000291    |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0659      |
|    std                  | 0.492       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.29e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.93        |
| rollout/                |             |
|    adjusted_reward      | 2.73        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.41e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 233         |
|    time_elapsed         | 2012        |
|    total_timesteps      | 2385920     |
| train/                  |             |
|    approx_kl            | 0.022393273 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.839       |
|    cost_value_loss      | 1.46e-05    |
|    entropy_loss         | -4.19       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.106       |
|    n_updates            | 2320        |
|    nu                   | 6.24        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000417    |
|    reward_explained_... | 0.935       |
|    reward_value_loss    | 0.0633      |
|    std                  | 0.49        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.35e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 3.08        |
| rollout/                |             |
|    adjusted_reward      | 2.81        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.41e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 234         |
|    time_elapsed         | 2021        |
|    total_timesteps      | 2396160     |
| train/                  |             |
|    approx_kl            | 0.015563461 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.676       |
|    cost_value_loss      | 3.05e-06    |
|    entropy_loss         | -4.19       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0187      |
|    n_updates            | 2330        |
|    nu                   | 6.24        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000272    |
|    reward_explained_... | 0.891       |
|    reward_value_loss    | 0.0915      |
|    std                  | 0.49        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.63e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.418      |
|    reward_run           | 3.51        |
| rollout/                |             |
|    adjusted_reward      | 2.93        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.41e+03    |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 235         |
|    time_elapsed         | 2029        |
|    total_timesteps      | 2406400     |
| train/                  |             |
|    approx_kl            | 0.017447907 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.849       |
|    cost_value_loss      | 9.45e-06    |
|    entropy_loss         | -4.19       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0445      |
|    n_updates            | 2340        |
|    nu                   | 6.25        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00125     |
|    reward_explained_... | 0.92        |
|    reward_value_loss    | 0.0746      |
|    std                  | 0.49        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.63e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.51e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.396     |
|    reward_run           | 3.02       |
| rollout/                |            |
|    adjusted_reward      | 2.91       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.43e+03   |
| time/                   |            |
|    fps                  | 1185       |
|    iterations           | 236        |
|    time_elapsed         | 2038       |
|    total_timesteps      | 2416640    |
| train/                  |            |
|    approx_kl            | 0.01807047 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.748      |
|    cost_value_loss      | 8.14e-06   |
|    entropy_loss         | -4.18      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0187     |
|    n_updates            | 2350       |
|    nu                   | 6.25       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000276   |
|    reward_explained_... | 0.905      |
|    reward_value_loss    | 0.0758     |
|    std                  | 0.49       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.42e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.397      |
|    reward_run           | 2.67        |
| rollout/                |             |
|    adjusted_reward      | 2.66        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.4e+03     |
| time/                   |             |
|    fps                  | 1185        |
|    iterations           | 237         |
|    time_elapsed         | 2046        |
|    total_timesteps      | 2426880     |
| train/                  |             |
|    approx_kl            | 0.022228858 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.459       |
|    cost_value_loss      | 1.07e-06    |
|    entropy_loss         | -4.18       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 2360        |
|    nu                   | 6.25        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -1.41e-05   |
|    reward_explained_... | 0.897       |
|    reward_value_loss    | 0.0752      |
|    std                  | 0.489       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.63e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.44e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.413     |
|    reward_run           | 3.76       |
| rollout/                |            |
|    adjusted_reward      | 2.74       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.4e+03    |
| time/                   |            |
|    fps                  | 1186       |
|    iterations           | 238        |
|    time_elapsed         | 2054       |
|    total_timesteps      | 2437120    |
| train/                  |            |
|    approx_kl            | 0.01904736 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.626      |
|    cost_value_loss      | 3.17e-06   |
|    entropy_loss         | -4.18      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0595     |
|    n_updates            | 2370       |
|    nu                   | 6.26       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00109    |
|    reward_explained_... | 0.883      |
|    reward_value_loss    | 0.0948     |
|    std                  | 0.489      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.61e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 3.44        |
| rollout/                |             |
|    adjusted_reward      | 2.99        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.41e+03    |
| time/                   |             |
|    fps                  | 1186        |
|    iterations           | 239         |
|    time_elapsed         | 2062        |
|    total_timesteps      | 2447360     |
| train/                  |             |
|    approx_kl            | 0.017979521 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.807       |
|    cost_value_loss      | 1.69e-06    |
|    entropy_loss         | -4.17       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0467      |
|    n_updates            | 2380        |
|    nu                   | 6.26        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000975    |
|    reward_explained_... | 0.93        |
|    reward_value_loss    | 0.069       |
|    std                  | 0.488       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.6e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.389      |
|    reward_run           | 1.52        |
| rollout/                |             |
|    adjusted_reward      | 2.82        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.4e+03     |
| time/                   |             |
|    fps                  | 1187        |
|    iterations           | 240         |
|    time_elapsed         | 2070        |
|    total_timesteps      | 2457600     |
| train/                  |             |
|    approx_kl            | 0.019908678 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.404       |
|    cost_value_loss      | 8.85e-07    |
|    entropy_loss         | -4.16       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.021       |
|    n_updates            | 2390        |
|    nu                   | 6.26        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0001      |
|    reward_explained_... | 0.925       |
|    reward_value_loss    | 0.0661      |
|    std                  | 0.487       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 3.46        |
| rollout/                |             |
|    adjusted_reward      | 3.01        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1187        |
|    iterations           | 241         |
|    time_elapsed         | 2078        |
|    total_timesteps      | 2467840     |
| train/                  |             |
|    approx_kl            | 0.018499326 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.432       |
|    cost_value_loss      | 1.09e-06    |
|    entropy_loss         | -4.15       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0306      |
|    n_updates            | 2400        |
|    nu                   | 6.26        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000146    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0714      |
|    std                  | 0.487       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.4e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.401      |
|    reward_run           | 4.2         |
| rollout/                |             |
|    adjusted_reward      | 3.04        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.47e+03    |
| time/                   |             |
|    fps                  | 1187        |
|    iterations           | 242         |
|    time_elapsed         | 2086        |
|    total_timesteps      | 2478080     |
| train/                  |             |
|    approx_kl            | 0.016640719 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.187       |
|    cost_value_loss      | 5.58e-07    |
|    entropy_loss         | -4.15       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0343      |
|    n_updates            | 2410        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00105     |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0655      |
|    std                  | 0.487       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.48e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.406      |
|    reward_run           | 3.98        |
| rollout/                |             |
|    adjusted_reward      | 2.92        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.47e+03    |
| time/                   |             |
|    fps                  | 1188        |
|    iterations           | 243         |
|    time_elapsed         | 2094        |
|    total_timesteps      | 2488320     |
| train/                  |             |
|    approx_kl            | 0.016198412 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.277       |
|    cost_value_loss      | 6.03e-07    |
|    entropy_loss         | -4.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0385      |
|    n_updates            | 2420        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00143     |
|    reward_explained_... | 0.932       |
|    reward_value_loss    | 0.0645      |
|    std                  | 0.485       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.56e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 3.52        |
| rollout/                |             |
|    adjusted_reward      | 3.08        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1188        |
|    iterations           | 244         |
|    time_elapsed         | 2102        |
|    total_timesteps      | 2498560     |
| train/                  |             |
|    approx_kl            | 0.016698938 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.653       |
|    cost_value_loss      | 8.84e-07    |
|    entropy_loss         | -4.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0386      |
|    n_updates            | 2430        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00132     |
|    reward_explained_... | 0.893       |
|    reward_value_loss    | 0.0914      |
|    std                  | 0.486       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 485         |
|    mean_reward          | 1.37e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.391      |
|    reward_run           | 3.25        |
| rollout/                |             |
|    adjusted_reward      | 2.79        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1190        |
|    iterations           | 245         |
|    time_elapsed         | 2107        |
|    total_timesteps      | 2508800     |
| train/                  |             |
|    approx_kl            | 0.015769683 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.673       |
|    cost_value_loss      | 1.88e-06    |
|    entropy_loss         | -4.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0211      |
|    n_updates            | 2440        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000893    |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.0439      |
|    std                  | 0.486       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.46e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.418      |
|    reward_run           | 3.62        |
| rollout/                |             |
|    adjusted_reward      | 2.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.46e+03    |
| time/                   |             |
|    fps                  | 1190        |
|    iterations           | 246         |
|    time_elapsed         | 2115        |
|    total_timesteps      | 2519040     |
| train/                  |             |
|    approx_kl            | 0.018800188 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.589       |
|    cost_value_loss      | 1.82e-06    |
|    entropy_loss         | -4.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0347      |
|    n_updates            | 2450        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000469    |
|    reward_explained_... | 0.872       |
|    reward_value_loss    | 0.0936      |
|    std                  | 0.486       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.51e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 3.3         |
| rollout/                |             |
|    adjusted_reward      | 3.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1191        |
|    iterations           | 247         |
|    time_elapsed         | 2123        |
|    total_timesteps      | 2529280     |
| train/                  |             |
|    approx_kl            | 0.015090505 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.631       |
|    cost_value_loss      | 1.36e-06    |
|    entropy_loss         | -4.14       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0204      |
|    n_updates            | 2460        |
|    nu                   | 6.27        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00114     |
|    reward_explained_... | 0.913       |
|    reward_value_loss    | 0.0703      |
|    std                  | 0.485       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.47e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.414      |
|    reward_run           | 3.61        |
| rollout/                |             |
|    adjusted_reward      | 3           |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1191        |
|    iterations           | 248         |
|    time_elapsed         | 2131        |
|    total_timesteps      | 2539520     |
| train/                  |             |
|    approx_kl            | 0.015751624 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.684       |
|    cost_value_loss      | 1.07e-06    |
|    entropy_loss         | -4.13       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0258      |
|    n_updates            | 2470        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00099     |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0491      |
|    std                  | 0.485       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.53e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.414      |
|    reward_run           | 3.36        |
| rollout/                |             |
|    adjusted_reward      | 3           |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1193        |
|    iterations           | 249         |
|    time_elapsed         | 2136        |
|    total_timesteps      | 2549760     |
| train/                  |             |
|    approx_kl            | 0.017275797 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.516       |
|    cost_value_loss      | 9.6e-07     |
|    entropy_loss         | -4.12       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0416      |
|    n_updates            | 2480        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.914       |
|    reward_value_loss    | 0.0626      |
|    std                  | 0.483       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.46e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.41       |
|    reward_run           | 2.92        |
| rollout/                |             |
|    adjusted_reward      | 2.98        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 1193        |
|    iterations           | 250         |
|    time_elapsed         | 2144        |
|    total_timesteps      | 2560000     |
| train/                  |             |
|    approx_kl            | 0.016249416 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.746       |
|    cost_value_loss      | 5.15e-05    |
|    entropy_loss         | -4.1        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0204      |
|    n_updates            | 2490        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000214    |
|    reward_explained_... | 0.934       |
|    reward_value_loss    | 0.0588      |
|    std                  | 0.482       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.63e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.24e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.399      |
|    reward_run           | 3.23        |
| rollout/                |             |
|    adjusted_reward      | 2.73        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1193        |
|    iterations           | 251         |
|    time_elapsed         | 2152        |
|    total_timesteps      | 2570240     |
| train/                  |             |
|    approx_kl            | 0.016500581 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.195       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.766       |
|    cost_value_loss      | 6.99e-06    |
|    entropy_loss         | -4.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0252      |
|    n_updates            | 2500        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00121     |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0785      |
|    std                  | 0.482       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.68e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.68e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.399      |
|    reward_run           | 3.98        |
| rollout/                |             |
|    adjusted_reward      | 2.75        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1194        |
|    iterations           | 252         |
|    time_elapsed         | 2161        |
|    total_timesteps      | 2580480     |
| train/                  |             |
|    approx_kl            | 0.018671714 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.71        |
|    cost_value_loss      | 2.42e-06    |
|    entropy_loss         | -4.09       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0229      |
|    n_updates            | 2510        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000201    |
|    reward_explained_... | 0.942       |
|    reward_value_loss    | 0.0477      |
|    std                  | 0.482       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.68e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.52e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.56        |
| rollout/                |             |
|    adjusted_reward      | 3.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.46e+03    |
| time/                   |             |
|    fps                  | 1194        |
|    iterations           | 253         |
|    time_elapsed         | 2169        |
|    total_timesteps      | 2590720     |
| train/                  |             |
|    approx_kl            | 0.016014298 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.622       |
|    cost_value_loss      | 1.93e-06    |
|    entropy_loss         | -4.08       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0449      |
|    n_updates            | 2520        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000258    |
|    reward_explained_... | 0.918       |
|    reward_value_loss    | 0.0734      |
|    std                  | 0.48        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.68e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.62e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.395      |
|    reward_run           | 2.88        |
| rollout/                |             |
|    adjusted_reward      | 2.91        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1194        |
|    iterations           | 254         |
|    time_elapsed         | 2177        |
|    total_timesteps      | 2600960     |
| train/                  |             |
|    approx_kl            | 0.020061236 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.46        |
|    cost_value_loss      | 5.39e-07    |
|    entropy_loss         | -4.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0175      |
|    n_updates            | 2530        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00011     |
|    reward_explained_... | 0.959       |
|    reward_value_loss    | 0.0392      |
|    std                  | 0.481       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.68e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.3e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 3.38        |
| rollout/                |             |
|    adjusted_reward      | 2.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1194        |
|    iterations           | 255         |
|    time_elapsed         | 2185        |
|    total_timesteps      | 2611200     |
| train/                  |             |
|    approx_kl            | 0.019981518 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.67        |
|    cost_value_loss      | 3.86e-06    |
|    entropy_loss         | -4.07       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0213      |
|    n_updates            | 2540        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000144    |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0572      |
|    std                  | 0.48        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.68e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.6e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 3.61        |
| rollout/                |             |
|    adjusted_reward      | 3.03        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1194        |
|    iterations           | 256         |
|    time_elapsed         | 2193        |
|    total_timesteps      | 2621440     |
| train/                  |             |
|    approx_kl            | 0.016723994 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.397       |
|    cost_value_loss      | 5.67e-07    |
|    entropy_loss         | -4.06       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0168      |
|    n_updates            | 2550        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00111     |
|    reward_explained_... | 0.933       |
|    reward_value_loss    | 0.0647      |
|    std                  | 0.479       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.72e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.406      |
|    reward_run           | 3.19        |
| rollout/                |             |
|    adjusted_reward      | 2.94        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1195        |
|    iterations           | 257         |
|    time_elapsed         | 2201        |
|    total_timesteps      | 2631680     |
| train/                  |             |
|    approx_kl            | 0.017004136 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.713       |
|    cost_value_loss      | 3.52e-06    |
|    entropy_loss         | -4.05       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0371      |
|    n_updates            | 2560        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0014      |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0749      |
|    std                  | 0.477       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.53e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.41       |
|    reward_run           | 3.53        |
| rollout/                |             |
|    adjusted_reward      | 3.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1196        |
|    iterations           | 258         |
|    time_elapsed         | 2208        |
|    total_timesteps      | 2641920     |
| train/                  |             |
|    approx_kl            | 0.016528796 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.613       |
|    cost_value_loss      | 3.67e-06    |
|    entropy_loss         | -4.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 2570        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00144     |
|    reward_explained_... | 0.918       |
|    reward_value_loss    | 0.0866      |
|    std                  | 0.476       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.61e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 3.35        |
| rollout/                |             |
|    adjusted_reward      | 3.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.5e+03     |
| time/                   |             |
|    fps                  | 1196        |
|    iterations           | 259         |
|    time_elapsed         | 2216        |
|    total_timesteps      | 2652160     |
| train/                  |             |
|    approx_kl            | 0.016443769 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.215       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.789       |
|    cost_value_loss      | 2.83e-06    |
|    entropy_loss         | -4.02       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0291      |
|    n_updates            | 2580        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000306    |
|    reward_explained_... | 0.906       |
|    reward_value_loss    | 0.0824      |
|    std                  | 0.477       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.42e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.392      |
|    reward_run           | 3.61        |
| rollout/                |             |
|    adjusted_reward      | 3.29        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.54e+03    |
| time/                   |             |
|    fps                  | 1196        |
|    iterations           | 260         |
|    time_elapsed         | 2224        |
|    total_timesteps      | 2662400     |
| train/                  |             |
|    approx_kl            | 0.021307638 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.545       |
|    cost_value_loss      | 9.08e-07    |
|    entropy_loss         | -4.01       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0241      |
|    n_updates            | 2590        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000483    |
|    reward_explained_... | 0.931       |
|    reward_value_loss    | 0.0601      |
|    std                  | 0.474       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.68e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.384      |
|    reward_run           | 2.49        |
| rollout/                |             |
|    adjusted_reward      | 3.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1197        |
|    iterations           | 261         |
|    time_elapsed         | 2232        |
|    total_timesteps      | 2672640     |
| train/                  |             |
|    approx_kl            | 0.017332157 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.478       |
|    cost_value_loss      | 3.22e-07    |
|    entropy_loss         | -3.99       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 2600        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00118     |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0509      |
|    std                  | 0.474       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.72e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.69e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.408     |
|    reward_run           | 4.62       |
| rollout/                |            |
|    adjusted_reward      | 3.27       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.59e+03   |
| time/                   |            |
|    fps                  | 1197       |
|    iterations           | 262        |
|    time_elapsed         | 2240       |
|    total_timesteps      | 2682880    |
| train/                  |            |
|    approx_kl            | 0.02146959 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.162      |
|    cost_value_loss      | 3.87e-07   |
|    entropy_loss         | -3.97      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.035      |
|    n_updates            | 2610       |
|    nu                   | 6.28       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000339   |
|    reward_explained_... | 0.92       |
|    reward_value_loss    | 0.0585     |
|    std                  | 0.471      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.45e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 3.67        |
| rollout/                |             |
|    adjusted_reward      | 3.35        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.62e+03    |
| time/                   |             |
|    fps                  | 1197        |
|    iterations           | 263         |
|    time_elapsed         | 2248        |
|    total_timesteps      | 2693120     |
| train/                  |             |
|    approx_kl            | 0.018227717 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.169       |
|    cost_value_loss      | 4.88e-07    |
|    entropy_loss         | -3.96       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0768      |
|    n_updates            | 2620        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000444    |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0599      |
|    std                  | 0.473       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.35e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 4.47        |
| rollout/                |             |
|    adjusted_reward      | 3.35        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.64e+03    |
| time/                   |             |
|    fps                  | 1198        |
|    iterations           | 264         |
|    time_elapsed         | 2256        |
|    total_timesteps      | 2703360     |
| train/                  |             |
|    approx_kl            | 0.020726327 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.141       |
|    cost_value_loss      | 8.22e-07    |
|    entropy_loss         | -3.96       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0112      |
|    n_updates            | 2630        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000379    |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.041       |
|    std                  | 0.471       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.57e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.403      |
|    reward_run           | 3.92        |
| rollout/                |             |
|    adjusted_reward      | 3.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.63e+03    |
| time/                   |             |
|    fps                  | 1198        |
|    iterations           | 265         |
|    time_elapsed         | 2264        |
|    total_timesteps      | 2713600     |
| train/                  |             |
|    approx_kl            | 0.015472382 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0896      |
|    cost_value_loss      | 1.89e-06    |
|    entropy_loss         | -3.96       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.024       |
|    n_updates            | 2640        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00116     |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0615      |
|    std                  | 0.472       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.72e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.65e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.398     |
|    reward_run           | 3.27       |
| rollout/                |            |
|    adjusted_reward      | 3.14       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.63e+03   |
| time/                   |            |
|    fps                  | 1198       |
|    iterations           | 266        |
|    time_elapsed         | 2272       |
|    total_timesteps      | 2723840    |
| train/                  |            |
|    approx_kl            | 0.02297859 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.312      |
|    cost_value_loss      | 1.28e-06   |
|    entropy_loss         | -3.95      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0257     |
|    n_updates            | 2650       |
|    nu                   | 6.28       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000191   |
|    reward_explained_... | 0.922      |
|    reward_value_loss    | 0.0614     |
|    std                  | 0.47       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.5e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.413      |
|    reward_run           | 4.34        |
| rollout/                |             |
|    adjusted_reward      | 3.13        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.61e+03    |
| time/                   |             |
|    fps                  | 1198        |
|    iterations           | 267         |
|    time_elapsed         | 2280        |
|    total_timesteps      | 2734080     |
| train/                  |             |
|    approx_kl            | 0.020003773 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0773      |
|    cost_value_loss      | 4.51e-07    |
|    entropy_loss         | -3.94       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.058       |
|    n_updates            | 2660        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000266    |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0528      |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.72e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.55e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.404      |
|    reward_run           | 4.29        |
| rollout/                |             |
|    adjusted_reward      | 3.23        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 1198        |
|    iterations           | 268         |
|    time_elapsed         | 2288        |
|    total_timesteps      | 2744320     |
| train/                  |             |
|    approx_kl            | 0.017573481 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.261       |
|    cost_value_loss      | 2.25e-06    |
|    entropy_loss         | -3.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 2670        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000475    |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0689      |
|    std                  | 0.472       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.72e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.59e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.42      |
|    reward_run           | 4.3        |
| rollout/                |            |
|    adjusted_reward      | 3.15       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.57e+03   |
| time/                   |            |
|    fps                  | 1199       |
|    iterations           | 269        |
|    time_elapsed         | 2296       |
|    total_timesteps      | 2754560    |
| train/                  |            |
|    approx_kl            | 0.01521005 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.293      |
|    cost_value_loss      | 2.76e-07   |
|    entropy_loss         | -3.95      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0398     |
|    n_updates            | 2680       |
|    nu                   | 6.28       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00122    |
|    reward_explained_... | 0.939      |
|    reward_value_loss    | 0.0437     |
|    std                  | 0.471      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 4.27        |
| rollout/                |             |
|    adjusted_reward      | 3.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1199        |
|    iterations           | 270         |
|    time_elapsed         | 2304        |
|    total_timesteps      | 2764800     |
| train/                  |             |
|    approx_kl            | 0.016981065 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.656       |
|    cost_value_loss      | 6.85e-07    |
|    entropy_loss         | -3.94       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0112      |
|    n_updates            | 2690        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00143     |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0549      |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.56e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 1.81        |
| rollout/                |             |
|    adjusted_reward      | 3.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1199        |
|    iterations           | 271         |
|    time_elapsed         | 2312        |
|    total_timesteps      | 2775040     |
| train/                  |             |
|    approx_kl            | 0.016559757 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.596       |
|    cost_value_loss      | 2.2e-06     |
|    entropy_loss         | -3.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0263      |
|    n_updates            | 2700        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00045     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.053       |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.55e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.402      |
|    reward_run           | 4.15        |
| rollout/                |             |
|    adjusted_reward      | 3.05        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1200        |
|    iterations           | 272         |
|    time_elapsed         | 2320        |
|    total_timesteps      | 2785280     |
| train/                  |             |
|    approx_kl            | 0.015189531 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.161       |
|    cost_value_loss      | 4.79e-07    |
|    entropy_loss         | -3.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0375      |
|    n_updates            | 2710        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.92        |
|    reward_value_loss    | 0.0704      |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
Violated constraint in the test environment, terminating the episode.
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.77e+03   |
|    mean_ep_length       | 416        |
|    mean_reward          | 1.16e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.414     |
|    reward_run           | 4          |
| rollout/                |            |
|    adjusted_reward      | 3.13       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.55e+03   |
| time/                   |            |
|    fps                  | 1200       |
|    iterations           | 273        |
|    time_elapsed         | 2327       |
|    total_timesteps      | 2795520    |
| train/                  |            |
|    approx_kl            | 0.02141169 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.241      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.545      |
|    cost_value_loss      | 2.35e-06   |
|    entropy_loss         | -3.94      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0199     |
|    n_updates            | 2720       |
|    nu                   | 6.28       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000297   |
|    reward_explained_... | 0.947      |
|    reward_value_loss    | 0.0562     |
|    std                  | 0.471      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.77e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.45e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.406     |
|    reward_run           | 4.66       |
| rollout/                |            |
|    adjusted_reward      | 3.06       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.54e+03   |
| time/                   |            |
|    fps                  | 1201       |
|    iterations           | 274        |
|    time_elapsed         | 2336       |
|    total_timesteps      | 2805760    |
| train/                  |            |
|    approx_kl            | 0.01626941 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.593      |
|    cost_value_loss      | 9.03e-07   |
|    entropy_loss         | -3.94      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0119     |
|    n_updates            | 2730       |
|    nu                   | 6.28       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00117    |
|    reward_explained_... | 0.946      |
|    reward_value_loss    | 0.0561     |
|    std                  | 0.471      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.55e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.42       |
|    reward_run           | 3.67        |
| rollout/                |             |
|    adjusted_reward      | 3.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.56e+03    |
| time/                   |             |
|    fps                  | 1201        |
|    iterations           | 275         |
|    time_elapsed         | 2344        |
|    total_timesteps      | 2816000     |
| train/                  |             |
|    approx_kl            | 0.021125661 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.781       |
|    cost_value_loss      | 2.16e-06    |
|    entropy_loss         | -3.94       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.044       |
|    n_updates            | 2740        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000234    |
|    reward_explained_... | 0.935       |
|    reward_value_loss    | 0.0607      |
|    std                  | 0.471       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 4.06        |
| rollout/                |             |
|    adjusted_reward      | 3.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.56e+03    |
| time/                   |             |
|    fps                  | 1201        |
|    iterations           | 276         |
|    time_elapsed         | 2352        |
|    total_timesteps      | 2826240     |
| train/                  |             |
|    approx_kl            | 0.016068807 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.226       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.321       |
|    cost_value_loss      | 5.21e-07    |
|    entropy_loss         | -3.94       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 2750        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000451    |
|    reward_explained_... | 0.927       |
|    reward_value_loss    | 0.066       |
|    std                  | 0.469       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.53e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 3.98        |
| rollout/                |             |
|    adjusted_reward      | 3.02        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.55e+03    |
| time/                   |             |
|    fps                  | 1201        |
|    iterations           | 277         |
|    time_elapsed         | 2360        |
|    total_timesteps      | 2836480     |
| train/                  |             |
|    approx_kl            | 0.017157182 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.44        |
|    cost_value_loss      | 4.29e-07    |
|    entropy_loss         | -3.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.053       |
|    n_updates            | 2760        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00151     |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0701      |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.5e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 4.19        |
| rollout/                |             |
|    adjusted_reward      | 3.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1201        |
|    iterations           | 278         |
|    time_elapsed         | 2368        |
|    total_timesteps      | 2846720     |
| train/                  |             |
|    approx_kl            | 0.015903197 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.229       |
|    cost_value_loss      | 5.98e-07    |
|    entropy_loss         | -3.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0285      |
|    n_updates            | 2770        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000956    |
|    reward_explained_... | 0.903       |
|    reward_value_loss    | 0.0828      |
|    std                  | 0.472       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.73e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.404      |
|    reward_run           | 3.19        |
| rollout/                |             |
|    adjusted_reward      | 3.24        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 279         |
|    time_elapsed         | 2376        |
|    total_timesteps      | 2856960     |
| train/                  |             |
|    approx_kl            | 0.016290609 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.332       |
|    cost_value_loss      | 1.12e-06    |
|    entropy_loss         | -3.95       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0154      |
|    n_updates            | 2780        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00121     |
|    reward_explained_... | 0.933       |
|    reward_value_loss    | 0.0536      |
|    std                  | 0.47        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.43e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 3.03        |
| rollout/                |             |
|    adjusted_reward      | 3.22        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.58e+03    |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 280         |
|    time_elapsed         | 2385        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.015538966 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.38        |
|    cost_value_loss      | 5.11e-07    |
|    entropy_loss         | -3.94       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0151      |
|    n_updates            | 2790        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000486    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0568      |
|    std                  | 0.469       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.7e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 3.88        |
| rollout/                |             |
|    adjusted_reward      | 3.27        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 281         |
|    time_elapsed         | 2393        |
|    total_timesteps      | 2877440     |
| train/                  |             |
|    approx_kl            | 0.017649312 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.841       |
|    cost_value_loss      | 2.96e-06    |
|    entropy_loss         | -3.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0315      |
|    n_updates            | 2800        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00138     |
|    reward_explained_... | 0.933       |
|    reward_value_loss    | 0.0593      |
|    std                  | 0.469       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.67e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 2.75        |
| rollout/                |             |
|    adjusted_reward      | 2.97        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.6e+03     |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 282         |
|    time_elapsed         | 2401        |
|    total_timesteps      | 2887680     |
| train/                  |             |
|    approx_kl            | 0.017086849 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.222       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.113       |
|    cost_value_loss      | 6.6e-07     |
|    entropy_loss         | -3.93       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 2810        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00135     |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.0651      |
|    std                  | 0.469       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.63e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 4.19        |
| rollout/                |             |
|    adjusted_reward      | 3.27        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.6e+03     |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 283         |
|    time_elapsed         | 2409        |
|    total_timesteps      | 2897920     |
| train/                  |             |
|    approx_kl            | 0.019341178 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.249       |
|    cost_value_loss      | 8.14e-07    |
|    entropy_loss         | -3.92       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0488      |
|    n_updates            | 2820        |
|    nu                   | 6.28        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000196    |
|    reward_explained_... | 0.906       |
|    reward_value_loss    | 0.0739      |
|    std                  | 0.468       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.67e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.407      |
|    reward_run           | 3.93        |
| rollout/                |             |
|    adjusted_reward      | 3.13        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.58e+03    |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 284         |
|    time_elapsed         | 2418        |
|    total_timesteps      | 2908160     |
| train/                  |             |
|    approx_kl            | 0.023600122 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.127       |
|    cost_value_loss      | 4.22e-07    |
|    entropy_loss         | -3.92       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0276      |
|    n_updates            | 2830        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000383    |
|    reward_explained_... | 0.914       |
|    reward_value_loss    | 0.0528      |
|    std                  | 0.468       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.48e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.396      |
|    reward_run           | 3.84        |
| rollout/                |             |
|    adjusted_reward      | 3.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.58e+03    |
| time/                   |             |
|    fps                  | 1202        |
|    iterations           | 285         |
|    time_elapsed         | 2426        |
|    total_timesteps      | 2918400     |
| train/                  |             |
|    approx_kl            | 0.020694228 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.21        |
|    cost_value_loss      | 2.56e-06    |
|    entropy_loss         | -3.91       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0239      |
|    n_updates            | 2840        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000304    |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0474      |
|    std                  | 0.468       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.77e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.57e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.412     |
|    reward_run           | 3.78       |
| rollout/                |            |
|    adjusted_reward      | 3.31       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.58e+03   |
| time/                   |            |
|    fps                  | 1202       |
|    iterations           | 286        |
|    time_elapsed         | 2434       |
|    total_timesteps      | 2928640    |
| train/                  |            |
|    approx_kl            | 0.01619551 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.53       |
|    cost_value_loss      | 9.45e-07   |
|    entropy_loss         | -3.91      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0203     |
|    n_updates            | 2850       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0011     |
|    reward_explained_... | 0.926      |
|    reward_value_loss    | 0.0519     |
|    std                  | 0.468      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.64e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 4.43        |
| rollout/                |             |
|    adjusted_reward      | 3.4         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.61e+03    |
| time/                   |             |
|    fps                  | 1204        |
|    iterations           | 287         |
|    time_elapsed         | 2439        |
|    total_timesteps      | 2938880     |
| train/                  |             |
|    approx_kl            | 0.016427565 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.637       |
|    cost_value_loss      | 5.71e-07    |
|    entropy_loss         | -3.91       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.032       |
|    n_updates            | 2860        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00129     |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.0358      |
|    std                  | 0.468       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.64e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 3.39        |
| rollout/                |             |
|    adjusted_reward      | 3.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.62e+03    |
| time/                   |             |
|    fps                  | 1204        |
|    iterations           | 288         |
|    time_elapsed         | 2447        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.019192684 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.538       |
|    cost_value_loss      | 3.2e-07     |
|    entropy_loss         | -3.9        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0456      |
|    n_updates            | 2870        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0011      |
|    reward_explained_... | 0.935       |
|    reward_value_loss    | 0.0567      |
|    std                  | 0.467       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.59e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 4.37        |
| rollout/                |             |
|    adjusted_reward      | 3.07        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.61e+03    |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 289         |
|    time_elapsed         | 2455        |
|    total_timesteps      | 2959360     |
| train/                  |             |
|    approx_kl            | 0.015721496 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.848       |
|    cost_value_loss      | 4.56e-05    |
|    entropy_loss         | -3.89       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0574      |
|    n_updates            | 2880        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00125     |
|    reward_explained_... | 0.921       |
|    reward_value_loss    | 0.0619      |
|    std                  | 0.466       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.74e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 4.26        |
| rollout/                |             |
|    adjusted_reward      | 3.34        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.63e+03    |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 290         |
|    time_elapsed         | 2463        |
|    total_timesteps      | 2969600     |
| train/                  |             |
|    approx_kl            | 0.019045088 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.403       |
|    cost_value_loss      | 8.43e-07    |
|    entropy_loss         | -3.88       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0197      |
|    n_updates            | 2890        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00126     |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0494      |
|    std                  | 0.465       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.75e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 2.77        |
| rollout/                |             |
|    adjusted_reward      | 2.9         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.6e+03     |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 291         |
|    time_elapsed         | 2471        |
|    total_timesteps      | 2979840     |
| train/                  |             |
|    approx_kl            | 0.019568598 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.291       |
|    cost_value_loss      | 3.26e-07    |
|    entropy_loss         | -3.88       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 2900        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000942    |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0464      |
|    std                  | 0.464       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.393      |
|    reward_run           | 1.89        |
| rollout/                |             |
|    adjusted_reward      | 3.32        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 292         |
|    time_elapsed         | 2479        |
|    total_timesteps      | 2990080     |
| train/                  |             |
|    approx_kl            | 0.016265938 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.662       |
|    cost_value_loss      | 2.45e-06    |
|    entropy_loss         | -3.86       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0197      |
|    n_updates            | 2910        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0013      |
|    reward_explained_... | 0.928       |
|    reward_value_loss    | 0.0705      |
|    std                  | 0.464       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 3.87        |
| rollout/                |             |
|    adjusted_reward      | 3.21        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.59e+03    |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 293         |
|    time_elapsed         | 2488        |
|    total_timesteps      | 3000320     |
| train/                  |             |
|    approx_kl            | 0.019660335 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.196       |
|    cost_value_loss      | 5.42e-07    |
|    entropy_loss         | -3.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0233      |
|    n_updates            | 2920        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00118     |
|    reward_explained_... | 0.935       |
|    reward_value_loss    | 0.0623      |
|    std                  | 0.463       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.68e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 3.29        |
| rollout/                |             |
|    adjusted_reward      | 2.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1206        |
|    iterations           | 294         |
|    time_elapsed         | 2496        |
|    total_timesteps      | 3010560     |
| train/                  |             |
|    approx_kl            | 0.016345304 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.105       |
|    cost_value_loss      | 9.09e-07    |
|    entropy_loss         | -3.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0501      |
|    n_updates            | 2930        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00117     |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0583      |
|    std                  | 0.464       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.72e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 3.9         |
| rollout/                |             |
|    adjusted_reward      | 3.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1206        |
|    iterations           | 295         |
|    time_elapsed         | 2503        |
|    total_timesteps      | 3020800     |
| train/                  |             |
|    approx_kl            | 0.015366915 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.496       |
|    cost_value_loss      | 1.9e-06     |
|    entropy_loss         | -3.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0219      |
|    n_updates            | 2940        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0014      |
|    reward_explained_... | 0.924       |
|    reward_value_loss    | 0.0793      |
|    std                  | 0.464       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.69e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 4.11        |
| rollout/                |             |
|    adjusted_reward      | 3.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.62e+03    |
| time/                   |             |
|    fps                  | 1206        |
|    iterations           | 296         |
|    time_elapsed         | 2511        |
|    total_timesteps      | 3031040     |
| train/                  |             |
|    approx_kl            | 0.017105961 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.423       |
|    cost_value_loss      | 1.8e-06     |
|    entropy_loss         | -3.85       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0109      |
|    n_updates            | 2950        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00112     |
|    reward_explained_... | 0.955       |
|    reward_value_loss    | 0.0414      |
|    std                  | 0.463       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.77e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.47e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.41       |
|    reward_run           | 4.37        |
| rollout/                |             |
|    adjusted_reward      | 3.09        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.6e+03     |
| time/                   |             |
|    fps                  | 1206        |
|    iterations           | 297         |
|    time_elapsed         | 2519        |
|    total_timesteps      | 3041280     |
| train/                  |             |
|    approx_kl            | 0.016509932 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.223       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.145       |
|    cost_value_loss      | 6.43e-07    |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0181      |
|    n_updates            | 2960        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00134     |
|    reward_explained_... | 0.947       |
|    reward_value_loss    | 0.042       |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.82e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.82e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.41       |
|    reward_run           | 3.45        |
| rollout/                |             |
|    adjusted_reward      | 3.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.63e+03    |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 298         |
|    time_elapsed         | 2527        |
|    total_timesteps      | 3051520     |
| train/                  |             |
|    approx_kl            | 0.020745585 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.434       |
|    cost_value_loss      | 8.2e-07     |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0343      |
|    n_updates            | 2970        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000802    |
|    reward_explained_... | 0.92        |
|    reward_value_loss    | 0.0611      |
|    std                  | 0.463       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.82e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.74e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.43       |
|    reward_run           | 4.18        |
| rollout/                |             |
|    adjusted_reward      | 3.5         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.69e+03    |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 299         |
|    time_elapsed         | 2535        |
|    total_timesteps      | 3061760     |
| train/                  |             |
|    approx_kl            | 0.015573338 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.212       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.424       |
|    cost_value_loss      | 3.15e-07    |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.012       |
|    n_updates            | 2980        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00104     |
|    reward_explained_... | 0.961       |
|    reward_value_loss    | 0.0283      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.82e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.8e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.39       |
|    reward_run           | 3.33        |
| rollout/                |             |
|    adjusted_reward      | 3.29        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.68e+03    |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 300         |
|    time_elapsed         | 2544        |
|    total_timesteps      | 3072000     |
| train/                  |             |
|    approx_kl            | 0.022602612 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.535       |
|    cost_value_loss      | 6.19e-07    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 2990        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000367    |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0373      |
|    std                  | 0.46        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.83e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 4.14        |
| rollout/                |             |
|    adjusted_reward      | 3.27        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.66e+03    |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 301         |
|    time_elapsed         | 2552        |
|    total_timesteps      | 3082240     |
| train/                  |             |
|    approx_kl            | 0.019094601 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.276       |
|    cost_value_loss      | 6.53e-07    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 3000        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00047     |
|    reward_explained_... | 0.934       |
|    reward_value_loss    | 0.0522      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.81e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.413      |
|    reward_run           | 3.86        |
| rollout/                |             |
|    adjusted_reward      | 3.51        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 302         |
|    time_elapsed         | 2560        |
|    total_timesteps      | 3092480     |
| train/                  |             |
|    approx_kl            | 0.015713863 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.322       |
|    cost_value_loss      | 4.32e-07    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0228      |
|    n_updates            | 3010        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00126     |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0651      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.61e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 3.82        |
| rollout/                |             |
|    adjusted_reward      | 3.47        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.71e+03    |
| time/                   |             |
|    fps                  | 1207        |
|    iterations           | 303         |
|    time_elapsed         | 2568        |
|    total_timesteps      | 3102720     |
| train/                  |             |
|    approx_kl            | 0.016852167 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0418      |
|    cost_value_loss      | 5.77e-07    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00963     |
|    n_updates            | 3020        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000678    |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0457      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.83e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.81e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.412     |
|    reward_run           | 4.98       |
| rollout/                |            |
|    adjusted_reward      | 3.49       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.7e+03    |
| time/                   |            |
|    fps                  | 1208       |
|    iterations           | 304        |
|    time_elapsed         | 2576       |
|    total_timesteps      | 3112960    |
| train/                  |            |
|    approx_kl            | 0.01761887 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.515      |
|    cost_value_loss      | 5.44e-07   |
|    entropy_loss         | -3.8       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.015      |
|    n_updates            | 3030       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00089    |
|    reward_explained_... | 0.928      |
|    reward_value_loss    | 0.0522     |
|    std                  | 0.46       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.83e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.46e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.415     |
|    reward_run           | 3.52       |
| rollout/                |            |
|    adjusted_reward      | 3.46       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.73e+03   |
| time/                   |            |
|    fps                  | 1208       |
|    iterations           | 305        |
|    time_elapsed         | 2584       |
|    total_timesteps      | 3123200    |
| train/                  |            |
|    approx_kl            | 0.01569276 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.553      |
|    cost_value_loss      | 1.78e-06   |
|    entropy_loss         | -3.8       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0156     |
|    n_updates            | 3040       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000916   |
|    reward_explained_... | 0.949      |
|    reward_value_loss    | 0.0423     |
|    std                  | 0.461      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.421      |
|    reward_run           | 4.65        |
| rollout/                |             |
|    adjusted_reward      | 3.26        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.72e+03    |
| time/                   |             |
|    fps                  | 1208        |
|    iterations           | 306         |
|    time_elapsed         | 2592        |
|    total_timesteps      | 3133440     |
| train/                  |             |
|    approx_kl            | 0.016100392 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.719       |
|    cost_value_loss      | 1.44e-06    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0508      |
|    n_updates            | 3050        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000919    |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0507      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 4.09        |
| rollout/                |             |
|    adjusted_reward      | 3.31        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 1208        |
|    iterations           | 307         |
|    time_elapsed         | 2601        |
|    total_timesteps      | 3143680     |
| train/                  |             |
|    approx_kl            | 0.019865993 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.814       |
|    cost_value_loss      | 7.47e-06    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0171      |
|    n_updates            | 3060        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000345    |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0667      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.61e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 4.14        |
| rollout/                |             |
|    adjusted_reward      | 3.29        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.68e+03    |
| time/                   |             |
|    fps                  | 1208        |
|    iterations           | 308         |
|    time_elapsed         | 2609        |
|    total_timesteps      | 3153920     |
| train/                  |             |
|    approx_kl            | 0.016013512 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.251       |
|    cost_value_loss      | 6.05e-07    |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0314      |
|    n_updates            | 3070        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00095     |
|    reward_explained_... | 0.925       |
|    reward_value_loss    | 0.0609      |
|    std                  | 0.463       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.3e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 3.21        |
| rollout/                |             |
|    adjusted_reward      | 3.54        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.68e+03    |
| time/                   |             |
|    fps                  | 1209        |
|    iterations           | 309         |
|    time_elapsed         | 2616        |
|    total_timesteps      | 3164160     |
| train/                  |             |
|    approx_kl            | 0.021750975 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.547       |
|    cost_value_loss      | 2.05e-06    |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0171      |
|    n_updates            | 3080        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000365    |
|    reward_explained_... | 0.916       |
|    reward_value_loss    | 0.0667      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.7e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.392      |
|    reward_run           | 2.92        |
| rollout/                |             |
|    adjusted_reward      | 3.63        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.69e+03    |
| time/                   |             |
|    fps                  | 1209        |
|    iterations           | 310         |
|    time_elapsed         | 2625        |
|    total_timesteps      | 3174400     |
| train/                  |             |
|    approx_kl            | 0.019830614 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.215       |
|    cost_value_loss      | 6.26e-07    |
|    entropy_loss         | -3.83       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0669      |
|    n_updates            | 3090        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000494    |
|    reward_explained_... | 0.937       |
|    reward_value_loss    | 0.0481      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.72e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.402      |
|    reward_run           | 3.79        |
| rollout/                |             |
|    adjusted_reward      | 3.24        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 1209        |
|    iterations           | 311         |
|    time_elapsed         | 2632        |
|    total_timesteps      | 3184640     |
| train/                  |             |
|    approx_kl            | 0.015984546 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.157       |
|    cost_value_loss      | 1.44e-07    |
|    entropy_loss         | -3.84       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00668     |
|    n_updates            | 3100        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.959       |
|    reward_value_loss    | 0.0305      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.83e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.66e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.425     |
|    reward_run           | 4.16       |
| rollout/                |            |
|    adjusted_reward      | 3.64       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.72e+03   |
| time/                   |            |
|    fps                  | 1209       |
|    iterations           | 312        |
|    time_elapsed         | 2640       |
|    total_timesteps      | 3194880    |
| train/                  |            |
|    approx_kl            | 0.01661401 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.546      |
|    cost_value_loss      | 9.83e-07   |
|    entropy_loss         | -3.83      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0177     |
|    n_updates            | 3110       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00103    |
|    reward_explained_... | 0.935      |
|    reward_value_loss    | 0.0583     |
|    std                  | 0.461      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.83e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.78e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.4        |
|    reward_run           | 2.09        |
| rollout/                |             |
|    adjusted_reward      | 3.53        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 1210        |
|    iterations           | 313         |
|    time_elapsed         | 2648        |
|    total_timesteps      | 3205120     |
| train/                  |             |
|    approx_kl            | 0.016617352 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.325       |
|    cost_value_loss      | 2.48e-07    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0142      |
|    n_updates            | 3120        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00121     |
|    reward_explained_... | 0.944       |
|    reward_value_loss    | 0.0419      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 4.61        |
| rollout/                |             |
|    adjusted_reward      | 3.6         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 1210        |
|    iterations           | 314         |
|    time_elapsed         | 2656        |
|    total_timesteps      | 3215360     |
| train/                  |             |
|    approx_kl            | 0.015389641 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.315       |
|    cost_value_loss      | 3.16e-07    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0138      |
|    n_updates            | 3130        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00106     |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.0377      |
|    std                  | 0.46        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.81e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 3.54        |
| rollout/                |             |
|    adjusted_reward      | 3.49        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.75e+03    |
| time/                   |             |
|    fps                  | 1210        |
|    iterations           | 315         |
|    time_elapsed         | 2664        |
|    total_timesteps      | 3225600     |
| train/                  |             |
|    approx_kl            | 0.017234659 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.238       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.568       |
|    cost_value_loss      | 4.31e-07    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0301      |
|    n_updates            | 3140        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000422    |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0404      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.7e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 4.71        |
| rollout/                |             |
|    adjusted_reward      | 3.59        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 1210        |
|    iterations           | 316         |
|    time_elapsed         | 2672        |
|    total_timesteps      | 3235840     |
| train/                  |             |
|    approx_kl            | 0.018772308 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.238       |
|    cost_value_loss      | 6.41e-07    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0123      |
|    n_updates            | 3150        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00103     |
|    reward_explained_... | 0.924       |
|    reward_value_loss    | 0.0594      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.73e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.418      |
|    reward_run           | 3.57        |
| rollout/                |             |
|    adjusted_reward      | 3.58        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 1211        |
|    iterations           | 317         |
|    time_elapsed         | 2680        |
|    total_timesteps      | 3246080     |
| train/                  |             |
|    approx_kl            | 0.019199485 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.243       |
|    cost_value_loss      | 2.28e-06    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0211      |
|    n_updates            | 3160        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00115     |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0403      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.82e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.431      |
|    reward_run           | 3.93        |
| rollout/                |             |
|    adjusted_reward      | 3.45        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.77e+03    |
| time/                   |             |
|    fps                  | 1211        |
|    iterations           | 318         |
|    time_elapsed         | 2688        |
|    total_timesteps      | 3256320     |
| train/                  |             |
|    approx_kl            | 0.015775995 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.496       |
|    cost_value_loss      | 5.18e-07    |
|    entropy_loss         | -3.82       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0318      |
|    n_updates            | 3170        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00134     |
|    reward_explained_... | 0.947       |
|    reward_value_loss    | 0.0437      |
|    std                  | 0.462       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.87e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.82e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.409      |
|    reward_run           | 3.98        |
| rollout/                |             |
|    adjusted_reward      | 3.55        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 1211        |
|    iterations           | 319         |
|    time_elapsed         | 2696        |
|    total_timesteps      | 3266560     |
| train/                  |             |
|    approx_kl            | 0.019179258 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.816       |
|    cost_value_loss      | 9.69e-07    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0232      |
|    n_updates            | 3180        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000492    |
|    reward_explained_... | 0.932       |
|    reward_value_loss    | 0.0604      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.88e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.88e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.42       |
|    reward_run           | 4.02        |
| rollout/                |             |
|    adjusted_reward      | 3.17        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.73e+03    |
| time/                   |             |
|    fps                  | 1211        |
|    iterations           | 320         |
|    time_elapsed         | 2704        |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.015953949 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.496       |
|    cost_value_loss      | 2.39e-06    |
|    entropy_loss         | -3.81       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0396      |
|    n_updates            | 3190        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00123     |
|    reward_explained_... | 0.937       |
|    reward_value_loss    | 0.0565      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.88e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.83e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.419     |
|    reward_run           | 3.16       |
| rollout/                |            |
|    adjusted_reward      | 3.36       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.71e+03   |
| time/                   |            |
|    fps                  | 1211       |
|    iterations           | 321        |
|    time_elapsed         | 2712       |
|    total_timesteps      | 3287040    |
| train/                  |            |
|    approx_kl            | 0.01749739 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.708      |
|    cost_value_loss      | 2.64e-06   |
|    entropy_loss         | -3.81      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0317     |
|    n_updates            | 3200       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00139    |
|    reward_explained_... | 0.92       |
|    reward_value_loss    | 0.0706     |
|    std                  | 0.461      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.88e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.71e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.419      |
|    reward_run           | 4.46        |
| rollout/                |             |
|    adjusted_reward      | 3.55        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 1211        |
|    iterations           | 322         |
|    time_elapsed         | 2720        |
|    total_timesteps      | 3297280     |
| train/                  |             |
|    approx_kl            | 0.018291393 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.507       |
|    cost_value_loss      | 8.77e-07    |
|    entropy_loss         | -3.8        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.045       |
|    n_updates            | 3210        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000302    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0652      |
|    std                  | 0.461       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.88e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.74e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.405     |
|    reward_run           | 2.22       |
| rollout/                |            |
|    adjusted_reward      | 3.28       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.7e+03    |
| time/                   |            |
|    fps                  | 1212       |
|    iterations           | 323        |
|    time_elapsed         | 2728       |
|    total_timesteps      | 3307520    |
| train/                  |            |
|    approx_kl            | 0.01548017 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.245      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.276      |
|    cost_value_loss      | 7.54e-07   |
|    entropy_loss         | -3.8       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0303     |
|    n_updates            | 3220       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00123    |
|    reward_explained_... | 0.938      |
|    reward_value_loss    | 0.0454     |
|    std                  | 0.46       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 1.93e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 1.93e+03  |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.419    |
|    reward_run           | 3.83      |
| rollout/                |           |
|    adjusted_reward      | 3.27      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 1.66e+03  |
| time/                   |           |
|    fps                  | 1212      |
|    iterations           | 324       |
|    time_elapsed         | 2736      |
|    total_timesteps      | 3317760   |
| train/                  |           |
|    approx_kl            | 0.0171097 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.228     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.547     |
|    cost_value_loss      | 1.47e-06  |
|    entropy_loss         | -3.79     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0146    |
|    n_updates            | 3230      |
|    nu                   | 6.29      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00119   |
|    reward_explained_... | 0.931     |
|    reward_value_loss    | 0.0631    |
|    std                  | 0.459     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.93e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.432      |
|    reward_run           | 4.8         |
| rollout/                |             |
|    adjusted_reward      | 3.45        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.69e+03    |
| time/                   |             |
|    fps                  | 1212        |
|    iterations           | 325         |
|    time_elapsed         | 2744        |
|    total_timesteps      | 3328000     |
| train/                  |             |
|    approx_kl            | 0.019285917 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.24        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.819       |
|    cost_value_loss      | 4.14e-05    |
|    entropy_loss         | -3.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0335      |
|    n_updates            | 3240        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00124     |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0701      |
|    std                  | 0.459       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.93e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.43e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 4.18        |
| rollout/                |             |
|    adjusted_reward      | 3.44        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.7e+03     |
| time/                   |             |
|    fps                  | 1212        |
|    iterations           | 326         |
|    time_elapsed         | 2752        |
|    total_timesteps      | 3338240     |
| train/                  |             |
|    approx_kl            | 0.021040706 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.602       |
|    cost_value_loss      | 8.18e-07    |
|    entropy_loss         | -3.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0709      |
|    n_updates            | 3250        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00138     |
|    reward_explained_... | 0.923       |
|    reward_value_loss    | 0.0635      |
|    std                  | 0.459       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.93e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.405      |
|    reward_run           | 4.21        |
| rollout/                |             |
|    adjusted_reward      | 3.72        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.71e+03    |
| time/                   |             |
|    fps                  | 1213        |
|    iterations           | 327         |
|    time_elapsed         | 2760        |
|    total_timesteps      | 3348480     |
| train/                  |             |
|    approx_kl            | 0.017653087 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.679       |
|    cost_value_loss      | 3.85e-07    |
|    entropy_loss         | -3.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0141      |
|    n_updates            | 3260        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000755    |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0441      |
|    std                  | 0.459       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.95e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.95e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.434      |
|    reward_run           | 4.33        |
| rollout/                |             |
|    adjusted_reward      | 3.72        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.76e+03    |
| time/                   |             |
|    fps                  | 1213        |
|    iterations           | 328         |
|    time_elapsed         | 2767        |
|    total_timesteps      | 3358720     |
| train/                  |             |
|    approx_kl            | 0.016219854 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.661       |
|    cost_value_loss      | 2.43e-07    |
|    entropy_loss         | -3.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0378      |
|    n_updates            | 3270        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00096     |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0434      |
|    std                  | 0.458       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.95e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.91e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.425     |
|    reward_run           | 4.46       |
| rollout/                |            |
|    adjusted_reward      | 3.45       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.77e+03   |
| time/                   |            |
|    fps                  | 1213       |
|    iterations           | 329        |
|    time_elapsed         | 2775       |
|    total_timesteps      | 3368960    |
| train/                  |            |
|    approx_kl            | 0.01664262 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.243      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.597      |
|    cost_value_loss      | 2.62e-07   |
|    entropy_loss         | -3.78      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0136     |
|    n_updates            | 3280       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00106    |
|    reward_explained_... | 0.942      |
|    reward_value_loss    | 0.0405     |
|    std                  | 0.459      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.95e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.73e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.403     |
|    reward_run           | 5.07       |
| rollout/                |            |
|    adjusted_reward      | 3.62       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.79e+03   |
| time/                   |            |
|    fps                  | 1213       |
|    iterations           | 330        |
|    time_elapsed         | 2783       |
|    total_timesteps      | 3379200    |
| train/                  |            |
|    approx_kl            | 0.01573692 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.522      |
|    cost_value_loss      | 3.33e-07   |
|    entropy_loss         | -3.78      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0158     |
|    n_updates            | 3290       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00148    |
|    reward_explained_... | 0.939      |
|    reward_value_loss    | 0.056      |
|    std                  | 0.459      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.95e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.8e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 4.95        |
| rollout/                |             |
|    adjusted_reward      | 3.49        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.79e+03    |
| time/                   |             |
|    fps                  | 1214        |
|    iterations           | 331         |
|    time_elapsed         | 2791        |
|    total_timesteps      | 3389440     |
| train/                  |             |
|    approx_kl            | 0.015099752 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.613       |
|    cost_value_loss      | 2.98e-07    |
|    entropy_loss         | -3.78       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0226      |
|    n_updates            | 3300        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00104     |
|    reward_explained_... | 0.937       |
|    reward_value_loss    | 0.0502      |
|    std                  | 0.458       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.98e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.401      |
|    reward_run           | 4.13        |
| rollout/                |             |
|    adjusted_reward      | 3.58        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 1214        |
|    iterations           | 332         |
|    time_elapsed         | 2799        |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.016599018 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.517       |
|    cost_value_loss      | 3.3e-07     |
|    entropy_loss         | -3.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 3310        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00113     |
|    reward_explained_... | 0.897       |
|    reward_value_loss    | 0.0742      |
|    std                  | 0.459       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.7e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.414      |
|    reward_run           | 3.49        |
| rollout/                |             |
|    adjusted_reward      | 3.61        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.77e+03    |
| time/                   |             |
|    fps                  | 1214        |
|    iterations           | 333         |
|    time_elapsed         | 2806        |
|    total_timesteps      | 3409920     |
| train/                  |             |
|    approx_kl            | 0.017733794 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.217       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.643       |
|    cost_value_loss      | 2.77e-06    |
|    entropy_loss         | -3.77       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0553      |
|    n_updates            | 3320        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00121     |
|    reward_explained_... | 0.933       |
|    reward_value_loss    | 0.0525      |
|    std                  | 0.458       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.7e+03    |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.418     |
|    reward_run           | 2.5        |
| rollout/                |            |
|    adjusted_reward      | 3.38       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.76e+03   |
| time/                   |            |
|    fps                  | 1215       |
|    iterations           | 334        |
|    time_elapsed         | 2814       |
|    total_timesteps      | 3420160    |
| train/                  |            |
|    approx_kl            | 0.01526154 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.815      |
|    cost_value_loss      | 1.8e-06    |
|    entropy_loss         | -3.76      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0265     |
|    n_updates            | 3330       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00131    |
|    reward_explained_... | 0.956      |
|    reward_value_loss    | 0.0417     |
|    std                  | 0.457      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.75e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.411     |
|    reward_run           | 4.43       |
| rollout/                |            |
|    adjusted_reward      | 3.53       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.76e+03   |
| time/                   |            |
|    fps                  | 1215       |
|    iterations           | 335        |
|    time_elapsed         | 2822       |
|    total_timesteps      | 3430400    |
| train/                  |            |
|    approx_kl            | 0.01650362 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.837      |
|    cost_value_loss      | 1.62e-06   |
|    entropy_loss         | -3.75      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0407     |
|    n_updates            | 3340       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00089    |
|    reward_explained_... | 0.939      |
|    reward_value_loss    | 0.0593     |
|    std                  | 0.456      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.91e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 5.16        |
| rollout/                |             |
|    adjusted_reward      | 3.46        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.75e+03    |
| time/                   |             |
|    fps                  | 1215        |
|    iterations           | 336         |
|    time_elapsed         | 2830        |
|    total_timesteps      | 3440640     |
| train/                  |             |
|    approx_kl            | 0.019181374 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.526       |
|    cost_value_loss      | 4.59e-07    |
|    entropy_loss         | -3.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.025       |
|    n_updates            | 3350        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00134     |
|    reward_explained_... | 0.924       |
|    reward_value_loss    | 0.0563      |
|    std                  | 0.455       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.86e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 4.41        |
| rollout/                |             |
|    adjusted_reward      | 3.78        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.78e+03    |
| time/                   |             |
|    fps                  | 1215        |
|    iterations           | 337         |
|    time_elapsed         | 2839        |
|    total_timesteps      | 3450880     |
| train/                  |             |
|    approx_kl            | 0.020799408 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.658       |
|    cost_value_loss      | 2.44e-06    |
|    entropy_loss         | -3.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.032       |
|    n_updates            | 3360        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00116     |
|    reward_explained_... | 0.946       |
|    reward_value_loss    | 0.0532      |
|    std                  | 0.456       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.424      |
|    reward_run           | 4.6         |
| rollout/                |             |
|    adjusted_reward      | 3.74        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.79e+03    |
| time/                   |             |
|    fps                  | 1215        |
|    iterations           | 338         |
|    time_elapsed         | 2846        |
|    total_timesteps      | 3461120     |
| train/                  |             |
|    approx_kl            | 0.017130783 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.396       |
|    cost_value_loss      | 7.36e-07    |
|    entropy_loss         | -3.74       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0278      |
|    n_updates            | 3370        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00136     |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0528      |
|    std                  | 0.456       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.64e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.418     |
|    reward_run           | 4.53       |
| rollout/                |            |
|    adjusted_reward      | 3.71       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.82e+03   |
| time/                   |            |
|    fps                  | 1216       |
|    iterations           | 339        |
|    time_elapsed         | 2854       |
|    total_timesteps      | 3471360    |
| train/                  |            |
|    approx_kl            | 0.01817823 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.229      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.804      |
|    cost_value_loss      | 6.28e-07   |
|    entropy_loss         | -3.72      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0157     |
|    n_updates            | 3380       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00127    |
|    reward_explained_... | 0.957      |
|    reward_value_loss    | 0.037      |
|    std                  | 0.455      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.72e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 3.37        |
| rollout/                |             |
|    adjusted_reward      | 3.64        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.83e+03    |
| time/                   |             |
|    fps                  | 1216        |
|    iterations           | 340         |
|    time_elapsed         | 2862        |
|    total_timesteps      | 3481600     |
| train/                  |             |
|    approx_kl            | 0.018676776 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.399       |
|    cost_value_loss      | 3.5e-07     |
|    entropy_loss         | -3.72       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0257      |
|    n_updates            | 3390        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00141     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0537      |
|    std                  | 0.455       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.63e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.419      |
|    reward_run           | 4.3         |
| rollout/                |             |
|    adjusted_reward      | 3.92        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.87e+03    |
| time/                   |             |
|    fps                  | 1217        |
|    iterations           | 341         |
|    time_elapsed         | 2867        |
|    total_timesteps      | 3491840     |
| train/                  |             |
|    approx_kl            | 0.015542564 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.21        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.524       |
|    cost_value_loss      | 4.23e-07    |
|    entropy_loss         | -3.73       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0897      |
|    n_updates            | 3400        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00109     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0527      |
|    std                  | 0.455       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.71e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.405     |
|    reward_run           | 2.56       |
| rollout/                |            |
|    adjusted_reward      | 3.62       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.88e+03   |
| time/                   |            |
|    fps                  | 1217       |
|    iterations           | 342        |
|    time_elapsed         | 2875       |
|    total_timesteps      | 3502080    |
| train/                  |            |
|    approx_kl            | 0.01554442 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.253      |
|    cost_value_loss      | 4.21e-07   |
|    entropy_loss         | -3.73      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0168     |
|    n_updates            | 3410       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0013     |
|    reward_explained_... | 0.959      |
|    reward_value_loss    | 0.0297     |
|    std                  | 0.454      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.95e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.413     |
|    reward_run           | 3.97       |
| rollout/                |            |
|    adjusted_reward      | 3.62       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.84e+03   |
| time/                   |            |
|    fps                  | 1218       |
|    iterations           | 343        |
|    time_elapsed         | 2882       |
|    total_timesteps      | 3512320    |
| train/                  |            |
|    approx_kl            | 0.02137399 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.184      |
|    cost_value_loss      | 2.72e-07   |
|    entropy_loss         | -3.72      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0222     |
|    n_updates            | 3420       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00136    |
|    reward_explained_... | 0.94       |
|    reward_value_loss    | 0.0499     |
|    std                  | 0.455      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 1.98e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.79e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.399     |
|    reward_run           | 2.53       |
| rollout/                |            |
|    adjusted_reward      | 3.67       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.86e+03   |
| time/                   |            |
|    fps                  | 1218       |
|    iterations           | 344        |
|    time_elapsed         | 2890       |
|    total_timesteps      | 3522560    |
| train/                  |            |
|    approx_kl            | 0.01728529 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.27       |
|    cost_value_loss      | 1.86e-06   |
|    entropy_loss         | -3.72      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0297     |
|    n_updates            | 3430       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00135    |
|    reward_explained_... | 0.908      |
|    reward_value_loss    | 0.0676     |
|    std                  | 0.454      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.98e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.413      |
|    reward_run           | 3.87        |
| rollout/                |             |
|    adjusted_reward      | 3.69        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.85e+03    |
| time/                   |             |
|    fps                  | 1219        |
|    iterations           | 345         |
|    time_elapsed         | 2898        |
|    total_timesteps      | 3532800     |
| train/                  |             |
|    approx_kl            | 0.016093627 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.607       |
|    cost_value_loss      | 1.46e-06    |
|    entropy_loss         | -3.71       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0135      |
|    n_updates            | 3440        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.921       |
|    reward_value_loss    | 0.0562      |
|    std                  | 0.453       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.04e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 5.21        |
| rollout/                |             |
|    adjusted_reward      | 3.64        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.82e+03    |
| time/                   |             |
|    fps                  | 1219        |
|    iterations           | 346         |
|    time_elapsed         | 2906        |
|    total_timesteps      | 3543040     |
| train/                  |             |
|    approx_kl            | 0.016840661 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.248       |
|    cost_value_loss      | 7.36e-07    |
|    entropy_loss         | -3.7        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0168      |
|    n_updates            | 3450        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00148     |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0505      |
|    std                  | 0.453       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.6e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.421      |
|    reward_run           | 4.76        |
| rollout/                |             |
|    adjusted_reward      | 3.73        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.84e+03    |
| time/                   |             |
|    fps                  | 1219        |
|    iterations           | 347         |
|    time_elapsed         | 2913        |
|    total_timesteps      | 3553280     |
| train/                  |             |
|    approx_kl            | 0.021117233 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.53        |
|    cost_value_loss      | 9.69e-07    |
|    entropy_loss         | -3.7        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0174      |
|    n_updates            | 3460        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000101    |
|    reward_explained_... | 0.965       |
|    reward_value_loss    | 0.0267      |
|    std                  | 0.453       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.81e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.429      |
|    reward_run           | 4.35        |
| rollout/                |             |
|    adjusted_reward      | 3.91        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.86e+03    |
| time/                   |             |
|    fps                  | 1220        |
|    iterations           | 348         |
|    time_elapsed         | 2918        |
|    total_timesteps      | 3563520     |
| train/                  |             |
|    approx_kl            | 0.016963163 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.153       |
|    cost_value_loss      | 4.43e-07    |
|    entropy_loss         | -3.69       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0157      |
|    n_updates            | 3470        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00118     |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0409      |
|    std                  | 0.453       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.76e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 4.99        |
| rollout/                |             |
|    adjusted_reward      | 3.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.88e+03    |
| time/                   |             |
|    fps                  | 1221        |
|    iterations           | 349         |
|    time_elapsed         | 2926        |
|    total_timesteps      | 3573760     |
| train/                  |             |
|    approx_kl            | 0.017567847 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.146       |
|    cost_value_loss      | 1.72e-06    |
|    entropy_loss         | -3.69       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0203      |
|    n_updates            | 3480        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00139     |
|    reward_explained_... | 0.97        |
|    reward_value_loss    | 0.0233      |
|    std                  | 0.452       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.72e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.398      |
|    reward_run           | 2.59        |
| rollout/                |             |
|    adjusted_reward      | 3.77        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.9e+03     |
| time/                   |             |
|    fps                  | 1221        |
|    iterations           | 350         |
|    time_elapsed         | 2934        |
|    total_timesteps      | 3584000     |
| train/                  |             |
|    approx_kl            | 0.017190147 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.182       |
|    cost_value_loss      | 2.95e-07    |
|    entropy_loss         | -3.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0366      |
|    n_updates            | 3490        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00105     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0422      |
|    std                  | 0.451       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.65e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 4.05        |
| rollout/                |             |
|    adjusted_reward      | 3.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.91e+03    |
| time/                   |             |
|    fps                  | 1221        |
|    iterations           | 351         |
|    time_elapsed         | 2943        |
|    total_timesteps      | 3594240     |
| train/                  |             |
|    approx_kl            | 0.023048628 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.459       |
|    cost_value_loss      | 8.35e-06    |
|    entropy_loss         | -3.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0224      |
|    n_updates            | 3500        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000343    |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0343      |
|    std                  | 0.45        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.04e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.78e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.438     |
|    reward_run           | 4.9        |
| rollout/                |            |
|    adjusted_reward      | 4.07       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.94e+03   |
| time/                   |            |
|    fps                  | 1221       |
|    iterations           | 352        |
|    time_elapsed         | 2951       |
|    total_timesteps      | 3604480    |
| train/                  |            |
|    approx_kl            | 0.01546781 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.479      |
|    cost_value_loss      | 4.75e-07   |
|    entropy_loss         | -3.66      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0129     |
|    n_updates            | 3510       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000878   |
|    reward_explained_... | 0.934      |
|    reward_value_loss    | 0.0463     |
|    std                  | 0.451      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.9e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 4.59        |
| rollout/                |             |
|    adjusted_reward      | 3.68        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 1221        |
|    iterations           | 353         |
|    time_elapsed         | 2959        |
|    total_timesteps      | 3614720     |
| train/                  |             |
|    approx_kl            | 0.020380143 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.447       |
|    cost_value_loss      | 1.48e-07    |
|    entropy_loss         | -3.67       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 3520        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00158     |
|    reward_explained_... | 0.949       |
|    reward_value_loss    | 0.0362      |
|    std                  | 0.45        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.93e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 4.2         |
| rollout/                |             |
|    adjusted_reward      | 3.86        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 1221        |
|    iterations           | 354         |
|    time_elapsed         | 2966        |
|    total_timesteps      | 3624960     |
| train/                  |             |
|    approx_kl            | 0.021276575 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.476       |
|    cost_value_loss      | 7.45e-07    |
|    entropy_loss         | -3.66       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0355      |
|    n_updates            | 3530        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00107     |
|    reward_explained_... | 0.931       |
|    reward_value_loss    | 0.0522      |
|    std                  | 0.45        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.8e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.3         |
| rollout/                |             |
|    adjusted_reward      | 3.71        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.93e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 355         |
|    time_elapsed         | 2974        |
|    total_timesteps      | 3635200     |
| train/                  |             |
|    approx_kl            | 0.015732843 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.284       |
|    cost_value_loss      | 1.59e-07    |
|    entropy_loss         | -3.65       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0208      |
|    n_updates            | 3540        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00123     |
|    reward_explained_... | 0.96        |
|    reward_value_loss    | 0.0318      |
|    std                  | 0.449       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.04e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.426      |
|    reward_run           | 4.95        |
| rollout/                |             |
|    adjusted_reward      | 3.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.91e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 356         |
|    time_elapsed         | 2982        |
|    total_timesteps      | 3645440     |
| train/                  |             |
|    approx_kl            | 0.018185759 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.176       |
|    cost_value_loss      | 7.02e-07    |
|    entropy_loss         | -3.64       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 3550        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00113     |
|    reward_explained_... | 0.934       |
|    reward_value_loss    | 0.0464      |
|    std                  | 0.449       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.05e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.418      |
|    reward_run           | 4.76        |
| rollout/                |             |
|    adjusted_reward      | 3.88        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.89e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 357         |
|    time_elapsed         | 2990        |
|    total_timesteps      | 3655680     |
| train/                  |             |
|    approx_kl            | 0.019466912 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.112       |
|    cost_value_loss      | 1.95e-07    |
|    entropy_loss         | -3.63       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0219      |
|    n_updates            | 3560        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0366      |
|    std                  | 0.448       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.05e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.81e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.424      |
|    reward_run           | 4.87        |
| rollout/                |             |
|    adjusted_reward      | 3.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.91e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 358         |
|    time_elapsed         | 2998        |
|    total_timesteps      | 3665920     |
| train/                  |             |
|    approx_kl            | 0.016411973 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.229       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.271       |
|    cost_value_loss      | 4.43e-07    |
|    entropy_loss         | -3.63       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.02        |
|    n_updates            | 3570        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00127     |
|    reward_explained_... | 0.96        |
|    reward_value_loss    | 0.0335      |
|    std                  | 0.448       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.05e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.97e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.421      |
|    reward_run           | 4.57        |
| rollout/                |             |
|    adjusted_reward      | 4           |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 359         |
|    time_elapsed         | 3007        |
|    total_timesteps      | 3676160     |
| train/                  |             |
|    approx_kl            | 0.019704929 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.395       |
|    cost_value_loss      | 2.62e-07    |
|    entropy_loss         | -3.63       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0112      |
|    n_updates            | 3580        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00123     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0421      |
|    std                  | 0.448       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.05e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.93e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.432      |
|    reward_run           | 4.37        |
| rollout/                |             |
|    adjusted_reward      | 3.87        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.94e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 360         |
|    time_elapsed         | 3015        |
|    total_timesteps      | 3686400     |
| train/                  |             |
|    approx_kl            | 0.017038858 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.319       |
|    cost_value_loss      | 7.98e-07    |
|    entropy_loss         | -3.62       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00521     |
|    n_updates            | 3590        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00084     |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0339      |
|    std                  | 0.447       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.13e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 3.9         |
| rollout/                |             |
|    adjusted_reward      | 3.7         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.93e+03    |
| time/                   |             |
|    fps                  | 1222        |
|    iterations           | 361         |
|    time_elapsed         | 3023        |
|    total_timesteps      | 3696640     |
| train/                  |             |
|    approx_kl            | 0.015519765 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.23        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.00709     |
|    cost_value_loss      | 6.1e-07     |
|    entropy_loss         | -3.6        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00898     |
|    n_updates            | 3600        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00129     |
|    reward_explained_... | 0.963       |
|    reward_value_loss    | 0.0288      |
|    std                  | 0.446       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.02e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 3.46        |
| rollout/                |             |
|    adjusted_reward      | 3.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 1223        |
|    iterations           | 362         |
|    time_elapsed         | 3028        |
|    total_timesteps      | 3706880     |
| train/                  |             |
|    approx_kl            | 0.019708753 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.714       |
|    cost_value_loss      | 1.25e-06    |
|    entropy_loss         | -3.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 3610        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000557    |
|    reward_explained_... | 0.946       |
|    reward_value_loss    | 0.0412      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.02e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 4.02        |
| rollout/                |             |
|    adjusted_reward      | 3.83        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.92e+03    |
| time/                   |             |
|    fps                  | 1223        |
|    iterations           | 363         |
|    time_elapsed         | 3037        |
|    total_timesteps      | 3717120     |
| train/                  |             |
|    approx_kl            | 0.020800766 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.452       |
|    cost_value_loss      | 2.04e-06    |
|    entropy_loss         | -3.58       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0229      |
|    n_updates            | 3620        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000619    |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0429      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.94e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.394      |
|    reward_run           | 2.76        |
| rollout/                |             |
|    adjusted_reward      | 3.7         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.89e+03    |
| time/                   |             |
|    fps                  | 1223        |
|    iterations           | 364         |
|    time_elapsed         | 3045        |
|    total_timesteps      | 3727360     |
| train/                  |             |
|    approx_kl            | 0.017329821 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.613       |
|    cost_value_loss      | 5.79e-07    |
|    entropy_loss         | -3.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.022       |
|    n_updates            | 3630        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.937       |
|    reward_value_loss    | 0.0518      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.93e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.408      |
|    reward_run           | 1.61        |
| rollout/                |             |
|    adjusted_reward      | 3.79        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.88e+03    |
| time/                   |             |
|    fps                  | 1225        |
|    iterations           | 365         |
|    time_elapsed         | 3050        |
|    total_timesteps      | 3737600     |
| train/                  |             |
|    approx_kl            | 0.017339177 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.418       |
|    cost_value_loss      | 2.34e-07    |
|    entropy_loss         | -3.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0184      |
|    n_updates            | 3640        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00138     |
|    reward_explained_... | 0.915       |
|    reward_value_loss    | 0.0668      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.97e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 4.18        |
| rollout/                |             |
|    adjusted_reward      | 3.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.9e+03     |
| time/                   |             |
|    fps                  | 1225        |
|    iterations           | 366         |
|    time_elapsed         | 3058        |
|    total_timesteps      | 3747840     |
| train/                  |             |
|    approx_kl            | 0.019007565 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.21        |
|    cost_value_loss      | 2.54e-07    |
|    entropy_loss         | -3.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 3650        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00112     |
|    reward_explained_... | 0.952       |
|    reward_value_loss    | 0.0403      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.13e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.04e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.439     |
|    reward_run           | 5.49       |
| rollout/                |            |
|    adjusted_reward      | 3.72       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.88e+03   |
| time/                   |            |
|    fps                  | 1225       |
|    iterations           | 367        |
|    time_elapsed         | 3066       |
|    total_timesteps      | 3758080    |
| train/                  |            |
|    approx_kl            | 0.01845857 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.258      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.259      |
|    cost_value_loss      | 2.32e-07   |
|    entropy_loss         | -3.57      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0359     |
|    n_updates            | 3660       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00112    |
|    reward_explained_... | 0.953      |
|    reward_value_loss    | 0.0405     |
|    std                  | 0.444      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.09e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.432      |
|    reward_run           | 4.96        |
| rollout/                |             |
|    adjusted_reward      | 4.11        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.91e+03    |
| time/                   |             |
|    fps                  | 1226        |
|    iterations           | 368         |
|    time_elapsed         | 3071        |
|    total_timesteps      | 3768320     |
| train/                  |             |
|    approx_kl            | 0.016679363 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.412       |
|    cost_value_loss      | 2.99e-06    |
|    entropy_loss         | -3.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.013       |
|    n_updates            | 3670        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00104     |
|    reward_explained_... | 0.952       |
|    reward_value_loss    | 0.0453      |
|    std                  | 0.444       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.9e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 4.4         |
| rollout/                |             |
|    adjusted_reward      | 3.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.94e+03    |
| time/                   |             |
|    fps                  | 1226        |
|    iterations           | 369         |
|    time_elapsed         | 3079        |
|    total_timesteps      | 3778560     |
| train/                  |             |
|    approx_kl            | 0.021238925 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.108      |
|    cost_value_loss      | 1.52e-06    |
|    entropy_loss         | -3.57       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0129      |
|    n_updates            | 3680        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000391    |
|    reward_explained_... | 0.974       |
|    reward_value_loss    | 0.0188      |
|    std                  | 0.443       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.13e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.09e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.429     |
|    reward_run           | 4.85       |
| rollout/                |            |
|    adjusted_reward      | 4.01       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.95e+03   |
| time/                   |            |
|    fps                  | 1227       |
|    iterations           | 370        |
|    time_elapsed         | 3087       |
|    total_timesteps      | 3788800    |
| train/                  |            |
|    approx_kl            | 0.02033397 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.471      |
|    cost_value_loss      | 1.24e-06   |
|    entropy_loss         | -3.56      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0102     |
|    n_updates            | 3690       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0013     |
|    reward_explained_... | 0.951      |
|    reward_value_loss    | 0.04       |
|    std                  | 0.443      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 2.13e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 2.13e+03  |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.423    |
|    reward_run           | 4.31      |
| rollout/                |           |
|    adjusted_reward      | 3.91      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 1.97e+03  |
| time/                   |           |
|    fps                  | 1227      |
|    iterations           | 371       |
|    time_elapsed         | 3095      |
|    total_timesteps      | 3799040   |
| train/                  |           |
|    approx_kl            | 0.0188347 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.258     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.393     |
|    cost_value_loss      | 3.43e-07  |
|    entropy_loss         | -3.56     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0792    |
|    n_updates            | 3700      |
|    nu                   | 6.29      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00156   |
|    reward_explained_... | 0.94      |
|    reward_value_loss    | 0.0404    |
|    std                  | 0.443     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.99e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 4.84        |
| rollout/                |             |
|    adjusted_reward      | 3.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.99e+03    |
| time/                   |             |
|    fps                  | 1227        |
|    iterations           | 372         |
|    time_elapsed         | 3103        |
|    total_timesteps      | 3809280     |
| train/                  |             |
|    approx_kl            | 0.018437738 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.231       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.336       |
|    cost_value_loss      | 4.99e-07    |
|    entropy_loss         | -3.54       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0032      |
|    n_updates            | 3710        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00116     |
|    reward_explained_... | 0.957       |
|    reward_value_loss    | 0.0319      |
|    std                  | 0.442       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.02e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.84        |
| rollout/                |             |
|    adjusted_reward      | 3.76        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.96e+03    |
| time/                   |             |
|    fps                  | 1227        |
|    iterations           | 373         |
|    time_elapsed         | 3110        |
|    total_timesteps      | 3819520     |
| train/                  |             |
|    approx_kl            | 0.016442513 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.257       |
|    cost_value_loss      | 7.23e-07    |
|    entropy_loss         | -3.54       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0157      |
|    n_updates            | 3720        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000749    |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0469      |
|    std                  | 0.442       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.13e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.85e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.416     |
|    reward_run           | 4.4        |
| rollout/                |            |
|    adjusted_reward      | 3.55       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.93e+03   |
| time/                   |            |
|    fps                  | 1227       |
|    iterations           | 374        |
|    time_elapsed         | 3118       |
|    total_timesteps      | 3829760    |
| train/                  |            |
|    approx_kl            | 0.02034029 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.54       |
|    cost_value_loss      | 4.96e-07   |
|    entropy_loss         | -3.54      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0411     |
|    n_updates            | 3730       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00133    |
|    reward_explained_... | 0.956      |
|    reward_value_loss    | 0.0389     |
|    std                  | 0.441      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 2.13e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 2.08e+03  |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.408    |
|    reward_run           | 4.27      |
| rollout/                |           |
|    adjusted_reward      | 4.03      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 1.91e+03  |
| time/                   |           |
|    fps                  | 1228      |
|    iterations           | 375       |
|    time_elapsed         | 3126      |
|    total_timesteps      | 3840000   |
| train/                  |           |
|    approx_kl            | 0.0245303 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.272     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.385     |
|    cost_value_loss      | 1.34e-06  |
|    entropy_loss         | -3.53     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0418    |
|    n_updates            | 3740      |
|    nu                   | 6.29      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00139   |
|    reward_explained_... | 0.95      |
|    reward_value_loss    | 0.0475    |
|    std                  | 0.441     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.98e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.418      |
|    reward_run           | 4.07        |
| rollout/                |             |
|    adjusted_reward      | 3.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.93e+03    |
| time/                   |             |
|    fps                  | 1228        |
|    iterations           | 376         |
|    time_elapsed         | 3135        |
|    total_timesteps      | 3850240     |
| train/                  |             |
|    approx_kl            | 0.020196121 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.536       |
|    cost_value_loss      | 1.37e-06    |
|    entropy_loss         | -3.53       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0207      |
|    n_updates            | 3750        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00119     |
|    reward_explained_... | 0.903       |
|    reward_value_loss    | 0.0668      |
|    std                  | 0.441       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.11e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 4.8         |
| rollout/                |             |
|    adjusted_reward      | 4.02        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.93e+03    |
| time/                   |             |
|    fps                  | 1228        |
|    iterations           | 377         |
|    time_elapsed         | 3143        |
|    total_timesteps      | 3860480     |
| train/                  |             |
|    approx_kl            | 0.019673726 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.674       |
|    cost_value_loss      | 4.68e-07    |
|    entropy_loss         | -3.53       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00846     |
|    n_updates            | 3760        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000867    |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0378      |
|    std                  | 0.441       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.02e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.443      |
|    reward_run           | 4.97        |
| rollout/                |             |
|    adjusted_reward      | 4.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.97e+03    |
| time/                   |             |
|    fps                  | 1228        |
|    iterations           | 378         |
|    time_elapsed         | 3151        |
|    total_timesteps      | 3870720     |
| train/                  |             |
|    approx_kl            | 0.019611713 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.247       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.606       |
|    cost_value_loss      | 1.31e-06    |
|    entropy_loss         | -3.52       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 3770        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000854    |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.0328      |
|    std                  | 0.441       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.13e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.89e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.435     |
|    reward_run           | 4.8        |
| rollout/                |            |
|    adjusted_reward      | 4          |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.01e+03   |
| time/                   |            |
|    fps                  | 1228       |
|    iterations           | 379        |
|    time_elapsed         | 3159       |
|    total_timesteps      | 3880960    |
| train/                  |            |
|    approx_kl            | 0.01711819 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    cost_explained_va... | -0.452     |
|    cost_value_loss      | 2.7e-06    |
|    entropy_loss         | -3.53      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0271     |
|    n_updates            | 3780       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00131    |
|    reward_explained_... | 0.956      |
|    reward_value_loss    | 0.0312     |
|    std                  | 0.44       |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.94e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.43       |
|    reward_run           | 4.52        |
| rollout/                |             |
|    adjusted_reward      | 3.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 1228        |
|    iterations           | 380         |
|    time_elapsed         | 3167        |
|    total_timesteps      | 3891200     |
| train/                  |             |
|    approx_kl            | 0.016360639 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    cost_explained_va... | -0.305      |
|    cost_value_loss      | 1.02e-06    |
|    entropy_loss         | -3.52       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0145      |
|    n_updates            | 3790        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00153     |
|    reward_explained_... | 0.957       |
|    reward_value_loss    | 0.0312      |
|    std                  | 0.44        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 4.71        |
| rollout/                |             |
|    adjusted_reward      | 4.2         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1228        |
|    iterations           | 381         |
|    time_elapsed         | 3175        |
|    total_timesteps      | 3901440     |
| train/                  |             |
|    approx_kl            | 0.020174718 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.436       |
|    cost_value_loss      | 5.76e-07    |
|    entropy_loss         | -3.52       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0052      |
|    n_updates            | 3800        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000852    |
|    reward_explained_... | 0.946       |
|    reward_value_loss    | 0.0454      |
|    std                  | 0.44        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.13e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2e+03       |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.437      |
|    reward_run           | 4.78        |
| rollout/                |             |
|    adjusted_reward      | 3.98        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 1229        |
|    iterations           | 382         |
|    time_elapsed         | 3182        |
|    total_timesteps      | 3911680     |
| train/                  |             |
|    approx_kl            | 0.015442228 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.748       |
|    cost_value_loss      | 2.54e-07    |
|    entropy_loss         | -3.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0328      |
|    n_updates            | 3810        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00111     |
|    reward_explained_... | 0.963       |
|    reward_value_loss    | 0.026       |
|    std                  | 0.44        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.437      |
|    reward_run           | 5.03        |
| rollout/                |             |
|    adjusted_reward      | 4.05        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1229        |
|    iterations           | 383         |
|    time_elapsed         | 3190        |
|    total_timesteps      | 3921920     |
| train/                  |             |
|    approx_kl            | 0.016972493 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.596       |
|    cost_value_loss      | 5.19e-07    |
|    entropy_loss         | -3.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0455      |
|    n_updates            | 3820        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00127     |
|    reward_explained_... | 0.965       |
|    reward_value_loss    | 0.031       |
|    std                  | 0.439       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.77e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 3.97        |
| rollout/                |             |
|    adjusted_reward      | 4.04        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 1229        |
|    iterations           | 384         |
|    time_elapsed         | 3198        |
|    total_timesteps      | 3932160     |
| train/                  |             |
|    approx_kl            | 0.016514504 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.741       |
|    cost_value_loss      | 7.38e-07    |
|    entropy_loss         | -3.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00968     |
|    n_updates            | 3830        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00132     |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.0296      |
|    std                  | 0.439       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.96e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.422      |
|    reward_run           | 5.06        |
| rollout/                |             |
|    adjusted_reward      | 3.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 1229        |
|    iterations           | 385         |
|    time_elapsed         | 3206        |
|    total_timesteps      | 3942400     |
| train/                  |             |
|    approx_kl            | 0.017704811 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.768       |
|    cost_value_loss      | 3.92e-07    |
|    entropy_loss         | -3.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0186      |
|    n_updates            | 3840        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00129     |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.0314      |
|    std                  | 0.439       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.01e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 2.63        |
| rollout/                |             |
|    adjusted_reward      | 3.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 1229        |
|    iterations           | 386         |
|    time_elapsed         | 3213        |
|    total_timesteps      | 3952640     |
| train/                  |             |
|    approx_kl            | 0.019604476 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.665       |
|    cost_value_loss      | 5.78e-07    |
|    entropy_loss         | -3.51       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0496      |
|    n_updates            | 3850        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00103     |
|    reward_explained_... | 0.946       |
|    reward_value_loss    | 0.0417      |
|    std                  | 0.44        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.18e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.98e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.422     |
|    reward_run           | 4.39       |
| rollout/                |            |
|    adjusted_reward      | 4.05       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.01e+03   |
| time/                   |            |
|    fps                  | 1230       |
|    iterations           | 387        |
|    time_elapsed         | 3221       |
|    total_timesteps      | 3962880    |
| train/                  |            |
|    approx_kl            | 0.01976269 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.611      |
|    cost_value_loss      | 5.68e-07   |
|    entropy_loss         | -3.5       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0114     |
|    n_updates            | 3860       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00125    |
|    reward_explained_... | 0.937      |
|    reward_value_loss    | 0.0531     |
|    std                  | 0.439      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.86e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.445      |
|    reward_run           | 5.27        |
| rollout/                |             |
|    adjusted_reward      | 4.07        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 388         |
|    time_elapsed         | 3229        |
|    total_timesteps      | 3973120     |
| train/                  |             |
|    approx_kl            | 0.019743226 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.627       |
|    cost_value_loss      | 4.57e-07    |
|    entropy_loss         | -3.49       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0111      |
|    n_updates            | 3870        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0013      |
|    reward_explained_... | 0.963       |
|    reward_value_loss    | 0.0343      |
|    std                  | 0.439       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.86e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.442      |
|    reward_run           | 4.87        |
| rollout/                |             |
|    adjusted_reward      | 4.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 389         |
|    time_elapsed         | 3237        |
|    total_timesteps      | 3983360     |
| train/                  |             |
|    approx_kl            | 0.016634133 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.728       |
|    cost_value_loss      | 5.35e-07    |
|    entropy_loss         | -3.5        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0109      |
|    n_updates            | 3880        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00123     |
|    reward_explained_... | 0.952       |
|    reward_value_loss    | 0.0423      |
|    std                  | 0.439       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.07e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 3.27        |
| rollout/                |             |
|    adjusted_reward      | 4.24        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 390         |
|    time_elapsed         | 3245        |
|    total_timesteps      | 3993600     |
| train/                  |             |
|    approx_kl            | 0.020676974 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.244       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.7         |
|    cost_value_loss      | 2.17e-06    |
|    entropy_loss         | -3.5        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0138      |
|    n_updates            | 3890        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00159     |
|    reward_explained_... | 0.959       |
|    reward_value_loss    | 0.0306      |
|    std                  | 0.437       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.11e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.445      |
|    reward_run           | 5.17        |
| rollout/                |             |
|    adjusted_reward      | 3.97        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.04e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 391         |
|    time_elapsed         | 3253        |
|    total_timesteps      | 4003840     |
| train/                  |             |
|    approx_kl            | 0.020369077 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.653       |
|    cost_value_loss      | 1.91e-07    |
|    entropy_loss         | -3.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00576     |
|    n_updates            | 3900        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.963       |
|    reward_value_loss    | 0.029       |
|    std                  | 0.437       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 4.84        |
| rollout/                |             |
|    adjusted_reward      | 3.84        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 392         |
|    time_elapsed         | 3261        |
|    total_timesteps      | 4014080     |
| train/                  |             |
|    approx_kl            | 0.020836309 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.378       |
|    cost_value_loss      | 4.3e-06     |
|    entropy_loss         | -3.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0264      |
|    n_updates            | 3910        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00105     |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0428      |
|    std                  | 0.437       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.83e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.424      |
|    reward_run           | 4.06        |
| rollout/                |             |
|    adjusted_reward      | 4.24        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.04e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 393         |
|    time_elapsed         | 3269        |
|    total_timesteps      | 4024320     |
| train/                  |             |
|    approx_kl            | 0.021647215 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.301       |
|    cost_value_loss      | 1.94e-07    |
|    entropy_loss         | -3.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0234      |
|    n_updates            | 3920        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.94        |
|    reward_value_loss    | 0.0445      |
|    std                  | 0.437       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.1e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.411      |
|    reward_run           | 3.96        |
| rollout/                |             |
|    adjusted_reward      | 4.08        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 1230        |
|    iterations           | 394         |
|    time_elapsed         | 3277        |
|    total_timesteps      | 4034560     |
| train/                  |             |
|    approx_kl            | 0.020180548 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.217       |
|    cost_value_loss      | 1.39e-07    |
|    entropy_loss         | -3.48       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0216      |
|    n_updates            | 3930        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00143     |
|    reward_explained_... | 0.963       |
|    reward_value_loss    | 0.0333      |
|    std                  | 0.436       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.98e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 5.11        |
| rollout/                |             |
|    adjusted_reward      | 4           |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 1231        |
|    iterations           | 395         |
|    time_elapsed         | 3285        |
|    total_timesteps      | 4044800     |
| train/                  |             |
|    approx_kl            | 0.017589834 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.242       |
|    cost_value_loss      | 1.71e-07    |
|    entropy_loss         | -3.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0124      |
|    n_updates            | 3940        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00149     |
|    reward_explained_... | 0.958       |
|    reward_value_loss    | 0.0342      |
|    std                  | 0.436       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.92e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 3.52        |
| rollout/                |             |
|    adjusted_reward      | 4.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1231        |
|    iterations           | 396         |
|    time_elapsed         | 3293        |
|    total_timesteps      | 4055040     |
| train/                  |             |
|    approx_kl            | 0.022433605 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.533       |
|    cost_value_loss      | 8.04e-07    |
|    entropy_loss         | -3.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0189      |
|    n_updates            | 3950        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0013      |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0452      |
|    std                  | 0.435       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.03e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 4.88        |
| rollout/                |             |
|    adjusted_reward      | 4.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 1231        |
|    iterations           | 397         |
|    time_elapsed         | 3300        |
|    total_timesteps      | 4065280     |
| train/                  |             |
|    approx_kl            | 0.018959856 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.251       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.239       |
|    cost_value_loss      | 3.39e-07    |
|    entropy_loss         | -3.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0308      |
|    n_updates            | 3960        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00168     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0536      |
|    std                  | 0.436       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.03e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.423      |
|    reward_run           | 4.44        |
| rollout/                |             |
|    adjusted_reward      | 3.85        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1231        |
|    iterations           | 398         |
|    time_elapsed         | 3308        |
|    total_timesteps      | 4075520     |
| train/                  |             |
|    approx_kl            | 0.017301064 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.028       |
|    cost_value_loss      | 2.86e-07    |
|    entropy_loss         | -3.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 3970        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00109     |
|    reward_explained_... | 0.959       |
|    reward_value_loss    | 0.0324      |
|    std                  | 0.436       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.11e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 5.25        |
| rollout/                |             |
|    adjusted_reward      | 3.97        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 1232        |
|    iterations           | 399         |
|    time_elapsed         | 3316        |
|    total_timesteps      | 4085760     |
| train/                  |             |
|    approx_kl            | 0.016439002 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0836      |
|    cost_value_loss      | 5.79e-07    |
|    entropy_loss         | -3.46       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0126      |
|    n_updates            | 3980        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00149     |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0413      |
|    std                  | 0.435       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.82e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.437      |
|    reward_run           | 4.64        |
| rollout/                |             |
|    adjusted_reward      | 3.84        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.99e+03    |
| time/                   |             |
|    fps                  | 1232        |
|    iterations           | 400         |
|    time_elapsed         | 3324        |
|    total_timesteps      | 4096000     |
| train/                  |             |
|    approx_kl            | 0.018413985 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.857       |
|    cost_value_loss      | 8.84e-06    |
|    entropy_loss         | -3.45       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0165      |
|    n_updates            | 3990        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0016      |
|    reward_explained_... | 0.952       |
|    reward_value_loss    | 0.0446      |
|    std                  | 0.435       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.412      |
|    reward_run           | 3.68        |
| rollout/                |             |
|    adjusted_reward      | 3.96        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.98e+03    |
| time/                   |             |
|    fps                  | 1232        |
|    iterations           | 401         |
|    time_elapsed         | 3331        |
|    total_timesteps      | 4106240     |
| train/                  |             |
|    approx_kl            | 0.019918561 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.277       |
|    cost_value_loss      | 3.12e-07    |
|    entropy_loss         | -3.45       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0366      |
|    n_updates            | 4000        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0013      |
|    reward_explained_... | 0.942       |
|    reward_value_loss    | 0.0533      |
|    std                  | 0.435       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.12e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 3.09        |
| rollout/                |             |
|    adjusted_reward      | 3.8         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.94e+03    |
| time/                   |             |
|    fps                  | 1232        |
|    iterations           | 402         |
|    time_elapsed         | 3339        |
|    total_timesteps      | 4116480     |
| train/                  |             |
|    approx_kl            | 0.020054579 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.232       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.246       |
|    cost_value_loss      | 1.54e-07    |
|    entropy_loss         | -3.45       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0255      |
|    n_updates            | 4010        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00147     |
|    reward_explained_... | 0.97        |
|    reward_value_loss    | 0.0306      |
|    std                  | 0.435       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.18e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.73e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.437     |
|    reward_run           | 5.89       |
| rollout/                |            |
|    adjusted_reward      | 3.97       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.94e+03   |
| time/                   |            |
|    fps                  | 1232       |
|    iterations           | 403        |
|    time_elapsed         | 3347       |
|    total_timesteps      | 4126720    |
| train/                  |            |
|    approx_kl            | 0.01835863 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.702      |
|    cost_value_loss      | 4.69e-07   |
|    entropy_loss         | -3.45      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.024      |
|    n_updates            | 4020       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.000948   |
|    reward_explained_... | 0.952      |
|    reward_value_loss    | 0.0504     |
|    std                  | 0.435      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.16e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.423      |
|    reward_run           | 4.02        |
| rollout/                |             |
|    adjusted_reward      | 3.82        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.94e+03    |
| time/                   |             |
|    fps                  | 1233        |
|    iterations           | 404         |
|    time_elapsed         | 3355        |
|    total_timesteps      | 4136960     |
| train/                  |             |
|    approx_kl            | 0.017167302 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.0889      |
|    cost_value_loss      | 2.49e-06    |
|    entropy_loss         | -3.44       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0333      |
|    n_updates            | 4030        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00173     |
|    reward_explained_... | 0.94        |
|    reward_value_loss    | 0.0498      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.18e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.84e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.435      |
|    reward_run           | 4.03        |
| rollout/                |             |
|    adjusted_reward      | 3.92        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.95e+03    |
| time/                   |             |
|    fps                  | 1233        |
|    iterations           | 405         |
|    time_elapsed         | 3362        |
|    total_timesteps      | 4147200     |
| train/                  |             |
|    approx_kl            | 0.020251855 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.667       |
|    cost_value_loss      | 2.23e-06    |
|    entropy_loss         | -3.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 4040        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00118     |
|    reward_explained_... | 0.96        |
|    reward_value_loss    | 0.0432      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.23e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.437     |
|    reward_run           | 4.96       |
| rollout/                |            |
|    adjusted_reward      | 3.93       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 1.93e+03   |
| time/                   |            |
|    fps                  | 1233       |
|    iterations           | 406        |
|    time_elapsed         | 3370       |
|    total_timesteps      | 4157440    |
| train/                  |            |
|    approx_kl            | 0.02093085 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.268      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.461      |
|    cost_value_loss      | 6.69e-07   |
|    entropy_loss         | -3.43      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.028      |
|    n_updates            | 4050       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00124    |
|    reward_explained_... | 0.953      |
|    reward_value_loss    | 0.0455     |
|    std                  | 0.434      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.95e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.429      |
|    reward_run           | 3.39        |
| rollout/                |             |
|    adjusted_reward      | 4.31        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 1233        |
|    iterations           | 407         |
|    time_elapsed         | 3378        |
|    total_timesteps      | 4167680     |
| train/                  |             |
|    approx_kl            | 0.017295646 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.155       |
|    cost_value_loss      | 5.13e-07    |
|    entropy_loss         | -3.44       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 4060        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00147     |
|    reward_explained_... | 0.923       |
|    reward_value_loss    | 0.0611      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.96e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.435     |
|    reward_run           | 5.25       |
| rollout/                |            |
|    adjusted_reward      | 4.06       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2e+03      |
| time/                   |            |
|    fps                  | 1233       |
|    iterations           | 408        |
|    time_elapsed         | 3386       |
|    total_timesteps      | 4177920    |
| train/                  |            |
|    approx_kl            | 0.01827113 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.25       |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.132      |
|    cost_value_loss      | 2.74e-07   |
|    entropy_loss         | -3.43      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.024      |
|    n_updates            | 4070       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0015     |
|    reward_explained_... | 0.96       |
|    reward_value_loss    | 0.0293     |
|    std                  | 0.434      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.82e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 5.69        |
| rollout/                |             |
|    adjusted_reward      | 4.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 1233        |
|    iterations           | 409         |
|    time_elapsed         | 3394        |
|    total_timesteps      | 4188160     |
| train/                  |             |
|    approx_kl            | 0.018453985 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.319       |
|    cost_value_loss      | 1.29e-06    |
|    entropy_loss         | -3.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0222      |
|    n_updates            | 4080        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.043       |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.18e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.445     |
|    reward_run           | 5.03       |
| rollout/                |            |
|    adjusted_reward      | 3.98       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.03e+03   |
| time/                   |            |
|    fps                  | 1233       |
|    iterations           | 410        |
|    time_elapsed         | 3402       |
|    total_timesteps      | 4198400    |
| train/                  |            |
|    approx_kl            | 0.02037752 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.493      |
|    cost_value_loss      | 3.57e-07   |
|    entropy_loss         | -3.43      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0402     |
|    n_updates            | 4090       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0016     |
|    reward_explained_... | 0.957      |
|    reward_value_loss    | 0.0368     |
|    std                  | 0.433      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2e+03       |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.441      |
|    reward_run           | 4.84        |
| rollout/                |             |
|    adjusted_reward      | 4.15        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 1234        |
|    iterations           | 411         |
|    time_elapsed         | 3410        |
|    total_timesteps      | 4208640     |
| train/                  |             |
|    approx_kl            | 0.017104214 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.255       |
|    cost_value_loss      | 3.29e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0154      |
|    n_updates            | 4100        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00129     |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0438      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.23e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.15e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.437     |
|    reward_run           | 4.26       |
| rollout/                |            |
|    adjusted_reward      | 3.95       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.02e+03   |
| time/                   |            |
|    fps                  | 1234       |
|    iterations           | 412        |
|    time_elapsed         | 3418       |
|    total_timesteps      | 4218880    |
| train/                  |            |
|    approx_kl            | 0.01945446 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.276      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.264      |
|    cost_value_loss      | 1.02e-06   |
|    entropy_loss         | -3.42      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0205     |
|    n_updates            | 4110       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00039    |
|    reward_explained_... | 0.939      |
|    reward_value_loss    | 0.0383     |
|    std                  | 0.434      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.14e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.441      |
|    reward_run           | 4.57        |
| rollout/                |             |
|    adjusted_reward      | 4.06        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.02e+03    |
| time/                   |             |
|    fps                  | 1234        |
|    iterations           | 413         |
|    time_elapsed         | 3426        |
|    total_timesteps      | 4229120     |
| train/                  |             |
|    approx_kl            | 0.021065269 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.677       |
|    cost_value_loss      | 1.07e-06    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0747      |
|    n_updates            | 4120        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000791    |
|    reward_explained_... | 0.91        |
|    reward_value_loss    | 0.0711      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 3.87        |
| rollout/                |             |
|    adjusted_reward      | 4.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.03e+03    |
| time/                   |             |
|    fps                  | 1234        |
|    iterations           | 414         |
|    time_elapsed         | 3434        |
|    total_timesteps      | 4239360     |
| train/                  |             |
|    approx_kl            | 0.017766003 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.529       |
|    cost_value_loss      | 7.38e-07    |
|    entropy_loss         | -3.43       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0427      |
|    n_updates            | 4130        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00177     |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.0453      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.21e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.443      |
|    reward_run           | 4.99        |
| rollout/                |             |
|    adjusted_reward      | 4.26        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 1234        |
|    iterations           | 415         |
|    time_elapsed         | 3442        |
|    total_timesteps      | 4249600     |
| train/                  |             |
|    approx_kl            | 0.021430138 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.385       |
|    cost_value_loss      | 1.86e-06    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0192      |
|    n_updates            | 4140        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00157     |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0361      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.9e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.435      |
|    reward_run           | 4.44        |
| rollout/                |             |
|    adjusted_reward      | 4.17        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 1234        |
|    iterations           | 416         |
|    time_elapsed         | 3450        |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.020425152 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.271       |
|    cost_value_loss      | 6.67e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0237      |
|    n_updates            | 4150        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00151     |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0372      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.09e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.415      |
|    reward_run           | 2.32        |
| rollout/                |             |
|    adjusted_reward      | 4.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.08e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 417         |
|    time_elapsed         | 3456        |
|    total_timesteps      | 4270080     |
| train/                  |             |
|    approx_kl            | 0.020182535 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.539       |
|    cost_value_loss      | 5.98e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0253      |
|    n_updates            | 4160        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00157     |
|    reward_explained_... | 0.947       |
|    reward_value_loss    | 0.0445      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.93e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.437      |
|    reward_run           | 4.32        |
| rollout/                |             |
|    adjusted_reward      | 3.95        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.07e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 418         |
|    time_elapsed         | 3464        |
|    total_timesteps      | 4280320     |
| train/                  |             |
|    approx_kl            | 0.015384247 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.759       |
|    cost_value_loss      | 4.12e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 4170        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00155     |
|    reward_explained_... | 0.953       |
|    reward_value_loss    | 0.0422      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.8e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 5.31        |
| rollout/                |             |
|    adjusted_reward      | 4.21        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.07e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 419         |
|    time_elapsed         | 3472        |
|    total_timesteps      | 4290560     |
| train/                  |             |
|    approx_kl            | 0.022720588 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.283       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.62        |
|    cost_value_loss      | 1.08e-06    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0235      |
|    n_updates            | 4180        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00129     |
|    reward_explained_... | 0.953       |
|    reward_value_loss    | 0.0462      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.23e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.451      |
|    reward_run           | 5.22        |
| rollout/                |             |
|    adjusted_reward      | 4.32        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.07e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 420         |
|    time_elapsed         | 3480        |
|    total_timesteps      | 4300800     |
| train/                  |             |
|    approx_kl            | 0.017584845 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.272       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.594       |
|    cost_value_loss      | 2.65e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0146      |
|    n_updates            | 4190        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00102     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0479      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.28e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.446      |
|    reward_run           | 5.35        |
| rollout/                |             |
|    adjusted_reward      | 4.28        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.08e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 421         |
|    time_elapsed         | 3488        |
|    total_timesteps      | 4311040     |
| train/                  |             |
|    approx_kl            | 0.018498529 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.556       |
|    cost_value_loss      | 1.43e-07    |
|    entropy_loss         | -3.42       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0179      |
|    n_updates            | 4200        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00152     |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.0333      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.75e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.451      |
|    reward_run           | 5.3         |
| rollout/                |             |
|    adjusted_reward      | 4.22        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 1235        |
|    iterations           | 422         |
|    time_elapsed         | 3496        |
|    total_timesteps      | 4321280     |
| train/                  |             |
|    approx_kl            | 0.018186836 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.55        |
|    cost_value_loss      | 2.89e-07    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0343      |
|    n_updates            | 4210        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00181     |
|    reward_explained_... | 0.931       |
|    reward_value_loss    | 0.043       |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.436      |
|    reward_run           | 4.65        |
| rollout/                |             |
|    adjusted_reward      | 4.12        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.11e+03    |
| time/                   |             |
|    fps                  | 1236        |
|    iterations           | 423         |
|    time_elapsed         | 3504        |
|    total_timesteps      | 4331520     |
| train/                  |             |
|    approx_kl            | 0.021382004 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.293       |
|    cost_value_loss      | 3.43e-07    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0226      |
|    n_updates            | 4220        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00186     |
|    reward_explained_... | 0.942       |
|    reward_value_loss    | 0.0458      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.15e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 3.46        |
| rollout/                |             |
|    adjusted_reward      | 4.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1236        |
|    iterations           | 424         |
|    time_elapsed         | 3512        |
|    total_timesteps      | 4341760     |
| train/                  |             |
|    approx_kl            | 0.022635065 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.432       |
|    cost_value_loss      | 1.26e-06    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0159      |
|    n_updates            | 4230        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00183     |
|    reward_explained_... | 0.92        |
|    reward_value_loss    | 0.0596      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.25e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.435      |
|    reward_run           | 4.93        |
| rollout/                |             |
|    adjusted_reward      | 4.32        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1236        |
|    iterations           | 425         |
|    time_elapsed         | 3520        |
|    total_timesteps      | 4352000     |
| train/                  |             |
|    approx_kl            | 0.020291205 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.331       |
|    cost_value_loss      | 1.45e-06    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0293      |
|    n_updates            | 4240        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.93        |
|    reward_value_loss    | 0.0486      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.14e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.433      |
|    reward_run           | 5.1         |
| rollout/                |             |
|    adjusted_reward      | 4.33        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1236        |
|    iterations           | 426         |
|    time_elapsed         | 3527        |
|    total_timesteps      | 4362240     |
| train/                  |             |
|    approx_kl            | 0.015564941 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.163       |
|    cost_value_loss      | 3.95e-07    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 4250        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00116     |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0349      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 3.09        |
| rollout/                |             |
|    adjusted_reward      | 4.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1236        |
|    iterations           | 427         |
|    time_elapsed         | 3535        |
|    total_timesteps      | 4372480     |
| train/                  |             |
|    approx_kl            | 0.019003823 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.278       |
|    cost_value_loss      | 1.97e-07    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00834     |
|    n_updates            | 4260        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00167     |
|    reward_explained_... | 0.952       |
|    reward_value_loss    | 0.0305      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.75e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.428     |
|    reward_run           | 4          |
| rollout/                |            |
|    adjusted_reward      | 4.11       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.11e+03   |
| time/                   |            |
|    fps                  | 1237       |
|    iterations           | 428        |
|    time_elapsed         | 3540       |
|    total_timesteps      | 4382720    |
| train/                  |            |
|    approx_kl            | 0.02264196 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.284      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.265      |
|    cost_value_loss      | 2.83e-07   |
|    entropy_loss         | -3.4       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.022      |
|    n_updates            | 4270       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00185    |
|    reward_explained_... | 0.957      |
|    reward_value_loss    | 0.0368     |
|    std                  | 0.433      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.08e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 4.48        |
| rollout/                |             |
|    adjusted_reward      | 4.3         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 1238        |
|    iterations           | 429         |
|    time_elapsed         | 3547        |
|    total_timesteps      | 4392960     |
| train/                  |             |
|    approx_kl            | 0.016055388 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.242       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.275       |
|    cost_value_loss      | 9.13e-07    |
|    entropy_loss         | -3.39       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0262      |
|    n_updates            | 4280        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0017      |
|    reward_explained_... | 0.953       |
|    reward_value_loss    | 0.0362      |
|    std                  | 0.432       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.11e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.432      |
|    reward_run           | 5.11        |
| rollout/                |             |
|    adjusted_reward      | 4.14        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.1e+03     |
| time/                   |             |
|    fps                  | 1238        |
|    iterations           | 430         |
|    time_elapsed         | 3554        |
|    total_timesteps      | 4403200     |
| train/                  |             |
|    approx_kl            | 0.019610394 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.248       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.397       |
|    cost_value_loss      | 1.98e-07    |
|    entropy_loss         | -3.39       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.015       |
|    n_updates            | 4290        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00145     |
|    reward_explained_... | 0.946       |
|    reward_value_loss    | 0.035       |
|    std                  | 0.432       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.06e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 4.86        |
| rollout/                |             |
|    adjusted_reward      | 4.26        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.1e+03     |
| time/                   |             |
|    fps                  | 1238        |
|    iterations           | 431         |
|    time_elapsed         | 3562        |
|    total_timesteps      | 4413440     |
| train/                  |             |
|    approx_kl            | 0.018190943 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.256       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.512       |
|    cost_value_loss      | 1.9e-06     |
|    entropy_loss         | -3.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00762     |
|    n_updates            | 4300        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00144     |
|    reward_explained_... | 0.962       |
|    reward_value_loss    | 0.0313      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.26e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.44      |
|    reward_run           | 4.6        |
| rollout/                |            |
|    adjusted_reward      | 4.32       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.11e+03   |
| time/                   |            |
|    fps                  | 1238       |
|    iterations           | 432        |
|    time_elapsed         | 3571       |
|    total_timesteps      | 4423680    |
| train/                  |            |
|    approx_kl            | 0.01853073 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.342      |
|    cost_value_loss      | 7.28e-07   |
|    entropy_loss         | -3.4       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.06       |
|    n_updates            | 4310       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00167    |
|    reward_explained_... | 0.955      |
|    reward_value_loss    | 0.0388     |
|    std                  | 0.433      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.09e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.445      |
|    reward_run           | 5.68        |
| rollout/                |             |
|    adjusted_reward      | 4.28        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 1238        |
|    iterations           | 433         |
|    time_elapsed         | 3579        |
|    total_timesteps      | 4433920     |
| train/                  |             |
|    approx_kl            | 0.022752114 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.722       |
|    cost_value_loss      | 2.79e-07    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0426      |
|    n_updates            | 4320        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00128     |
|    reward_explained_... | 0.957       |
|    reward_value_loss    | 0.0342      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.91e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.429     |
|    reward_run           | 5.05       |
| rollout/                |            |
|    adjusted_reward      | 4.17       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.12e+03   |
| time/                   |            |
|    fps                  | 1239       |
|    iterations           | 434        |
|    time_elapsed         | 3586       |
|    total_timesteps      | 4444160    |
| train/                  |            |
|    approx_kl            | 0.02020908 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.269      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.297      |
|    cost_value_loss      | 4.97e-07   |
|    entropy_loss         | -3.42      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.011      |
|    n_updates            | 4330       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00171    |
|    reward_explained_... | 0.937      |
|    reward_value_loss    | 0.0409     |
|    std                  | 0.434      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.91e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.416     |
|    reward_run           | 3.39       |
| rollout/                |            |
|    adjusted_reward      | 4.4        |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.14e+03   |
| time/                   |            |
|    fps                  | 1239       |
|    iterations           | 435        |
|    time_elapsed         | 3594       |
|    total_timesteps      | 4454400    |
| train/                  |            |
|    approx_kl            | 0.01743075 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.47       |
|    cost_value_loss      | 3.1e-07    |
|    entropy_loss         | -3.42      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.034      |
|    n_updates            | 4340       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00171    |
|    reward_explained_... | 0.94       |
|    reward_value_loss    | 0.037      |
|    std                  | 0.434      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.442      |
|    reward_run           | 5.26        |
| rollout/                |             |
|    adjusted_reward      | 4.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.15e+03    |
| time/                   |             |
|    fps                  | 1239        |
|    iterations           | 436         |
|    time_elapsed         | 3600        |
|    total_timesteps      | 4464640     |
| train/                  |             |
|    approx_kl            | 0.024217883 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.262       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.676       |
|    cost_value_loss      | 1.49e-06    |
|    entropy_loss         | -3.41       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.016       |
|    n_updates            | 4350        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00166     |
|    reward_explained_... | 0.966       |
|    reward_value_loss    | 0.0279      |
|    std                  | 0.434       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.28e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.439      |
|    reward_run           | 4.29        |
| rollout/                |             |
|    adjusted_reward      | 4.16        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 1239        |
|    iterations           | 437         |
|    time_elapsed         | 3609        |
|    total_timesteps      | 4474880     |
| train/                  |             |
|    approx_kl            | 0.019231403 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.239       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.416       |
|    cost_value_loss      | 3.31e-07    |
|    entropy_loss         | -3.4        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00985     |
|    n_updates            | 4360        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.964       |
|    reward_value_loss    | 0.0243      |
|    std                  | 0.433       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.63e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.444     |
|    reward_run           | 2.07       |
| rollout/                |            |
|    adjusted_reward      | 4.3        |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.14e+03   |
| time/                   |            |
|    fps                  | 1240       |
|    iterations           | 438        |
|    time_elapsed         | 3616       |
|    total_timesteps      | 4485120    |
| train/                  |            |
|    approx_kl            | 0.02404103 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.27       |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.478      |
|    cost_value_loss      | 4.54e-07   |
|    entropy_loss         | -3.4       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00794    |
|    n_updates            | 4370       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0016     |
|    reward_explained_... | 0.954      |
|    reward_value_loss    | 0.0386     |
|    std                  | 0.433      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.07e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.432      |
|    reward_run           | 5.09        |
| rollout/                |             |
|    adjusted_reward      | 4.15        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 1240        |
|    iterations           | 439         |
|    time_elapsed         | 3624        |
|    total_timesteps      | 4495360     |
| train/                  |             |
|    approx_kl            | 0.019979369 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.316       |
|    cost_value_loss      | 2.58e-07    |
|    entropy_loss         | -3.39       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 4380        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00122     |
|    reward_explained_... | 0.958       |
|    reward_value_loss    | 0.0328      |
|    std                  | 0.432       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.13e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.426      |
|    reward_run           | 5.07        |
| rollout/                |             |
|    adjusted_reward      | 4.34        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 1240        |
|    iterations           | 440         |
|    time_elapsed         | 3632        |
|    total_timesteps      | 4505600     |
| train/                  |             |
|    approx_kl            | 0.017503416 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.28        |
|    cost_value_loss      | 6.52e-07    |
|    entropy_loss         | -3.38       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.018       |
|    n_updates            | 4390        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00117     |
|    reward_explained_... | 0.924       |
|    reward_value_loss    | 0.0545      |
|    std                  | 0.431       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.455      |
|    reward_run           | 5.12        |
| rollout/                |             |
|    adjusted_reward      | 4.26        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1240        |
|    iterations           | 441         |
|    time_elapsed         | 3640        |
|    total_timesteps      | 4515840     |
| train/                  |             |
|    approx_kl            | 0.019506864 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.265       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.172       |
|    cost_value_loss      | 3.85e-06    |
|    entropy_loss         | -3.36       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0325      |
|    n_updates            | 4400        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00106     |
|    reward_explained_... | 0.965       |
|    reward_value_loss    | 0.031       |
|    std                  | 0.43        |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.96e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.447      |
|    reward_run           | 5.37        |
| rollout/                |             |
|    adjusted_reward      | 4.28        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 1240        |
|    iterations           | 442         |
|    time_elapsed         | 3648        |
|    total_timesteps      | 4526080     |
| train/                  |             |
|    approx_kl            | 0.017545164 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.637       |
|    cost_value_loss      | 3.37e-07    |
|    entropy_loss         | -3.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0276      |
|    n_updates            | 4410        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00166     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0488      |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.26e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.453      |
|    reward_run           | 5.1         |
| rollout/                |             |
|    adjusted_reward      | 4.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 1240        |
|    iterations           | 443         |
|    time_elapsed         | 3655        |
|    total_timesteps      | 4536320     |
| train/                  |             |
|    approx_kl            | 0.018425703 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.268       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.725       |
|    cost_value_loss      | 1.54e-07    |
|    entropy_loss         | -3.35       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0146      |
|    n_updates            | 4420        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00153     |
|    reward_explained_... | 0.942       |
|    reward_value_loss    | 0.0411      |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.99e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.437      |
|    reward_run           | 5.54        |
| rollout/                |             |
|    adjusted_reward      | 4.44        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1241        |
|    iterations           | 444         |
|    time_elapsed         | 3662        |
|    total_timesteps      | 4546560     |
| train/                  |             |
|    approx_kl            | 0.024742866 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.684       |
|    cost_value_loss      | 5.8e-07     |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0158      |
|    n_updates            | 4430        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00166     |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.034       |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.04e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.441      |
|    reward_run           | 5.68        |
| rollout/                |             |
|    adjusted_reward      | 4.45        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.17e+03    |
| time/                   |             |
|    fps                  | 1242        |
|    iterations           | 445         |
|    time_elapsed         | 3668        |
|    total_timesteps      | 4556800     |
| train/                  |             |
|    approx_kl            | 0.020555016 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.61        |
|    cost_value_loss      | 1.37e-07    |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00724     |
|    n_updates            | 4440        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00147     |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0297      |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.28e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 1.75e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.438     |
|    reward_run           | 3.59       |
| rollout/                |            |
|    adjusted_reward      | 4.25       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.18e+03   |
| time/                   |            |
|    fps                  | 1242       |
|    iterations           | 446        |
|    time_elapsed         | 3676       |
|    total_timesteps      | 4567040    |
| train/                  |            |
|    approx_kl            | 0.02091338 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.672      |
|    cost_value_loss      | 2.05e-07   |
|    entropy_loss         | -3.34      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00683    |
|    n_updates            | 4450       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00144    |
|    reward_explained_... | 0.974      |
|    reward_value_loss    | 0.0183     |
|    std                  | 0.429      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.28e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2e+03       |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 5.11        |
| rollout/                |             |
|    adjusted_reward      | 4.39        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 1242        |
|    iterations           | 447         |
|    time_elapsed         | 3684        |
|    total_timesteps      | 4577280     |
| train/                  |             |
|    approx_kl            | 0.018764405 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.467       |
|    cost_value_loss      | 5.43e-07    |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 4460        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00142     |
|    reward_explained_... | 0.964       |
|    reward_value_loss    | 0.0295      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.32e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.32e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.425      |
|    reward_run           | 4.55        |
| rollout/                |             |
|    adjusted_reward      | 4.31        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 1242        |
|    iterations           | 448         |
|    time_elapsed         | 3692        |
|    total_timesteps      | 4587520     |
| train/                  |             |
|    approx_kl            | 0.021562126 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.541       |
|    cost_value_loss      | 2.07e-06    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00962     |
|    n_updates            | 4470        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.967       |
|    reward_value_loss    | 0.0264      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.32e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.14e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.438      |
|    reward_run           | 4.41        |
| rollout/                |             |
|    adjusted_reward      | 4.2         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1242        |
|    iterations           | 449         |
|    time_elapsed         | 3700        |
|    total_timesteps      | 4597760     |
| train/                  |             |
|    approx_kl            | 0.020657605 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.357       |
|    cost_value_loss      | 4.42e-07    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0253      |
|    n_updates            | 4480        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00142     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0495      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.39e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.444     |
|    reward_run           | 5.01       |
| rollout/                |            |
|    adjusted_reward      | 4.4        |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.15e+03   |
| time/                   |            |
|    fps                  | 1242       |
|    iterations           | 450        |
|    time_elapsed         | 3708       |
|    total_timesteps      | 4608000    |
| train/                  |            |
|    approx_kl            | 0.01907998 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.244      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.243      |
|    cost_value_loss      | 4.25e-07   |
|    entropy_loss         | -3.33      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0212     |
|    n_updates            | 4490       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0016     |
|    reward_explained_... | 0.95       |
|    reward_value_loss    | 0.0401     |
|    std                  | 0.428      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 2.39e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 1.8e+03   |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.447    |
|    reward_run           | 5.25      |
| rollout/                |           |
|    adjusted_reward      | 4.3       |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 2.16e+03  |
| time/                   |           |
|    fps                  | 1242      |
|    iterations           | 451       |
|    time_elapsed         | 3715      |
|    total_timesteps      | 4618240   |
| train/                  |           |
|    approx_kl            | 0.0198228 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.246     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.269     |
|    cost_value_loss      | 1.45e-06  |
|    entropy_loss         | -3.33     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0136    |
|    n_updates            | 4500      |
|    nu                   | 6.29      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00148   |
|    reward_explained_... | 0.961     |
|    reward_value_loss    | 0.0335    |
|    std                  | 0.428     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.33e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.439      |
|    reward_run           | 4.59        |
| rollout/                |             |
|    adjusted_reward      | 4.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.15e+03    |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 452         |
|    time_elapsed         | 3723        |
|    total_timesteps      | 4628480     |
| train/                  |             |
|    approx_kl            | 0.017529797 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.146       |
|    cost_value_loss      | 2.31e-07    |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0143      |
|    n_updates            | 4510        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00147     |
|    reward_explained_... | 0.948       |
|    reward_value_loss    | 0.0419      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.06e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.424     |
|    reward_run           | 4.41       |
| rollout/                |            |
|    adjusted_reward      | 4.1        |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.13e+03   |
| time/                   |            |
|    fps                  | 1243       |
|    iterations           | 453        |
|    time_elapsed         | 3731       |
|    total_timesteps      | 4638720    |
| train/                  |            |
|    approx_kl            | 0.01942248 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.263      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.609      |
|    cost_value_loss      | 9.47e-07   |
|    entropy_loss         | -3.33      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0177     |
|    n_updates            | 4520       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00129    |
|    reward_explained_... | 0.956      |
|    reward_value_loss    | 0.0437     |
|    std                  | 0.428      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.06e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.446      |
|    reward_run           | 4.37        |
| rollout/                |             |
|    adjusted_reward      | 4.18        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 454         |
|    time_elapsed         | 3739        |
|    total_timesteps      | 4648960     |
| train/                  |             |
|    approx_kl            | 0.017818872 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.271       |
|    cost_value_loss      | 1.27e-06    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0115      |
|    n_updates            | 4530        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00144     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0625      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.35e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.458      |
|    reward_run           | 5.59        |
| rollout/                |             |
|    adjusted_reward      | 4.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.1e+03     |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 455         |
|    time_elapsed         | 3747        |
|    total_timesteps      | 4659200     |
| train/                  |             |
|    approx_kl            | 0.018351253 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.249       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.254       |
|    cost_value_loss      | 5.8e-07     |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0279      |
|    n_updates            | 4540        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00153     |
|    reward_explained_... | 0.929       |
|    reward_value_loss    | 0.0565      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.26e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 4.66        |
| rollout/                |             |
|    adjusted_reward      | 4.25        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.1e+03     |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 456         |
|    time_elapsed         | 3755        |
|    total_timesteps      | 4669440     |
| train/                  |             |
|    approx_kl            | 0.017228158 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.246       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.409       |
|    cost_value_loss      | 6.47e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0363      |
|    n_updates            | 4550        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00145     |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0525      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.03
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.21e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.436     |
|    reward_run           | 3.55       |
| rollout/                |            |
|    adjusted_reward      | 4.28       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.1e+03    |
| time/                   |            |
|    fps                  | 1243       |
|    iterations           | 457        |
|    time_elapsed         | 3763       |
|    total_timesteps      | 4679680    |
| train/                  |            |
|    approx_kl            | 0.02528349 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.429      |
|    cost_value_loss      | 1.26e-06   |
|    entropy_loss         | -3.33      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0205     |
|    n_updates            | 4560       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00125    |
|    reward_explained_... | 0.947      |
|    reward_value_loss    | 0.0462     |
|    std                  | 0.427      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 5.02        |
| rollout/                |             |
|    adjusted_reward      | 4.02        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 458         |
|    time_elapsed         | 3771        |
|    total_timesteps      | 4689920     |
| train/                  |             |
|    approx_kl            | 0.021162275 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.541       |
|    cost_value_loss      | 1.41e-06    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0198      |
|    n_updates            | 4570        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00167     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0509      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.15e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.417      |
|    reward_run           | 3.26        |
| rollout/                |             |
|    adjusted_reward      | 4.09        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 459         |
|    time_elapsed         | 3778        |
|    total_timesteps      | 4700160     |
| train/                  |             |
|    approx_kl            | 0.018833477 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.26        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.527       |
|    cost_value_loss      | 6.33e-07    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0167      |
|    n_updates            | 4580        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.931       |
|    reward_value_loss    | 0.0551      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.26e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 5.2         |
| rollout/                |             |
|    adjusted_reward      | 4.3         |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 1243        |
|    iterations           | 460         |
|    time_elapsed         | 3787        |
|    total_timesteps      | 4710400     |
| train/                  |             |
|    approx_kl            | 0.022867495 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.337       |
|    cost_value_loss      | 1.07e-06    |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.028       |
|    n_updates            | 4590        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00184     |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0544      |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.12e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.42      |
|    reward_run           | 4.54       |
| rollout/                |            |
|    adjusted_reward      | 4.41       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.11e+03   |
| time/                   |            |
|    fps                  | 1243       |
|    iterations           | 461        |
|    time_elapsed         | 3794       |
|    total_timesteps      | 4720640    |
| train/                  |            |
|    approx_kl            | 0.02041611 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.256      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.648      |
|    cost_value_loss      | 3.82e-07   |
|    entropy_loss         | -3.33      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0508     |
|    n_updates            | 4600       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00174    |
|    reward_explained_... | 0.93       |
|    reward_value_loss    | 0.0503     |
|    std                  | 0.427      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.429      |
|    reward_run           | 5.6         |
| rollout/                |             |
|    adjusted_reward      | 4.09        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.09e+03    |
| time/                   |             |
|    fps                  | 1244        |
|    iterations           | 462         |
|    time_elapsed         | 3802        |
|    total_timesteps      | 4730880     |
| train/                  |             |
|    approx_kl            | 0.020159196 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.622       |
|    cost_value_loss      | 2.07e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 4610        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00158     |
|    reward_explained_... | 0.953       |
|    reward_value_loss    | 0.0377      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.85e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 5.06        |
| rollout/                |             |
|    adjusted_reward      | 3.79        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.06e+03    |
| time/                   |             |
|    fps                  | 1244        |
|    iterations           | 463         |
|    time_elapsed         | 3811        |
|    total_timesteps      | 4741120     |
| train/                  |             |
|    approx_kl            | 0.019182777 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.349       |
|    cost_value_loss      | 4.97e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 4620        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00168     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0521      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 1 due to reaching max kl: 0.03
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.32e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 4.77        |
| rollout/                |             |
|    adjusted_reward      | 4.13        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.07e+03    |
| time/                   |             |
|    fps                  | 1244        |
|    iterations           | 464         |
|    time_elapsed         | 3816        |
|    total_timesteps      | 4751360     |
| train/                  |             |
|    approx_kl            | 0.026783952 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.288       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.564       |
|    cost_value_loss      | 2.98e-06    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0435      |
|    n_updates            | 4630        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.000691    |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.0784      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.17e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 5.67        |
| rollout/                |             |
|    adjusted_reward      | 3.68        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2e+03       |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 465         |
|    time_elapsed         | 3824        |
|    total_timesteps      | 4761600     |
| train/                  |             |
|    approx_kl            | 0.020812174 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.202       |
|    cost_value_loss      | 4.26e-07    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0176      |
|    n_updates            | 4640        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0014      |
|    reward_explained_... | 0.919       |
|    reward_value_loss    | 0.0612      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.94e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.426      |
|    reward_run           | 4.45        |
| rollout/                |             |
|    adjusted_reward      | 4.32        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 1.99e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 466         |
|    time_elapsed         | 3832        |
|    total_timesteps      | 4771840     |
| train/                  |             |
|    approx_kl            | 0.024547849 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.671       |
|    cost_value_loss      | 1.51e-06    |
|    entropy_loss         | -3.34       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0721      |
|    n_updates            | 4650        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00185     |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0843      |
|    std                  | 0.429       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.92e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.446      |
|    reward_run           | 5.82        |
| rollout/                |             |
|    adjusted_reward      | 4.31        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.01e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 467         |
|    time_elapsed         | 3840        |
|    total_timesteps      | 4782080     |
| train/                  |             |
|    approx_kl            | 0.017488983 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.292       |
|    cost_value_loss      | 5.26e-07    |
|    entropy_loss         | -3.33       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0171      |
|    n_updates            | 4660        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00121     |
|    reward_explained_... | 0.957       |
|    reward_value_loss    | 0.0334      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.426      |
|    reward_run           | 4.98        |
| rollout/                |             |
|    adjusted_reward      | 4.11        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.05e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 468         |
|    time_elapsed         | 3848        |
|    total_timesteps      | 4792320     |
| train/                  |             |
|    approx_kl            | 0.022983175 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.355       |
|    cost_value_loss      | 2.02e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0221      |
|    n_updates            | 4670        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00166     |
|    reward_explained_... | 0.95        |
|    reward_value_loss    | 0.0357      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.31e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.416      |
|    reward_run           | 2.3         |
| rollout/                |             |
|    adjusted_reward      | 4.35        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.08e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 469         |
|    time_elapsed         | 3856        |
|    total_timesteps      | 4802560     |
| train/                  |             |
|    approx_kl            | 0.018709907 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.255       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.642       |
|    cost_value_loss      | 6.61e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0369      |
|    n_updates            | 4680        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0012      |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0398      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.22e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.429      |
|    reward_run           | 4.71        |
| rollout/                |             |
|    adjusted_reward      | 4.67        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 470         |
|    time_elapsed         | 3864        |
|    total_timesteps      | 4812800     |
| train/                  |             |
|    approx_kl            | 0.020074014 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.27        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.276       |
|    cost_value_loss      | 2.12e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0198      |
|    n_updates            | 4690        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00163     |
|    reward_explained_... | 0.927       |
|    reward_value_loss    | 0.05        |
|    std                  | 0.426       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.24e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.44       |
|    reward_run           | 5.36        |
| rollout/                |             |
|    adjusted_reward      | 4.09        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 471         |
|    time_elapsed         | 3872        |
|    total_timesteps      | 4823040     |
| train/                  |             |
|    approx_kl            | 0.019888414 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.253       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.144       |
|    cost_value_loss      | 5.13e-07    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0138      |
|    n_updates            | 4700        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.0016      |
|    reward_explained_... | 0.974       |
|    reward_value_loss    | 0.0157      |
|    std                  | 0.426       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.16e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.434      |
|    reward_run           | 4.92        |
| rollout/                |             |
|    adjusted_reward      | 4.34        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 472         |
|    time_elapsed         | 3879        |
|    total_timesteps      | 4833280     |
| train/                  |             |
|    approx_kl            | 0.021290684 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.148       |
|    cost_value_loss      | 4.38e-07    |
|    entropy_loss         | -3.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0132      |
|    n_updates            | 4710        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00172     |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0627      |
|    std                  | 0.425       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.452      |
|    reward_run           | 5.31        |
| rollout/                |             |
|    adjusted_reward      | 4.23        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1245        |
|    iterations           | 473         |
|    time_elapsed         | 3887        |
|    total_timesteps      | 4843520     |
| train/                  |             |
|    approx_kl            | 0.020366495 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.128       |
|    cost_value_loss      | 2.02e-06    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0301      |
|    n_updates            | 4720        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.951       |
|    reward_value_loss    | 0.0336      |
|    std                  | 0.426       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.23e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.441      |
|    reward_run           | 4.9         |
| rollout/                |             |
|    adjusted_reward      | 4.59        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.19e+03    |
| time/                   |             |
|    fps                  | 1246        |
|    iterations           | 474         |
|    time_elapsed         | 3892        |
|    total_timesteps      | 4853760     |
| train/                  |             |
|    approx_kl            | 0.022184873 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.263       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.434       |
|    cost_value_loss      | 4.27e-07    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0265      |
|    n_updates            | 4730        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00158     |
|    reward_explained_... | 0.878       |
|    reward_value_loss    | 0.0632      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.445      |
|    reward_run           | 5.49        |
| rollout/                |             |
|    adjusted_reward      | 4.44        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 475         |
|    time_elapsed         | 3900        |
|    total_timesteps      | 4864000     |
| train/                  |             |
|    approx_kl            | 0.019795517 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.591       |
|    cost_value_loss      | 1.32e-06    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 4740        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00169     |
|    reward_explained_... | 0.969       |
|    reward_value_loss    | 0.0215      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.22e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.436      |
|    reward_run           | 5.4         |
| rollout/                |             |
|    adjusted_reward      | 4.43        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.19e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 476         |
|    time_elapsed         | 3907        |
|    total_timesteps      | 4874240     |
| train/                  |             |
|    approx_kl            | 0.019871157 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.197       |
|    cost_value_loss      | 2.86e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.000748   |
|    n_updates            | 4750        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00125     |
|    reward_explained_... | 0.945       |
|    reward_value_loss    | 0.0361      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.34e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.446      |
|    reward_run           | 4.82        |
| rollout/                |             |
|    adjusted_reward      | 4.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.2e+03     |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 477         |
|    time_elapsed         | 3916        |
|    total_timesteps      | 4884480     |
| train/                  |             |
|    approx_kl            | 0.020979203 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.19        |
|    cost_value_loss      | 7.53e-07    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0119      |
|    n_updates            | 4760        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00136     |
|    reward_explained_... | 0.97        |
|    reward_value_loss    | 0.0231      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.24e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.444      |
|    reward_run           | 4.65        |
| rollout/                |             |
|    adjusted_reward      | 4.22        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.2e+03     |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 478         |
|    time_elapsed         | 3924        |
|    total_timesteps      | 4894720     |
| train/                  |             |
|    approx_kl            | 0.018767305 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.241       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.67        |
|    cost_value_loss      | 2.26e-06    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0318      |
|    n_updates            | 4770        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00132     |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0456      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.461      |
|    reward_run           | 4.99        |
| rollout/                |             |
|    adjusted_reward      | 4.37        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 479         |
|    time_elapsed         | 3932        |
|    total_timesteps      | 4904960     |
| train/                  |             |
|    approx_kl            | 0.016451374 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.539       |
|    cost_value_loss      | 1.17e-06    |
|    entropy_loss         | -3.32       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.013       |
|    n_updates            | 4780        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00164     |
|    reward_explained_... | 0.973       |
|    reward_value_loss    | 0.0272      |
|    std                  | 0.428       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.88e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.436      |
|    reward_run           | 4.18        |
| rollout/                |             |
|    adjusted_reward      | 4.38        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.18e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 480         |
|    time_elapsed         | 3939        |
|    total_timesteps      | 4915200     |
| train/                  |             |
|    approx_kl            | 0.018124785 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.25        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.416       |
|    cost_value_loss      | 4.24e-07    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0178      |
|    n_updates            | 4790        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00158     |
|    reward_explained_... | 0.956       |
|    reward_value_loss    | 0.038       |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.36e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.426      |
|    reward_run           | 5.44        |
| rollout/                |             |
|    adjusted_reward      | 4.53        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.19e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 481         |
|    time_elapsed         | 3947        |
|    total_timesteps      | 4925440     |
| train/                  |             |
|    approx_kl            | 0.020979013 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.254       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.557       |
|    cost_value_loss      | 3.12e-07    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00773     |
|    n_updates            | 4800        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.914       |
|    reward_value_loss    | 0.0594      |
|    std                  | 0.427       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 1.96e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.429      |
|    reward_run           | 5.71        |
| rollout/                |             |
|    adjusted_reward      | 4.39        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.19e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 482         |
|    time_elapsed         | 3955        |
|    total_timesteps      | 4935680     |
| train/                  |             |
|    approx_kl            | 0.023157734 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.847       |
|    cost_value_loss      | 2.64e-06    |
|    entropy_loss         | -3.31       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0149      |
|    n_updates            | 4810        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00135     |
|    reward_explained_... | 0.964       |
|    reward_value_loss    | 0.0295      |
|    std                  | 0.426       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.27e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.436      |
|    reward_run           | 5.04        |
| rollout/                |             |
|    adjusted_reward      | 4.55        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.22e+03    |
| time/                   |             |
|    fps                  | 1247        |
|    iterations           | 483         |
|    time_elapsed         | 3963        |
|    total_timesteps      | 4945920     |
| train/                  |             |
|    approx_kl            | 0.019324332 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.376       |
|    cost_value_loss      | 6.43e-07    |
|    entropy_loss         | -3.3        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0319      |
|    n_updates            | 4820        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00137     |
|    reward_explained_... | 0.959       |
|    reward_value_loss    | 0.0296      |
|    std                  | 0.426       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
---------------------------------------
| eval/                   |           |
|    best_mean_reward     | 2.39e+03  |
|    mean_ep_length       | 500       |
|    mean_reward          | 2.16e+03  |
| infos/                  |           |
|    cost                 | 0         |
|    reward_ctrl          | -0.441    |
|    reward_run           | 5.38      |
| rollout/                |           |
|    adjusted_reward      | 4.51      |
|    ep_len_mean          | 500       |
|    ep_rew_mean          | 2.24e+03  |
| time/                   |           |
|    fps                  | 1247      |
|    iterations           | 484       |
|    time_elapsed         | 3971      |
|    total_timesteps      | 4956160   |
| train/                  |           |
|    approx_kl            | 0.0231908 |
|    average_cost         | 0.0       |
|    clip_fraction        | 0.282     |
|    clip_range           | 0.2       |
|    cost_explained_va... | 0.468     |
|    cost_value_loss      | 6.42e-07  |
|    entropy_loss         | -3.3      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0331    |
|    n_updates            | 4830      |
|    nu                   | 6.29      |
|    nu_loss              | -0        |
|    policy_gradient_loss | 0.00128   |
|    reward_explained_... | 0.932     |
|    reward_value_loss    | 0.0461    |
|    std                  | 0.425     |
|    total_cost           | 0.0       |
---------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.09e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.438     |
|    reward_run           | 5.06       |
| rollout/                |            |
|    adjusted_reward      | 4.51       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.24e+03   |
| time/                   |            |
|    fps                  | 1248       |
|    iterations           | 485        |
|    time_elapsed         | 3979       |
|    total_timesteps      | 4966400    |
| train/                  |            |
|    approx_kl            | 0.01569331 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.512      |
|    cost_value_loss      | 2.23e-07   |
|    entropy_loss         | -3.28      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0132     |
|    n_updates            | 4840       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.0015     |
|    reward_explained_... | 0.96       |
|    reward_value_loss    | 0.0309     |
|    std                  | 0.424      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.421      |
|    reward_run           | 4.65        |
| rollout/                |             |
|    adjusted_reward      | 4.38        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.23e+03    |
| time/                   |             |
|    fps                  | 1248        |
|    iterations           | 486         |
|    time_elapsed         | 3987        |
|    total_timesteps      | 4976640     |
| train/                  |             |
|    approx_kl            | 0.022937318 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.277       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.398       |
|    cost_value_loss      | 2.45e-07    |
|    entropy_loss         | -3.28       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.017       |
|    n_updates            | 4850        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00159     |
|    reward_explained_... | 0.939       |
|    reward_value_loss    | 0.0406      |
|    std                  | 0.425       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.24e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.449      |
|    reward_run           | 4.87        |
| rollout/                |             |
|    adjusted_reward      | 4.53        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 1248        |
|    iterations           | 487         |
|    time_elapsed         | 3995        |
|    total_timesteps      | 4986880     |
| train/                  |             |
|    approx_kl            | 0.020001959 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.275       |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.284       |
|    cost_value_loss      | 6.47e-07    |
|    entropy_loss         | -3.28       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0349      |
|    n_updates            | 4860        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00169     |
|    reward_explained_... | 0.954       |
|    reward_value_loss    | 0.0375      |
|    std                  | 0.424       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 2.39e+03   |
|    mean_ep_length       | 500        |
|    mean_reward          | 2.16e+03   |
| infos/                  |            |
|    cost                 | 0          |
|    reward_ctrl          | -0.423     |
|    reward_run           | 3.93       |
| rollout/                |            |
|    adjusted_reward      | 4.18       |
|    ep_len_mean          | 500        |
|    ep_rew_mean          | 2.22e+03   |
| time/                   |            |
|    fps                  | 1248       |
|    iterations           | 488        |
|    time_elapsed         | 4003       |
|    total_timesteps      | 4997120    |
| train/                  |            |
|    approx_kl            | 0.01781662 |
|    average_cost         | 0.0        |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.2        |
|    cost_explained_va... | 0.542      |
|    cost_value_loss      | 9.39e-07   |
|    entropy_loss         | -3.28      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.016      |
|    n_updates            | 4870       |
|    nu                   | 6.29       |
|    nu_loss              | -0         |
|    policy_gradient_loss | 0.00141    |
|    reward_explained_... | 0.962      |
|    reward_value_loss    | 0.0257     |
|    std                  | 0.424      |
|    total_cost           | 0.0        |
----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.39e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.07e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    reward_ctrl          | -0.428      |
|    reward_run           | 5.37        |
| rollout/                |             |
|    adjusted_reward      | 4.69        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.22e+03    |
| time/                   |             |
|    fps                  | 1248        |
|    iterations           | 489         |
|    time_elapsed         | 4011        |
|    total_timesteps      | 5007360     |
| train/                  |             |
|    approx_kl            | 0.023661938 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    cost_explained_va... | 0.574       |
|    cost_value_loss      | 1.41e-06    |
|    entropy_loss         | -3.27       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0169      |
|    n_updates            | 4880        |
|    nu                   | 6.29        |
|    nu_loss              | -0          |
|    policy_gradient_loss | 0.00122     |
|    reward_explained_... | 0.938       |
|    reward_value_loss    | 0.0527      |
|    std                  | 0.424       |
|    total_cost           | 0.0         |
-----------------------------------------
Early stopping at step 0 due to reaching max kl: 0.02
Saving video to  /home/linuxubuntu/Desktop/shehryar-usman/rl_codebase/icrl/wandb/run-20201217_092415-173sfsh3/files/final_policy-step-0-to-step-1500.mp4
Mean reward: 2221.492043 +/- 296.961958.
/home/linuxubuntu/Desktop/shehryar-usman/rl_codebase/.rl/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 67.36 minutes[0m
