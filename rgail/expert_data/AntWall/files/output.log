[32;1mConfigured folder ./icrl/wandb/run-20210108_164545-alci5o6e/files for saving[0m
[32;1mName: AntWall-v0_AntWallTest-v0_bs_128_cr_0.4_cgl_0.9_lr_3e-05_ne_20_rgl_0.9_s_37_sid_5[0m
Wrapping eval env in a VecNormalize.
Using cpu device
/home/vm5/rl_codebase/.env/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead
  warnings.warn("pickle support for Storage will be removed in 1.5. Use `torch.save` instead", FutureWarning)
(8,)
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | -3.1       |
|    mean_ep_length       | 59         |
|    mean_reward          | -3.1       |
| infos/                  |            |
|    cost                 | 0          |
|    distance_from_origin | 1.25       |
|    forward_reward       | 0.635      |
|    reward_contact       | 0          |
|    reward_ctrl          | -2.02      |
|    reward_forward       | 0.635      |
|    reward_survive       | 1          |
|    x_position           | -0.586     |
|    x_velocity           | 0.635      |
|    y_position           | -0.405     |
|    y_velocity           | 0.627      |
| rollout/                |            |
|    adjusted_reward      | 0.279      |
|    ep_len_mean          | 87.4       |
|    ep_rew_mean          | 58.5       |
| time/                   |            |
|    fps                  | 1967       |
|    iterations           | 1          |
|    time_elapsed         | 5          |
|    total_timesteps      | 10240      |
| torque/                 |            |
|    greater_than_0.25    | 10240      |
|    greater_than_0.3     | 10240      |
|    greater_than_0.5     | 10237      |
|    mean_motor0          | 0.8028609  |
|    mean_motor1          | 0.7996364  |
|    mean_motor2          | 0.7924708  |
|    mean_motor3          | 0.7967806  |
|    mean_motor4          | 0.7950528  |
|    mean_motor5          | 0.7942923  |
|    mean_motor6          | 0.7918023  |
|    mean_motor7          | 0.79938424 |
----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 90.3         |
|    mean_ep_length       | 140          |
|    mean_reward          | 90.3         |
| infos/                  |              |
|    cost                 | 0.0324       |
|    distance_from_origin | 1.85         |
|    forward_reward       | 0.509        |
|    reward_contact       | 0            |
|    reward_ctrl          | -2.03        |
|    reward_forward       | 0.509        |
|    reward_survive       | 1            |
|    x_position           | -1.69        |
|    x_velocity           | 0.509        |
|    y_position           | 0.59         |
|    y_velocity           | 0.429        |
| rollout/                |              |
|    adjusted_reward      | 0.582        |
|    ep_len_mean          | 91.5         |
|    ep_rew_mean          | 50.2         |
| time/                   |              |
|    fps                  | 1233         |
|    iterations           | 2            |
|    time_elapsed         | 16           |
|    total_timesteps      | 20480        |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10237        |
|    mean_motor0          | 0.79533297   |
|    mean_motor1          | 0.80275124   |
|    mean_motor2          | 0.7906486    |
|    mean_motor3          | 0.800813     |
|    mean_motor4          | 0.80285436   |
|    mean_motor5          | 0.80756265   |
|    mean_motor6          | 0.79170614   |
|    mean_motor7          | 0.7972608    |
| train/                  |              |
|    approx_kl            | 0.0075873635 |
|    average_cost         | 0.04121094   |
|    clip_fraction        | 0.00158      |
|    clip_range           | 0.4          |
|    cost_explained_va... | -9.7         |
|    cost_value_loss      | 3.28         |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -11.4        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.144        |
|    mean_cost_advantages | 0.21025701   |
|    mean_reward_advan... | 0.06957479   |
|    n_updates            | 20           |
|    nu                   | 1.06         |
|    nu_loss              | -0.0412      |
|    policy_gradient_loss | -0.0083      |
|    reward_explained_... | -0.264       |
|    reward_value_loss    | 0.18         |
|    std                  | 1            |
|    total_cost           | 422.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 90.3          |
|    mean_ep_length       | 83            |
|    mean_reward          | 37.8          |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 2.01          |
|    forward_reward       | 0.751         |
|    reward_contact       | 0             |
|    reward_ctrl          | -2.09         |
|    reward_forward       | 0.751         |
|    reward_survive       | 1             |
|    x_position           | 0.503         |
|    x_velocity           | 0.751         |
|    y_position           | -0.41         |
|    y_velocity           | 1.06          |
| rollout/                |               |
|    adjusted_reward      | 0.411         |
|    ep_len_mean          | 75.7          |
|    ep_rew_mean          | 34.3          |
| time/                   |               |
|    fps                  | 1110          |
|    iterations           | 3             |
|    time_elapsed         | 27            |
|    total_timesteps      | 30720         |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10238         |
|    mean_motor0          | 0.7926351     |
|    mean_motor1          | 0.7915021     |
|    mean_motor2          | 0.79254496    |
|    mean_motor3          | 0.7962879     |
|    mean_motor4          | 0.80258113    |
|    mean_motor5          | 0.8174299     |
|    mean_motor6          | 0.7881834     |
|    mean_motor7          | 0.79216874    |
| train/                  |               |
|    approx_kl            | 0.007995803   |
|    average_cost         | 0.020019531   |
|    clip_fraction        | 0.000996      |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.741         |
|    cost_value_loss      | 0.0384        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -11.3         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0198        |
|    mean_cost_advantages | -8.407943e-05 |
|    mean_reward_advan... | 0.056275617   |
|    n_updates            | 40            |
|    nu                   | 1.13          |
|    nu_loss              | -0.0213       |
|    policy_gradient_loss | -0.00402      |
|    reward_explained_... | 0.185         |
|    reward_value_loss    | 0.0254        |
|    std                  | 0.996         |
|    total_cost           | 205.0         |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 90.3        |
|    mean_ep_length       | 27.8        |
|    mean_reward          | -19.7       |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.29        |
|    forward_reward       | 0.891       |
|    reward_contact       | 0           |
|    reward_ctrl          | -2.05       |
|    reward_forward       | 0.891       |
|    reward_survive       | 1           |
|    x_position           | -0.362      |
|    x_velocity           | 0.891       |
|    y_position           | 0.214       |
|    y_velocity           | 0.693       |
| rollout/                |             |
|    adjusted_reward      | 0.291       |
|    ep_len_mean          | 79.9        |
|    ep_rew_mean          | 26.7        |
| time/                   |             |
|    fps                  | 974         |
|    iterations           | 4           |
|    time_elapsed         | 42          |
|    total_timesteps      | 40960       |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10232       |
|    mean_motor0          | 0.78856254  |
|    mean_motor1          | 0.7846636   |
|    mean_motor2          | 0.78164834  |
|    mean_motor3          | 0.78600293  |
|    mean_motor4          | 0.7895042   |
|    mean_motor5          | 0.79659593  |
|    mean_motor6          | 0.78942996  |
|    mean_motor7          | 0.7983786   |
| train/                  |             |
|    approx_kl            | 0.010725259 |
|    average_cost         | 0.019628907 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.76        |
|    cost_value_loss      | 0.0268      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -11.3       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.000455    |
|    mean_cost_advantages | 0.005329182 |
|    mean_reward_advan... | 0.038667448 |
|    n_updates            | 60          |
|    nu                   | 1.19        |
|    nu_loss              | -0.0221     |
|    policy_gradient_loss | -0.00507    |
|    reward_explained_... | 0.219       |
|    reward_value_loss    | 0.0175      |
|    std                  | 0.989       |
|    total_cost           | 201.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 90.3         |
|    mean_ep_length       | 51.2         |
|    mean_reward          | -4.17        |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 0.796        |
|    forward_reward       | 0.505        |
|    reward_contact       | 0            |
|    reward_ctrl          | -2.06        |
|    reward_forward       | 0.505        |
|    reward_survive       | 1            |
|    x_position           | -0.0636      |
|    x_velocity           | 0.505        |
|    y_position           | 0.0607       |
|    y_velocity           | 0.715        |
| rollout/                |              |
|    adjusted_reward      | 0.102        |
|    ep_len_mean          | 82.1         |
|    ep_rew_mean          | 13.1         |
| time/                   |              |
|    fps                  | 968          |
|    iterations           | 5            |
|    time_elapsed         | 52           |
|    total_timesteps      | 51200        |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10233        |
|    mean_motor0          | 0.7836639    |
|    mean_motor1          | 0.79007226   |
|    mean_motor2          | 0.7865798    |
|    mean_motor3          | 0.7879354    |
|    mean_motor4          | 0.793221     |
|    mean_motor5          | 0.80089986   |
|    mean_motor6          | 0.7851492    |
|    mean_motor7          | 0.7950205    |
| train/                  |              |
|    approx_kl            | 0.0102531845 |
|    average_cost         | 0.033203125  |
|    clip_fraction        | 0.003        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.583        |
|    cost_value_loss      | 0.0254       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -11.2        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0114       |
|    mean_cost_advantages | 0.021281192  |
|    mean_reward_advan... | 0.027120914  |
|    n_updates            | 80           |
|    nu                   | 1.26         |
|    nu_loss              | -0.0395      |
|    policy_gradient_loss | -0.0052      |
|    reward_explained_... | 0.346        |
|    reward_value_loss    | 0.0143       |
|    std                  | 0.98         |
|    total_cost           | 340.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 90.3          |
|    mean_ep_length       | 38.8          |
|    mean_reward          | -17.5         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.33          |
|    forward_reward       | 0.389         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.95         |
|    reward_forward       | 0.389         |
|    reward_survive       | 1             |
|    x_position           | 0.356         |
|    x_velocity           | 0.389         |
|    y_position           | 0.171         |
|    y_velocity           | 0.34          |
| rollout/                |               |
|    adjusted_reward      | 0.268         |
|    ep_len_mean          | 97.6          |
|    ep_rew_mean          | 27.4          |
| time/                   |               |
|    fps                  | 964           |
|    iterations           | 6             |
|    time_elapsed         | 63            |
|    total_timesteps      | 61440         |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10233         |
|    mean_motor0          | 0.7788982     |
|    mean_motor1          | 0.76531535    |
|    mean_motor2          | 0.77162343    |
|    mean_motor3          | 0.781023      |
|    mean_motor4          | 0.79444945    |
|    mean_motor5          | 0.79333955    |
|    mean_motor6          | 0.779887      |
|    mean_motor7          | 0.7693218     |
| train/                  |               |
|    approx_kl            | 0.011380514   |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.00193       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.85          |
|    cost_value_loss      | 0.0102        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -11.2         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00675       |
|    mean_cost_advantages | -0.008393659  |
|    mean_reward_advan... | 0.0046782037  |
|    n_updates            | 100           |
|    nu                   | 1.32          |
|    nu_loss              | -0.000859     |
|    policy_gradient_loss | -0.00502      |
|    reward_explained_... | 0.489         |
|    reward_value_loss    | 0.0118        |
|    std                  | 0.972         |
|    total_cost           | 7.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 90.3          |
|    mean_ep_length       | 48.6          |
|    mean_reward          | -7.01         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 2.5           |
|    forward_reward       | 0.492         |
|    reward_contact       | 0             |
|    reward_ctrl          | -2.03         |
|    reward_forward       | 0.492         |
|    reward_survive       | 1             |
|    x_position           | 0.919         |
|    x_velocity           | 0.492         |
|    y_position           | 0.055         |
|    y_velocity           | 0.568         |
| rollout/                |               |
|    adjusted_reward      | 0.359         |
|    ep_len_mean          | 84.1          |
|    ep_rew_mean          | 19.4          |
| time/                   |               |
|    fps                  | 961           |
|    iterations           | 7             |
|    time_elapsed         | 74            |
|    total_timesteps      | 71680         |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10229         |
|    mean_motor0          | 0.77622473    |
|    mean_motor1          | 0.7618048     |
|    mean_motor2          | 0.7669452     |
|    mean_motor3          | 0.7739073     |
|    mean_motor4          | 0.7932654     |
|    mean_motor5          | 0.8011589     |
|    mean_motor6          | 0.766063      |
|    mean_motor7          | 0.756154      |
| train/                  |               |
|    approx_kl            | 0.009792726   |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0.0026        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.839         |
|    cost_value_loss      | 0.00682       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -11.1         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.000243     |
|    mean_cost_advantages | -0.0153281065 |
|    mean_reward_advan... | 0.023934083   |
|    n_updates            | 120           |
|    nu                   | 1.37          |
|    nu_loss              | -0.00154      |
|    policy_gradient_loss | -0.00511      |
|    reward_explained_... | 0.443         |
|    reward_value_loss    | 0.0111        |
|    std                  | 0.962         |
|    total_cost           | 12.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 90.3         |
|    mean_ep_length       | 68.2         |
|    mean_reward          | 18.2         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.27         |
|    forward_reward       | 0.343        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.93        |
|    reward_forward       | 0.343        |
|    reward_survive       | 1            |
|    x_position           | -0.162       |
|    x_velocity           | 0.343        |
|    y_position           | 0.085        |
|    y_velocity           | 0.34         |
| rollout/                |              |
|    adjusted_reward      | 0.256        |
|    ep_len_mean          | 79.2         |
|    ep_rew_mean          | 10.4         |
| time/                   |              |
|    fps                  | 958          |
|    iterations           | 8            |
|    time_elapsed         | 85           |
|    total_timesteps      | 81920        |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10235        |
|    mean_motor0          | 0.76386803   |
|    mean_motor1          | 0.76524687   |
|    mean_motor2          | 0.77321416   |
|    mean_motor3          | 0.7636773    |
|    mean_motor4          | 0.78230697   |
|    mean_motor5          | 0.7807065    |
|    mean_motor6          | 0.7616653    |
|    mean_motor7          | 0.7544714    |
| train/                  |              |
|    approx_kl            | 0.010843182  |
|    average_cost         | 0.0016601563 |
|    clip_fraction        | 0.00153      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.805        |
|    cost_value_loss      | 0.00495      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -11          |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00231     |
|    mean_cost_advantages | -0.005870022 |
|    mean_reward_advan... | 0.03815737   |
|    n_updates            | 140          |
|    nu                   | 1.42         |
|    nu_loss              | -0.00227     |
|    policy_gradient_loss | -0.00434     |
|    reward_explained_... | 0.436        |
|    reward_value_loss    | 0.0121       |
|    std                  | 0.952        |
|    total_cost           | 17.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 90.3         |
|    mean_ep_length       | 88           |
|    mean_reward          | 33.2         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.69         |
|    forward_reward       | 0.774        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.95        |
|    reward_forward       | 0.774        |
|    reward_survive       | 1            |
|    x_position           | -0.203       |
|    x_velocity           | 0.774        |
|    y_position           | -0.78        |
|    y_velocity           | 0.655        |
| rollout/                |              |
|    adjusted_reward      | 0.358        |
|    ep_len_mean          | 90.5         |
|    ep_rew_mean          | 30.7         |
| time/                   |              |
|    fps                  | 954          |
|    iterations           | 9            |
|    time_elapsed         | 96           |
|    total_timesteps      | 92160        |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10236        |
|    mean_motor0          | 0.76139975   |
|    mean_motor1          | 0.745258     |
|    mean_motor2          | 0.7645892    |
|    mean_motor3          | 0.76081115   |
|    mean_motor4          | 0.76774806   |
|    mean_motor5          | 0.7821724    |
|    mean_motor6          | 0.761461     |
|    mean_motor7          | 0.7597492    |
| train/                  |              |
|    approx_kl            | 0.0121600395 |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.00308      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.872        |
|    cost_value_loss      | 0.00305      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -10.9        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00407      |
|    mean_cost_advantages | -0.011768972 |
|    mean_reward_advan... | 0.025425646  |
|    n_updates            | 160          |
|    nu                   | 1.46         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00469     |
|    reward_explained_... | 0.409        |
|    reward_value_loss    | 0.0136       |
|    std                  | 0.945        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 90.3         |
|    mean_ep_length       | 28.4         |
|    mean_reward          | -12          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.25         |
|    forward_reward       | 0.642        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.95        |
|    reward_forward       | 0.642        |
|    reward_survive       | 1            |
|    x_position           | 0.265        |
|    x_velocity           | 0.642        |
|    y_position           | 0.585        |
|    y_velocity           | 0.488        |
| rollout/                |              |
|    adjusted_reward      | 0.282        |
|    ep_len_mean          | 98           |
|    ep_rew_mean          | 28.8         |
| time/                   |              |
|    fps                  | 954          |
|    iterations           | 10           |
|    time_elapsed         | 107          |
|    total_timesteps      | 102400       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10228        |
|    mean_motor0          | 0.74745786   |
|    mean_motor1          | 0.7521502    |
|    mean_motor2          | 0.7387975    |
|    mean_motor3          | 0.7473032    |
|    mean_motor4          | 0.76336277   |
|    mean_motor5          | 0.7753412    |
|    mean_motor6          | 0.76298213   |
|    mean_motor7          | 0.7352117    |
| train/                  |              |
|    approx_kl            | 0.013718406  |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.00463      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.818        |
|    cost_value_loss      | 0.00285      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -10.9        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00526     |
|    mean_cost_advantages | -0.007975709 |
|    mean_reward_advan... | 0.035263468  |
|    n_updates            | 180          |
|    nu                   | 1.5          |
|    nu_loss              | -0.00199     |
|    policy_gradient_loss | -0.00541     |
|    reward_explained_... | 0.6          |
|    reward_value_loss    | 0.012        |
|    std                  | 0.934        |
|    total_cost           | 14.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 124           |
|    mean_ep_length       | 131           |
|    mean_reward          | 124           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.19          |
|    forward_reward       | 0.543         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.94         |
|    reward_forward       | 0.543         |
|    reward_survive       | 1             |
|    x_position           | 0.206         |
|    x_velocity           | 0.543         |
|    y_position           | -0.0265       |
|    y_velocity           | 0.536         |
| rollout/                |               |
|    adjusted_reward      | 0.28          |
|    ep_len_mean          | 82.3          |
|    ep_rew_mean          | 13.6          |
| time/                   |               |
|    fps                  | 950           |
|    iterations           | 11            |
|    time_elapsed         | 118           |
|    total_timesteps      | 112640        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10235         |
|    mean_motor0          | 0.74262327    |
|    mean_motor1          | 0.74786013    |
|    mean_motor2          | 0.74643654    |
|    mean_motor3          | 0.7370466     |
|    mean_motor4          | 0.75947225    |
|    mean_motor5          | 0.75171095    |
|    mean_motor6          | 0.74311125    |
|    mean_motor7          | 0.7325623     |
| train/                  |               |
|    approx_kl            | 0.008163953   |
|    average_cost         | 0.0069335937  |
|    clip_fraction        | 0.00249       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.299         |
|    cost_value_loss      | 0.00768       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -10.8         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00753       |
|    mean_cost_advantages | 2.1692365e-06 |
|    mean_reward_advan... | 0.022948328   |
|    n_updates            | 200           |
|    nu                   | 1.54          |
|    nu_loss              | -0.0104       |
|    policy_gradient_loss | -0.00432      |
|    reward_explained_... | 0.676         |
|    reward_value_loss    | 0.0119        |
|    std                  | 0.925         |
|    total_cost           | 71.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 124           |
|    mean_ep_length       | 30.4          |
|    mean_reward          | -13.7         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.17          |
|    forward_reward       | 0.424         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.95         |
|    reward_forward       | 0.424         |
|    reward_survive       | 1             |
|    x_position           | 0.402         |
|    x_velocity           | 0.424         |
|    y_position           | 0.536         |
|    y_velocity           | 0.675         |
| rollout/                |               |
|    adjusted_reward      | 0.747         |
|    ep_len_mean          | 91.2          |
|    ep_rew_mean          | 74.9          |
| time/                   |               |
|    fps                  | 950           |
|    iterations           | 12            |
|    time_elapsed         | 129           |
|    total_timesteps      | 122880        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10235         |
|    mean_motor0          | 0.73810303    |
|    mean_motor1          | 0.74255306    |
|    mean_motor2          | 0.74071175    |
|    mean_motor3          | 0.7325585     |
|    mean_motor4          | 0.7486621     |
|    mean_motor5          | 0.760505      |
|    mean_motor6          | 0.7319032     |
|    mean_motor7          | 0.73268324    |
| train/                  |               |
|    approx_kl            | 0.0127726225  |
|    average_cost         | 0.0049804687  |
|    clip_fraction        | 0.00241       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.408         |
|    cost_value_loss      | 0.00576       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -10.7         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00489      |
|    mean_cost_advantages | -0.0024788892 |
|    mean_reward_advan... | 0.026196772   |
|    n_updates            | 220           |
|    nu                   | 1.58          |
|    nu_loss              | -0.00766      |
|    policy_gradient_loss | -0.00485      |
|    reward_explained_... | 0.706         |
|    reward_value_loss    | 0.0115        |
|    std                  | 0.916         |
|    total_cost           | 51.0          |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 124         |
|    mean_ep_length       | 37.8        |
|    mean_reward          | -12.1       |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.24        |
|    forward_reward       | 0.422       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.9        |
|    reward_forward       | 0.422       |
|    reward_survive       | 1           |
|    x_position           | 0.111       |
|    x_velocity           | 0.422       |
|    y_position           | -0.643      |
|    y_velocity           | 0.52        |
| rollout/                |             |
|    adjusted_reward      | 0.339       |
|    ep_len_mean          | 86.2        |
|    ep_rew_mean          | 33.5        |
| time/                   |             |
|    fps                  | 951         |
|    iterations           | 13          |
|    time_elapsed         | 139         |
|    total_timesteps      | 133120      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10236       |
|    mean_motor0          | 0.7180135   |
|    mean_motor1          | 0.72858536  |
|    mean_motor2          | 0.7306808   |
|    mean_motor3          | 0.71777815  |
|    mean_motor4          | 0.7507857   |
|    mean_motor5          | 0.7608127   |
|    mean_motor6          | 0.73578894  |
|    mean_motor7          | 0.7309328   |
| train/                  |             |
|    approx_kl            | 0.011718194 |
|    average_cost         | 0.062597655 |
|    clip_fraction        | 0.00585     |
|    clip_range           | 0.4         |
|    cost_explained_va... | -4.36       |
|    cost_value_loss      | 0.0476      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -10.6       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0176      |
|    mean_cost_advantages | 0.07234272  |
|    mean_reward_advan... | 0.07930181  |
|    n_updates            | 240         |
|    nu                   | 1.63        |
|    nu_loss              | -0.0988     |
|    policy_gradient_loss | -0.00425    |
|    reward_explained_... | 0.57        |
|    reward_value_loss    | 0.0168      |
|    std                  | 0.908       |
|    total_cost           | 641.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 124          |
|    mean_ep_length       | 26           |
|    mean_reward          | -14.5        |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.91         |
|    forward_reward       | 0.624        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.86        |
|    reward_forward       | 0.624        |
|    reward_survive       | 1            |
|    x_position           | 1.8          |
|    x_velocity           | 0.624        |
|    y_position           | -1.33        |
|    y_velocity           | 0.554        |
| rollout/                |              |
|    adjusted_reward      | 0.548        |
|    ep_len_mean          | 105          |
|    ep_rew_mean          | 51.2         |
| time/                   |              |
|    fps                  | 951          |
|    iterations           | 14           |
|    time_elapsed         | 150          |
|    total_timesteps      | 143360       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10230        |
|    mean_motor0          | 0.70905316   |
|    mean_motor1          | 0.7303072    |
|    mean_motor2          | 0.73565257   |
|    mean_motor3          | 0.7142131    |
|    mean_motor4          | 0.74319035   |
|    mean_motor5          | 0.76884836   |
|    mean_motor6          | 0.7209931    |
|    mean_motor7          | 0.7163681    |
| train/                  |              |
|    approx_kl            | 0.01633559   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.00447      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.869        |
|    cost_value_loss      | 0.00401      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -10.6        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00769     |
|    mean_cost_advantages | -0.011482758 |
|    mean_reward_advan... | 0.023914587  |
|    n_updates            | 260          |
|    nu                   | 1.68         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00466     |
|    reward_explained_... | 0.801        |
|    reward_value_loss    | 0.0119       |
|    std                  | 0.904        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 124         |
|    mean_ep_length       | 43.8        |
|    mean_reward          | 1.34        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.3         |
|    forward_reward       | 0.598       |
|    reward_contact       | 0           |
|    reward_ctrl          | -2          |
|    reward_forward       | 0.598       |
|    reward_survive       | 1           |
|    x_position           | 0.884       |
|    x_velocity           | 0.598       |
|    y_position           | -0.0594     |
|    y_velocity           | 0.783       |
| rollout/                |             |
|    adjusted_reward      | 0.802       |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 82.5        |
| time/                   |             |
|    fps                  | 951         |
|    iterations           | 15          |
|    time_elapsed         | 161         |
|    total_timesteps      | 153600      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10234       |
|    mean_motor0          | 0.71045697  |
|    mean_motor1          | 0.72679496  |
|    mean_motor2          | 0.71417373  |
|    mean_motor3          | 0.70661724  |
|    mean_motor4          | 0.73938626  |
|    mean_motor5          | 0.7619776   |
|    mean_motor6          | 0.7222893   |
|    mean_motor7          | 0.71303326  |
| train/                  |             |
|    approx_kl            | 0.015026689 |
|    average_cost         | 0.08408203  |
|    clip_fraction        | 0.0035      |
|    clip_range           | 0.4         |
|    cost_explained_va... | -0.225      |
|    cost_value_loss      | 0.0349      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -10.5       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0109      |
|    mean_cost_advantages | 0.06922997  |
|    mean_reward_advan... | 0.051040582 |
|    n_updates            | 280         |
|    nu                   | 1.74        |
|    nu_loss              | -0.141      |
|    policy_gradient_loss | -0.00394    |
|    reward_explained_... | 0.597       |
|    reward_value_loss    | 0.0127      |
|    std                  | 0.894       |
|    total_cost           | 861.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 124           |
|    mean_ep_length       | 148           |
|    mean_reward          | 47.5          |
| infos/                  |               |
|    cost                 | 0.0197        |
|    distance_from_origin | 1.37          |
|    forward_reward       | 0.416         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.88         |
|    reward_forward       | 0.416         |
|    reward_survive       | 1             |
|    x_position           | -0.772        |
|    x_velocity           | 0.416         |
|    y_position           | 0.235         |
|    y_velocity           | 0.436         |
| rollout/                |               |
|    adjusted_reward      | 1.02          |
|    ep_len_mean          | 124           |
|    ep_rew_mean          | 111           |
| time/                   |               |
|    fps                  | 948           |
|    iterations           | 16            |
|    time_elapsed         | 172           |
|    total_timesteps      | 163840        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10223         |
|    mean_motor0          | 0.70567954    |
|    mean_motor1          | 0.71396816    |
|    mean_motor2          | 0.710394      |
|    mean_motor3          | 0.70767355    |
|    mean_motor4          | 0.74243134    |
|    mean_motor5          | 0.7431763     |
|    mean_motor6          | 0.7196099     |
|    mean_motor7          | 0.71073925    |
| train/                  |               |
|    approx_kl            | 0.010177941   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.00438       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.905         |
|    cost_value_loss      | 0.00679       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -10.4         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00253       |
|    mean_cost_advantages | -0.0037271208 |
|    mean_reward_advan... | 0.061648123   |
|    n_updates            | 300           |
|    nu                   | 1.79          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00423      |
|    reward_explained_... | 0.811         |
|    reward_value_loss    | 0.0172        |
|    std                  | 0.888         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 209         |
|    mean_ep_length       | 317         |
|    mean_reward          | 209         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.07        |
|    forward_reward       | 0.612       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.88       |
|    reward_forward       | 0.612       |
|    reward_survive       | 1           |
|    x_position           | 0.461       |
|    x_velocity           | 0.612       |
|    y_position           | 0.26        |
|    y_velocity           | 0.404       |
| rollout/                |             |
|    adjusted_reward      | 0.508       |
|    ep_len_mean          | 104         |
|    ep_rew_mean          | 55          |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 17          |
|    time_elapsed         | 184         |
|    total_timesteps      | 174080      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10223       |
|    mean_motor0          | 0.70000416  |
|    mean_motor1          | 0.69898015  |
|    mean_motor2          | 0.71740764  |
|    mean_motor3          | 0.70624363  |
|    mean_motor4          | 0.72873634  |
|    mean_motor5          | 0.7453125   |
|    mean_motor6          | 0.7304641   |
|    mean_motor7          | 0.7021161   |
| train/                  |             |
|    approx_kl            | 0.014772396 |
|    average_cost         | 0.07138672  |
|    clip_fraction        | 0.00653     |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.691       |
|    cost_value_loss      | 0.0395      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -10.4       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00322     |
|    mean_cost_advantages | 0.04962296  |
|    mean_reward_advan... | 0.07929863  |
|    n_updates            | 320         |
|    nu                   | 1.86        |
|    nu_loss              | -0.128      |
|    policy_gradient_loss | -0.00464    |
|    reward_explained_... | 0.699       |
|    reward_value_loss    | 0.0157      |
|    std                  | 0.88        |
|    total_cost           | 731.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 209         |
|    mean_ep_length       | 67.4        |
|    mean_reward          | 1.8         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.3         |
|    forward_reward       | 0.448       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.82       |
|    reward_forward       | 0.448       |
|    reward_survive       | 1           |
|    x_position           | 0.86        |
|    x_velocity           | 0.448       |
|    y_position           | -0.236      |
|    y_velocity           | 0.555       |
| rollout/                |             |
|    adjusted_reward      | 0.73        |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 73.1        |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 18          |
|    time_elapsed         | 195         |
|    total_timesteps      | 184320      |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10230       |
|    mean_motor0          | 0.69543475  |
|    mean_motor1          | 0.6970883   |
|    mean_motor2          | 0.7014021   |
|    mean_motor3          | 0.6948334   |
|    mean_motor4          | 0.7217849   |
|    mean_motor5          | 0.7351065   |
|    mean_motor6          | 0.70537823  |
|    mean_motor7          | 0.6902735   |
| train/                  |             |
|    approx_kl            | 0.018104225 |
|    average_cost         | 0.031542968 |
|    clip_fraction        | 0.00802     |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.815       |
|    cost_value_loss      | 0.0228      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -10.3       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00426     |
|    mean_cost_advantages | 0.011215039 |
|    mean_reward_advan... | 0.027312558 |
|    n_updates            | 340         |
|    nu                   | 1.92        |
|    nu_loss              | -0.0586     |
|    policy_gradient_loss | -0.00514    |
|    reward_explained_... | 0.794       |
|    reward_value_loss    | 0.0144      |
|    std                  | 0.87        |
|    total_cost           | 323.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 209         |
|    mean_ep_length       | 83          |
|    mean_reward          | 6.79        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.23        |
|    forward_reward       | 0.462       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.79       |
|    reward_forward       | 0.462       |
|    reward_survive       | 1           |
|    x_position           | -0.7        |
|    x_velocity           | 0.462       |
|    y_position           | -0.0791     |
|    y_velocity           | 0.351       |
| rollout/                |             |
|    adjusted_reward      | 0.478       |
|    ep_len_mean          | 101         |
|    ep_rew_mean          | 58.3        |
| time/                   |             |
|    fps                  | 941         |
|    iterations           | 19          |
|    time_elapsed         | 206         |
|    total_timesteps      | 194560      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10226       |
|    mean_motor0          | 0.68945664  |
|    mean_motor1          | 0.7059774   |
|    mean_motor2          | 0.6961869   |
|    mean_motor3          | 0.6996542   |
|    mean_motor4          | 0.71271104  |
|    mean_motor5          | 0.7266751   |
|    mean_motor6          | 0.6986076   |
|    mean_motor7          | 0.6962372   |
| train/                  |             |
|    approx_kl            | 0.013332242 |
|    average_cost         | 0.07841797  |
|    clip_fraction        | 0.00429     |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.718       |
|    cost_value_loss      | 0.0266      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -10.2       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0158      |
|    mean_cost_advantages | 0.03856715  |
|    mean_reward_advan... | 0.050969563 |
|    n_updates            | 360         |
|    nu                   | 2           |
|    nu_loss              | -0.151      |
|    policy_gradient_loss | -0.00382    |
|    reward_explained_... | 0.692       |
|    reward_value_loss    | 0.0157      |
|    std                  | 0.864       |
|    total_cost           | 803.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 209           |
|    mean_ep_length       | 55.4          |
|    mean_reward          | 6.59          |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.25          |
|    forward_reward       | 0.536         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.94         |
|    reward_forward       | 0.536         |
|    reward_survive       | 1             |
|    x_position           | -0.139        |
|    x_velocity           | 0.536         |
|    y_position           | -0.36         |
|    y_velocity           | 0.404         |
| rollout/                |               |
|    adjusted_reward      | 0.516         |
|    ep_len_mean          | 111           |
|    ep_rew_mean          | 50.8          |
| time/                   |               |
|    fps                  | 941           |
|    iterations           | 20            |
|    time_elapsed         | 217           |
|    total_timesteps      | 204800        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10232         |
|    mean_motor0          | 0.6924851     |
|    mean_motor1          | 0.6995276     |
|    mean_motor2          | 0.66699207    |
|    mean_motor3          | 0.69789875    |
|    mean_motor4          | 0.7120317     |
|    mean_motor5          | 0.7267455     |
|    mean_motor6          | 0.6944383     |
|    mean_motor7          | 0.69043285    |
| train/                  |               |
|    approx_kl            | 0.01645739    |
|    average_cost         | 0.021484375   |
|    clip_fraction        | 0.00775       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.881         |
|    cost_value_loss      | 0.0174        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -10.1         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00247      |
|    mean_cost_advantages | -0.0039982377 |
|    mean_reward_advan... | 0.018385297   |
|    n_updates            | 380           |
|    nu                   | 2.07          |
|    nu_loss              | -0.0429       |
|    policy_gradient_loss | -0.0047       |
|    reward_explained_... | 0.84          |
|    reward_value_loss    | 0.0143        |
|    std                  | 0.854         |
|    total_cost           | 220.0         |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 209          |
|    mean_ep_length       | 225          |
|    mean_reward          | 11.9         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2            |
|    forward_reward       | 0.724        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.84        |
|    reward_forward       | 0.724        |
|    reward_survive       | 1            |
|    x_position           | 0.427        |
|    x_velocity           | 0.724        |
|    y_position           | 0.231        |
|    y_velocity           | 0.468        |
| rollout/                |              |
|    adjusted_reward      | 0.558        |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 67.2         |
| time/                   |              |
|    fps                  | 937          |
|    iterations           | 21           |
|    time_elapsed         | 229          |
|    total_timesteps      | 215040       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10232        |
|    mean_motor0          | 0.7044386    |
|    mean_motor1          | 0.6919981    |
|    mean_motor2          | 0.6850735    |
|    mean_motor3          | 0.70763606   |
|    mean_motor4          | 0.7138956    |
|    mean_motor5          | 0.7163722    |
|    mean_motor6          | 0.6961324    |
|    mean_motor7          | 0.67758906   |
| train/                  |              |
|    approx_kl            | 0.014790429  |
|    average_cost         | 0.0033203126 |
|    clip_fraction        | 0.00765      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.892        |
|    cost_value_loss      | 0.00994      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -10.1        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.000499     |
|    mean_cost_advantages | -0.015548436 |
|    mean_reward_advan... | 0.027676841  |
|    n_updates            | 400          |
|    nu                   | 2.14         |
|    nu_loss              | -0.00688     |
|    policy_gradient_loss | -0.00498     |
|    reward_explained_... | 0.727        |
|    reward_value_loss    | 0.018        |
|    std                  | 0.847        |
|    total_cost           | 34.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 209           |
|    mean_ep_length       | 223           |
|    mean_reward          | 157           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 2.43          |
|    forward_reward       | 0.332         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.82         |
|    reward_forward       | 0.332         |
|    reward_survive       | 1             |
|    x_position           | 0.0643        |
|    x_velocity           | 0.332         |
|    y_position           | -0.969        |
|    y_velocity           | 0.322         |
| rollout/                |               |
|    adjusted_reward      | 0.6           |
|    ep_len_mean          | 122           |
|    ep_rew_mean          | 72.1          |
| time/                   |               |
|    fps                  | 934           |
|    iterations           | 22            |
|    time_elapsed         | 241           |
|    total_timesteps      | 225280        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10221         |
|    mean_motor0          | 0.7000581     |
|    mean_motor1          | 0.68313664    |
|    mean_motor2          | 0.6943438     |
|    mean_motor3          | 0.7056227     |
|    mean_motor4          | 0.7059877     |
|    mean_motor5          | 0.71115315    |
|    mean_motor6          | 0.6848836     |
|    mean_motor7          | 0.6780143     |
| train/                  |               |
|    approx_kl            | 0.014329824   |
|    average_cost         | 0.0146484375  |
|    clip_fraction        | 0.00973       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.895         |
|    cost_value_loss      | 0.0113        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -10           |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00544       |
|    mean_cost_advantages | -3.052745e-05 |
|    mean_reward_advan... | 0.02790073    |
|    n_updates            | 420           |
|    nu                   | 2.21          |
|    nu_loss              | -0.0314       |
|    policy_gradient_loss | -0.00465      |
|    reward_explained_... | 0.835         |
|    reward_value_loss    | 0.0147        |
|    std                  | 0.842         |
|    total_cost           | 150.0         |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 269         |
|    mean_ep_length       | 224         |
|    mean_reward          | 269         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.03        |
|    forward_reward       | 0.739       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.74       |
|    reward_forward       | 0.739       |
|    reward_survive       | 1           |
|    x_position           | 0.453       |
|    x_velocity           | 0.739       |
|    y_position           | -0.407      |
|    y_velocity           | 0.558       |
| rollout/                |             |
|    adjusted_reward      | 0.486       |
|    ep_len_mean          | 151         |
|    ep_rew_mean          | 77.4        |
| time/                   |             |
|    fps                  | 931         |
|    iterations           | 23          |
|    time_elapsed         | 252         |
|    total_timesteps      | 235520      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10226       |
|    mean_motor0          | 0.6905562   |
|    mean_motor1          | 0.687227    |
|    mean_motor2          | 0.7071385   |
|    mean_motor3          | 0.6989629   |
|    mean_motor4          | 0.7139393   |
|    mean_motor5          | 0.7035418   |
|    mean_motor6          | 0.68340313  |
|    mean_motor7          | 0.6720291   |
| train/                  |             |
|    approx_kl            | 0.019423395 |
|    average_cost         | 0.0421875   |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.819       |
|    cost_value_loss      | 0.0136      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.94       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.00415    |
|    mean_cost_advantages | 0.024060551 |
|    mean_reward_advan... | 0.027848547 |
|    n_updates            | 440         |
|    nu                   | 2.28        |
|    nu_loss              | -0.0932     |
|    policy_gradient_loss | -0.00456    |
|    reward_explained_... | 0.878       |
|    reward_value_loss    | 0.0158      |
|    std                  | 0.834       |
|    total_cost           | 432.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 269           |
|    mean_ep_length       | 37.4          |
|    mean_reward          | -14.4         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.04          |
|    forward_reward       | 0.469         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.77         |
|    reward_forward       | 0.469         |
|    reward_survive       | 1             |
|    x_position           | -0.288        |
|    x_velocity           | 0.469         |
|    y_position           | 0.375         |
|    y_velocity           | 0.41          |
| rollout/                |               |
|    adjusted_reward      | 0.871         |
|    ep_len_mean          | 149           |
|    ep_rew_mean          | 124           |
| time/                   |               |
|    fps                  | 932           |
|    iterations           | 24            |
|    time_elapsed         | 263           |
|    total_timesteps      | 245760        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10226         |
|    mean_motor0          | 0.6878034     |
|    mean_motor1          | 0.6810079     |
|    mean_motor2          | 0.6817852     |
|    mean_motor3          | 0.68032455    |
|    mean_motor4          | 0.70408803    |
|    mean_motor5          | 0.6944337     |
|    mean_motor6          | 0.6780192     |
|    mean_motor7          | 0.66465545    |
| train/                  |               |
|    approx_kl            | 0.01857628    |
|    average_cost         | 0.017773438   |
|    clip_fraction        | 0.00738       |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.868         |
|    cost_value_loss      | 0.0146        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -9.87         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0317        |
|    mean_cost_advantages | -0.0061697387 |
|    mean_reward_advan... | 0.015290665   |
|    n_updates            | 460           |
|    nu                   | 2.35          |
|    nu_loss              | -0.0405       |
|    policy_gradient_loss | -0.0042       |
|    reward_explained_... | 0.82          |
|    reward_value_loss    | 0.0199        |
|    std                  | 0.827         |
|    total_cost           | 182.0         |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 269         |
|    mean_ep_length       | 170         |
|    mean_reward          | 135         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.43        |
|    forward_reward       | 0.496       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.76       |
|    reward_forward       | 0.496       |
|    reward_survive       | 1           |
|    x_position           | 0.318       |
|    x_velocity           | 0.496       |
|    y_position           | 0.862       |
|    y_velocity           | 0.483       |
| rollout/                |             |
|    adjusted_reward      | 0.636       |
|    ep_len_mean          | 102         |
|    ep_rew_mean          | 60.5        |
| time/                   |             |
|    fps                  | 930         |
|    iterations           | 25          |
|    time_elapsed         | 275         |
|    total_timesteps      | 256000      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10227       |
|    mean_motor0          | 0.67769617  |
|    mean_motor1          | 0.6711346   |
|    mean_motor2          | 0.68315876  |
|    mean_motor3          | 0.67043173  |
|    mean_motor4          | 0.69578856  |
|    mean_motor5          | 0.6901236   |
|    mean_motor6          | 0.6715456   |
|    mean_motor7          | 0.6672014   |
| train/                  |             |
|    approx_kl            | 0.022274371 |
|    average_cost         | 0.036035158 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.88        |
|    cost_value_loss      | 0.0172      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.8        |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0151      |
|    mean_cost_advantages | 0.011017995 |
|    mean_reward_advan... | 0.04292969  |
|    n_updates            | 480         |
|    nu                   | 2.43        |
|    nu_loss              | -0.0847     |
|    policy_gradient_loss | -0.00418    |
|    reward_explained_... | 0.854       |
|    reward_value_loss    | 0.0194      |
|    std                  | 0.82        |
|    total_cost           | 369.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 269         |
|    mean_ep_length       | 141         |
|    mean_reward          | 68.9        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 0.979       |
|    forward_reward       | 0.468       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.69       |
|    reward_forward       | 0.468       |
|    reward_survive       | 1           |
|    x_position           | -0.427      |
|    x_velocity           | 0.468       |
|    y_position           | -0.707      |
|    y_velocity           | 0.847       |
| rollout/                |             |
|    adjusted_reward      | 0.929       |
|    ep_len_mean          | 131         |
|    ep_rew_mean          | 114         |
| time/                   |             |
|    fps                  | 929         |
|    iterations           | 26          |
|    time_elapsed         | 286         |
|    total_timesteps      | 266240      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10224       |
|    mean_motor0          | 0.67170006  |
|    mean_motor1          | 0.65917575  |
|    mean_motor2          | 0.6756402   |
|    mean_motor3          | 0.6652976   |
|    mean_motor4          | 0.6915264   |
|    mean_motor5          | 0.68570936  |
|    mean_motor6          | 0.6702949   |
|    mean_motor7          | 0.658184    |
| train/                  |             |
|    approx_kl            | 0.023198938 |
|    average_cost         | 0.00546875  |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.821       |
|    cost_value_loss      | 0.00922     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.74       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00536     |
|    mean_cost_advantages | -0.00308704 |
|    mean_reward_advan... | 0.025798315 |
|    n_updates            | 500         |
|    nu                   | 2.5         |
|    nu_loss              | -0.0133     |
|    policy_gradient_loss | -0.00495    |
|    reward_explained_... | 0.865       |
|    reward_value_loss    | 0.0208      |
|    std                  | 0.814       |
|    total_cost           | 56.0        |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 288         |
|    mean_ep_length       | 166         |
|    mean_reward          | 288         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 0.819       |
|    forward_reward       | 0.503       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.76       |
|    reward_forward       | 0.503       |
|    reward_survive       | 1           |
|    x_position           | 0.237       |
|    x_velocity           | 0.503       |
|    y_position           | -0.274      |
|    y_velocity           | 0.517       |
| rollout/                |             |
|    adjusted_reward      | 1.03        |
|    ep_len_mean          | 134         |
|    ep_rew_mean          | 137         |
| time/                   |             |
|    fps                  | 928         |
|    iterations           | 27          |
|    time_elapsed         | 297         |
|    total_timesteps      | 276480      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10231       |
|    mean_motor0          | 0.668768    |
|    mean_motor1          | 0.6568812   |
|    mean_motor2          | 0.6691562   |
|    mean_motor3          | 0.6885904   |
|    mean_motor4          | 0.69211966  |
|    mean_motor5          | 0.6847213   |
|    mean_motor6          | 0.66490704  |
|    mean_motor7          | 0.66097915  |
| train/                  |             |
|    approx_kl            | 0.012527317 |
|    average_cost         | 0.04921875  |
|    clip_fraction        | 0.00703     |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.917       |
|    cost_value_loss      | 0.0161      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.68       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0106      |
|    mean_cost_advantages | 0.019942934 |
|    mean_reward_advan... | 0.04573447  |
|    n_updates            | 520         |
|    nu                   | 2.57        |
|    nu_loss              | -0.123      |
|    policy_gradient_loss | -0.00331    |
|    reward_explained_... | 0.859       |
|    reward_value_loss    | 0.0259      |
|    std                  | 0.81        |
|    total_cost           | 504.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 244          |
|    mean_reward          | 64.4         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 0.928        |
|    forward_reward       | 0.547        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.75        |
|    reward_forward       | 0.547        |
|    reward_survive       | 1            |
|    x_position           | -0.405       |
|    x_velocity           | 0.547        |
|    y_position           | 0.119        |
|    y_velocity           | 0.514        |
| rollout/                |              |
|    adjusted_reward      | 0.737        |
|    ep_len_mean          | 133          |
|    ep_rew_mean          | 101          |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 28           |
|    time_elapsed         | 309          |
|    total_timesteps      | 286720       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10228        |
|    mean_motor0          | 0.666093     |
|    mean_motor1          | 0.6584196    |
|    mean_motor2          | 0.67085963   |
|    mean_motor3          | 0.67051053   |
|    mean_motor4          | 0.6905662    |
|    mean_motor5          | 0.6859642    |
|    mean_motor6          | 0.6708496    |
|    mean_motor7          | 0.6523671    |
| train/                  |              |
|    approx_kl            | 0.01349203   |
|    average_cost         | 0.029882813  |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.863        |
|    cost_value_loss      | 0.0192       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.63        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0115       |
|    mean_cost_advantages | 0.0036679953 |
|    mean_reward_advan... | 0.046464704  |
|    n_updates            | 540          |
|    nu                   | 2.65         |
|    nu_loss              | -0.0769      |
|    policy_gradient_loss | -0.00412     |
|    reward_explained_... | 0.893        |
|    reward_value_loss    | 0.03         |
|    std                  | 0.803        |
|    total_cost           | 306.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 171          |
|    mean_reward          | 44.3         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.74         |
|    forward_reward       | 0.391        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.76        |
|    reward_forward       | 0.391        |
|    reward_survive       | 1            |
|    x_position           | 0.354        |
|    x_velocity           | 0.391        |
|    y_position           | 0.669        |
|    y_velocity           | 0.336        |
| rollout/                |              |
|    adjusted_reward      | 0.842        |
|    ep_len_mean          | 147          |
|    ep_rew_mean          | 107          |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 29           |
|    time_elapsed         | 321          |
|    total_timesteps      | 296960       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10218        |
|    mean_motor0          | 0.6582564    |
|    mean_motor1          | 0.6475295    |
|    mean_motor2          | 0.67068475   |
|    mean_motor3          | 0.6777658    |
|    mean_motor4          | 0.6863186    |
|    mean_motor5          | 0.6751019    |
|    mean_motor6          | 0.65944874   |
|    mean_motor7          | 0.65315187   |
| train/                  |              |
|    approx_kl            | 0.016806353  |
|    average_cost         | 0.0296875    |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.847        |
|    cost_value_loss      | 0.0209       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.58        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0174       |
|    mean_cost_advantages | 0.0041371128 |
|    mean_reward_advan... | 0.0263044    |
|    n_updates            | 560          |
|    nu                   | 2.73         |
|    nu_loss              | -0.0787      |
|    policy_gradient_loss | -0.00409     |
|    reward_explained_... | 0.826        |
|    reward_value_loss    | 0.0274       |
|    std                  | 0.799        |
|    total_cost           | 304.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 122          |
|    mean_reward          | 176          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.27         |
|    forward_reward       | 0.437        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.72        |
|    reward_forward       | 0.437        |
|    reward_survive       | 1            |
|    x_position           | -0.511       |
|    x_velocity           | 0.437        |
|    y_position           | -0.0206      |
|    y_velocity           | 0.557        |
| rollout/                |              |
|    adjusted_reward      | 0.477        |
|    ep_len_mean          | 137          |
|    ep_rew_mean          | 93.5         |
| time/                   |              |
|    fps                  | 924          |
|    iterations           | 30           |
|    time_elapsed         | 332          |
|    total_timesteps      | 307200       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10223        |
|    mean_motor0          | 0.65333354   |
|    mean_motor1          | 0.6535958    |
|    mean_motor2          | 0.6631409    |
|    mean_motor3          | 0.672575     |
|    mean_motor4          | 0.67744803   |
|    mean_motor5          | 0.66862106   |
|    mean_motor6          | 0.64133465   |
|    mean_motor7          | 0.6440367    |
| train/                  |              |
|    approx_kl            | 0.012607122  |
|    average_cost         | 0.0078125    |
|    clip_fraction        | 0.00618      |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.827        |
|    cost_value_loss      | 0.011        |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.53        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00991      |
|    mean_cost_advantages | -0.008970969 |
|    mean_reward_advan... | 0.03025338   |
|    n_updates            | 580          |
|    nu                   | 2.8          |
|    nu_loss              | -0.0213      |
|    policy_gradient_loss | -0.00341     |
|    reward_explained_... | 0.846        |
|    reward_value_loss    | 0.0237       |
|    std                  | 0.793        |
|    total_cost           | 80.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 69.8         |
|    mean_reward          | -1.18        |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.58         |
|    forward_reward       | 0.9          |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.76        |
|    reward_forward       | 0.9          |
|    reward_survive       | 1            |
|    x_position           | 0.161        |
|    x_velocity           | 0.9          |
|    y_position           | -0.386       |
|    y_velocity           | 0.617        |
| rollout/                |              |
|    adjusted_reward      | 1.07         |
|    ep_len_mean          | 158          |
|    ep_rew_mean          | 134          |
| time/                   |              |
|    fps                  | 925          |
|    iterations           | 31           |
|    time_elapsed         | 343          |
|    total_timesteps      | 317440       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10220        |
|    mean_motor0          | 0.6392082    |
|    mean_motor1          | 0.65131456   |
|    mean_motor2          | 0.6563954    |
|    mean_motor3          | 0.6794467    |
|    mean_motor4          | 0.6778892    |
|    mean_motor5          | 0.6680353    |
|    mean_motor6          | 0.642786     |
|    mean_motor7          | 0.6500312    |
| train/                  |              |
|    approx_kl            | 0.020022798  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.769        |
|    cost_value_loss      | 0.00395      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.46        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00838      |
|    mean_cost_advantages | -0.009895401 |
|    mean_reward_advan... | 0.0027403168 |
|    n_updates            | 600          |
|    nu                   | 2.87         |
|    nu_loss              | -0.000548    |
|    policy_gradient_loss | -0.00412     |
|    reward_explained_... | 0.822        |
|    reward_value_loss    | 0.0209       |
|    std                  | 0.786        |
|    total_cost           | 2.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 288         |
|    mean_ep_length       | 251         |
|    mean_reward          | 43.2        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.38        |
|    forward_reward       | 0.515       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.74       |
|    reward_forward       | 0.515       |
|    reward_survive       | 1           |
|    x_position           | -0.231      |
|    x_velocity           | 0.515       |
|    y_position           | 0.266       |
|    y_velocity           | 0.391       |
| rollout/                |             |
|    adjusted_reward      | 0.709       |
|    ep_len_mean          | 200         |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 923         |
|    iterations           | 32          |
|    time_elapsed         | 354         |
|    total_timesteps      | 327680      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10222       |
|    mean_motor0          | 0.64268243  |
|    mean_motor1          | 0.6453645   |
|    mean_motor2          | 0.62781185  |
|    mean_motor3          | 0.6751984   |
|    mean_motor4          | 0.6712569   |
|    mean_motor5          | 0.6719667   |
|    mean_motor6          | 0.6441894   |
|    mean_motor7          | 0.63954175  |
| train/                  |             |
|    approx_kl            | 0.02031507  |
|    average_cost         | 0.020410156 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.827       |
|    cost_value_loss      | 0.00836     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.4        |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00862     |
|    mean_cost_advantages | 0.012059446 |
|    mean_reward_advan... | 0.046404503 |
|    n_updates            | 620         |
|    nu                   | 2.94        |
|    nu_loss              | -0.0587     |
|    policy_gradient_loss | -0.00385    |
|    reward_explained_... | 0.9         |
|    reward_value_loss    | 0.0247      |
|    std                  | 0.78        |
|    total_cost           | 209.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 139          |
|    mean_reward          | 195          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.14         |
|    forward_reward       | 0.381        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.68        |
|    reward_forward       | 0.381        |
|    reward_survive       | 1            |
|    x_position           | -0.105       |
|    x_velocity           | 0.381        |
|    y_position           | -0.0654      |
|    y_velocity           | 0.607        |
| rollout/                |              |
|    adjusted_reward      | 0.634        |
|    ep_len_mean          | 199          |
|    ep_rew_mean          | 137          |
| time/                   |              |
|    fps                  | 922          |
|    iterations           | 33           |
|    time_elapsed         | 366          |
|    total_timesteps      | 337920       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10221        |
|    mean_motor0          | 0.64891356   |
|    mean_motor1          | 0.635584     |
|    mean_motor2          | 0.6388558    |
|    mean_motor3          | 0.665895     |
|    mean_motor4          | 0.6839508    |
|    mean_motor5          | 0.65852773   |
|    mean_motor6          | 0.64276      |
|    mean_motor7          | 0.6402506    |
| train/                  |              |
|    approx_kl            | 0.018162534  |
|    average_cost         | 0.0020507812 |
|    clip_fraction        | 0.0167       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.896        |
|    cost_value_loss      | 0.00373      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.34        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00404      |
|    mean_cost_advantages | -0.010374313 |
|    mean_reward_advan... | 0.020842116  |
|    n_updates            | 640          |
|    nu                   | 3.01         |
|    nu_loss              | -0.00604     |
|    policy_gradient_loss | -0.00432     |
|    reward_explained_... | 0.878        |
|    reward_value_loss    | 0.0187       |
|    std                  | 0.775        |
|    total_cost           | 21.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 288           |
|    mean_ep_length       | 148           |
|    mean_reward          | 76.3          |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 0.968         |
|    forward_reward       | 0.43          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.67         |
|    reward_forward       | 0.43          |
|    reward_survive       | 1             |
|    x_position           | 0.0857        |
|    x_velocity           | 0.43          |
|    y_position           | 0.518         |
|    y_velocity           | 0.408         |
| rollout/                |               |
|    adjusted_reward      | 0.533         |
|    ep_len_mean          | 168           |
|    ep_rew_mean          | 102           |
| time/                   |               |
|    fps                  | 922           |
|    iterations           | 34            |
|    time_elapsed         | 377           |
|    total_timesteps      | 348160        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10219         |
|    mean_motor0          | 0.641723      |
|    mean_motor1          | 0.6475239     |
|    mean_motor2          | 0.63792485    |
|    mean_motor3          | 0.68175673    |
|    mean_motor4          | 0.66042244    |
|    mean_motor5          | 0.66404593    |
|    mean_motor6          | 0.64457905    |
|    mean_motor7          | 0.63575166    |
| train/                  |               |
|    approx_kl            | 0.020011002   |
|    average_cost         | 0.001953125   |
|    clip_fraction        | 0.0154        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.845         |
|    cost_value_loss      | 0.00239       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -9.29         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00275       |
|    mean_cost_advantages | -0.0062742145 |
|    mean_reward_advan... | 0.013766739   |
|    n_updates            | 660           |
|    nu                   | 3.07          |
|    nu_loss              | -0.00587      |
|    policy_gradient_loss | -0.00386      |
|    reward_explained_... | 0.864         |
|    reward_value_loss    | 0.0151        |
|    std                  | 0.77          |
|    total_cost           | 20.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 125          |
|    mean_reward          | 222          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.15         |
|    forward_reward       | 0.425        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.67        |
|    reward_forward       | 0.425        |
|    reward_survive       | 1            |
|    x_position           | 0.204        |
|    x_velocity           | 0.425        |
|    y_position           | 0.179        |
|    y_velocity           | 0.385        |
| rollout/                |              |
|    adjusted_reward      | 0.813        |
|    ep_len_mean          | 149          |
|    ep_rew_mean          | 103          |
| time/                   |              |
|    fps                  | 921          |
|    iterations           | 35           |
|    time_elapsed         | 388          |
|    total_timesteps      | 358400       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10217        |
|    mean_motor0          | 0.63019425   |
|    mean_motor1          | 0.625736     |
|    mean_motor2          | 0.6420821    |
|    mean_motor3          | 0.6854358    |
|    mean_motor4          | 0.67680305   |
|    mean_motor5          | 0.6486769    |
|    mean_motor6          | 0.6421045    |
|    mean_motor7          | 0.6438775    |
| train/                  |              |
|    approx_kl            | 0.018066157  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.831        |
|    cost_value_loss      | 0.00183      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.23        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00486      |
|    mean_cost_advantages | -0.008064121 |
|    mean_reward_advan... | 0.0068259453 |
|    n_updates            | 680          |
|    nu                   | 3.12         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00396     |
|    reward_explained_... | 0.832        |
|    reward_value_loss    | 0.0208       |
|    std                  | 0.764        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 288         |
|    mean_ep_length       | 193         |
|    mean_reward          | 66.4        |
| infos/                  |             |
|    cost                 | 0.0198      |
|    distance_from_origin | 2.01        |
|    forward_reward       | 0.417       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.68       |
|    reward_forward       | 0.417       |
|    reward_survive       | 1           |
|    x_position           | -0.828      |
|    x_velocity           | 0.417       |
|    y_position           | -0.0887     |
|    y_velocity           | 0.475       |
| rollout/                |             |
|    adjusted_reward      | 0.764       |
|    ep_len_mean          | 177         |
|    ep_rew_mean          | 138         |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 36          |
|    time_elapsed         | 400         |
|    total_timesteps      | 368640      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10221       |
|    mean_motor0          | 0.6371769   |
|    mean_motor1          | 0.6273304   |
|    mean_motor2          | 0.63527644  |
|    mean_motor3          | 0.68463     |
|    mean_motor4          | 0.6606737   |
|    mean_motor5          | 0.6498648   |
|    mean_motor6          | 0.6399073   |
|    mean_motor7          | 0.63505346  |
| train/                  |             |
|    approx_kl            | 0.019027121 |
|    average_cost         | 0.046777345 |
|    clip_fraction        | 0.0167      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.581       |
|    cost_value_loss      | 0.0123      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.16       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00939     |
|    mean_cost_advantages | 0.032640196 |
|    mean_reward_advan... | 0.030807167 |
|    n_updates            | 700         |
|    nu                   | 3.18        |
|    nu_loss              | -0.146      |
|    policy_gradient_loss | -0.00371    |
|    reward_explained_... | 0.834       |
|    reward_value_loss    | 0.024       |
|    std                  | 0.757       |
|    total_cost           | 479.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 162          |
|    mean_reward          | 93.6         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.67         |
|    forward_reward       | 0.375        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.62        |
|    reward_forward       | 0.375        |
|    reward_survive       | 1            |
|    x_position           | -0.283       |
|    x_velocity           | 0.375        |
|    y_position           | 0.594        |
|    y_velocity           | 0.321        |
| rollout/                |              |
|    adjusted_reward      | 0.897        |
|    ep_len_mean          | 210          |
|    ep_rew_mean          | 182          |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 37           |
|    time_elapsed         | 411          |
|    total_timesteps      | 378880       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10219        |
|    mean_motor0          | 0.6304866    |
|    mean_motor1          | 0.627969     |
|    mean_motor2          | 0.6219136    |
|    mean_motor3          | 0.6878113    |
|    mean_motor4          | 0.6707954    |
|    mean_motor5          | 0.647383     |
|    mean_motor6          | 0.63608295   |
|    mean_motor7          | 0.6306132    |
| train/                  |              |
|    approx_kl            | 0.015465677  |
|    average_cost         | 0.024316406  |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.789        |
|    cost_value_loss      | 0.00832      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -9.09        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00888      |
|    mean_cost_advantages | 0.0136981215 |
|    mean_reward_advan... | 0.026902724  |
|    n_updates            | 720          |
|    nu                   | 3.25         |
|    nu_loss              | -0.0774      |
|    policy_gradient_loss | -0.00428     |
|    reward_explained_... | 0.86         |
|    reward_value_loss    | 0.0202       |
|    std                  | 0.751        |
|    total_cost           | 249.0        |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 288         |
|    mean_ep_length       | 187         |
|    mean_reward          | 25.8        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 0.991       |
|    forward_reward       | 0.682       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.66       |
|    reward_forward       | 0.682       |
|    reward_survive       | 1           |
|    x_position           | -0.309      |
|    x_velocity           | 0.682       |
|    y_position           | 0.13        |
|    y_velocity           | 0.47        |
| rollout/                |             |
|    adjusted_reward      | 0.576       |
|    ep_len_mean          | 204         |
|    ep_rew_mean          | 160         |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 38          |
|    time_elapsed         | 423         |
|    total_timesteps      | 389120      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10223       |
|    mean_motor0          | 0.63487446  |
|    mean_motor1          | 0.6088294   |
|    mean_motor2          | 0.62595814  |
|    mean_motor3          | 0.65633553  |
|    mean_motor4          | 0.6612927   |
|    mean_motor5          | 0.6477911   |
|    mean_motor6          | 0.63124347  |
|    mean_motor7          | 0.62246215  |
| train/                  |             |
|    approx_kl            | 0.019995779 |
|    average_cost         | 0.054882813 |
|    clip_fraction        | 0.0205      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.87        |
|    cost_value_loss      | 0.0122      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -9.03       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0251      |
|    mean_cost_advantages | 0.037709676 |
|    mean_reward_advan... | 0.037751578 |
|    n_updates            | 740         |
|    nu                   | 3.32        |
|    nu_loss              | -0.178      |
|    policy_gradient_loss | -0.0039     |
|    reward_explained_... | 0.88        |
|    reward_value_loss    | 0.0224      |
|    std                  | 0.745       |
|    total_cost           | 562.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 288          |
|    mean_ep_length       | 84.4         |
|    mean_reward          | 10.5         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.93         |
|    forward_reward       | 0.38         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.57        |
|    reward_forward       | 0.38         |
|    reward_survive       | 1            |
|    x_position           | 0.0413       |
|    x_velocity           | 0.38         |
|    y_position           | -0.12        |
|    y_velocity           | 0.528        |
| rollout/                |              |
|    adjusted_reward      | 0.936        |
|    ep_len_mean          | 179          |
|    ep_rew_mean          | 132          |
| time/                   |              |
|    fps                  | 919          |
|    iterations           | 39           |
|    time_elapsed         | 434          |
|    total_timesteps      | 399360       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10211        |
|    mean_motor0          | 0.6264249    |
|    mean_motor1          | 0.6142045    |
|    mean_motor2          | 0.60727865   |
|    mean_motor3          | 0.66329706   |
|    mean_motor4          | 0.6512929    |
|    mean_motor5          | 0.64249873   |
|    mean_motor6          | 0.6145942    |
|    mean_motor7          | 0.61760795   |
| train/                  |              |
|    approx_kl            | 0.023853147  |
|    average_cost         | 0.0030273437 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.91         |
|    cost_value_loss      | 0.00412      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.96        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00392      |
|    mean_cost_advantages | -0.00788258  |
|    mean_reward_advan... | 0.008917933  |
|    n_updates            | 760          |
|    nu                   | 3.39         |
|    nu_loss              | -0.0101      |
|    policy_gradient_loss | -0.00454     |
|    reward_explained_... | 0.881        |
|    reward_value_loss    | 0.0193       |
|    std                  | 0.738        |
|    total_cost           | 31.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 288           |
|    mean_ep_length       | 144           |
|    mean_reward          | 128           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.6           |
|    forward_reward       | 0.854         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.72         |
|    reward_forward       | 0.854         |
|    reward_survive       | 1             |
|    x_position           | 0.704         |
|    x_velocity           | 0.854         |
|    y_position           | -0.835        |
|    y_velocity           | 0.611         |
| rollout/                |               |
|    adjusted_reward      | 0.89          |
|    ep_len_mean          | 161           |
|    ep_rew_mean          | 141           |
| time/                   |               |
|    fps                  | 918           |
|    iterations           | 40            |
|    time_elapsed         | 445           |
|    total_timesteps      | 409600        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10215         |
|    mean_motor0          | 0.61634403    |
|    mean_motor1          | 0.5990827     |
|    mean_motor2          | 0.61812127    |
|    mean_motor3          | 0.66086775    |
|    mean_motor4          | 0.64568174    |
|    mean_motor5          | 0.6408699     |
|    mean_motor6          | 0.6386646     |
|    mean_motor7          | 0.61831945    |
| train/                  |               |
|    approx_kl            | 0.021758389   |
|    average_cost         | 0.008496094   |
|    clip_fraction        | 0.0208        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.86          |
|    cost_value_loss      | 0.00744       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -8.9          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00854       |
|    mean_cost_advantages | -0.0017624197 |
|    mean_reward_advan... | 0.0328036     |
|    n_updates            | 780           |
|    nu                   | 3.46          |
|    nu_loss              | -0.0288       |
|    policy_gradient_loss | -0.00387      |
|    reward_explained_... | 0.838         |
|    reward_value_loss    | 0.0263        |
|    std                  | 0.734         |
|    total_cost           | 87.0          |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 288         |
|    mean_ep_length       | 160         |
|    mean_reward          | 270         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.69        |
|    forward_reward       | 0.349       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.51       |
|    reward_forward       | 0.349       |
|    reward_survive       | 1           |
|    x_position           | -0.596      |
|    x_velocity           | 0.349       |
|    y_position           | -0.824      |
|    y_velocity           | 0.432       |
| rollout/                |             |
|    adjusted_reward      | 0.706       |
|    ep_len_mean          | 167         |
|    ep_rew_mean          | 131         |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 41          |
|    time_elapsed         | 457         |
|    total_timesteps      | 419840      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10214       |
|    mean_motor0          | 0.6199129   |
|    mean_motor1          | 0.6074817   |
|    mean_motor2          | 0.59959424  |
|    mean_motor3          | 0.6552199   |
|    mean_motor4          | 0.66317165  |
|    mean_motor5          | 0.6387452   |
|    mean_motor6          | 0.6244825   |
|    mean_motor7          | 0.61790234  |
| train/                  |             |
|    approx_kl            | 0.026007455 |
|    average_cost         | 0.05205078  |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.739       |
|    cost_value_loss      | 0.00964     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.83       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00639     |
|    mean_cost_advantages | 0.036314927 |
|    mean_reward_advan... | 0.02582055  |
|    n_updates            | 800         |
|    nu                   | 3.54        |
|    nu_loss              | -0.18       |
|    policy_gradient_loss | -0.00476    |
|    reward_explained_... | 0.843       |
|    reward_value_loss    | 0.0254      |
|    std                  | 0.727       |
|    total_cost           | 533.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 318          |
|    mean_ep_length       | 219          |
|    mean_reward          | 318          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.26         |
|    forward_reward       | 0.335        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.63        |
|    reward_forward       | 0.335        |
|    reward_survive       | 1            |
|    x_position           | 0.343        |
|    x_velocity           | 0.335        |
|    y_position           | 0.146        |
|    y_velocity           | 0.479        |
| rollout/                |              |
|    adjusted_reward      | 1.1          |
|    ep_len_mean          | 217          |
|    ep_rew_mean          | 207          |
| time/                   |              |
|    fps                  | 917          |
|    iterations           | 42           |
|    time_elapsed         | 468          |
|    total_timesteps      | 430080       |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10208        |
|    mean_motor0          | 0.6107163    |
|    mean_motor1          | 0.5949368    |
|    mean_motor2          | 0.5993475    |
|    mean_motor3          | 0.6337793    |
|    mean_motor4          | 0.6563536    |
|    mean_motor5          | 0.64779216   |
|    mean_motor6          | 0.6337513    |
|    mean_motor7          | 0.6205709    |
| train/                  |              |
|    approx_kl            | 0.022215571  |
|    average_cost         | 0.014257813  |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.866        |
|    cost_value_loss      | 0.00896      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.76        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0181       |
|    mean_cost_advantages | 0.0047492282 |
|    mean_reward_advan... | 0.015932322  |
|    n_updates            | 820          |
|    nu                   | 3.61         |
|    nu_loss              | -0.0504      |
|    policy_gradient_loss | -0.00409     |
|    reward_explained_... | 0.868        |
|    reward_value_loss    | 0.0211       |
|    std                  | 0.721        |
|    total_cost           | 146.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 318           |
|    mean_ep_length       | 169           |
|    mean_reward          | 229           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 2.02          |
|    forward_reward       | 0.372         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.65         |
|    reward_forward       | 0.372         |
|    reward_survive       | 1             |
|    x_position           | 1.05          |
|    x_velocity           | 0.372         |
|    y_position           | 0.263         |
|    y_velocity           | 0.322         |
| rollout/                |               |
|    adjusted_reward      | 0.659         |
|    ep_len_mean          | 204           |
|    ep_rew_mean          | 173           |
| time/                   |               |
|    fps                  | 916           |
|    iterations           | 43            |
|    time_elapsed         | 480           |
|    total_timesteps      | 440320        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10200         |
|    mean_motor0          | 0.6001324     |
|    mean_motor1          | 0.5915167     |
|    mean_motor2          | 0.5976553     |
|    mean_motor3          | 0.62737304    |
|    mean_motor4          | 0.6506821     |
|    mean_motor5          | 0.63775337    |
|    mean_motor6          | 0.63516253    |
|    mean_motor7          | 0.5979849     |
| train/                  |               |
|    approx_kl            | 0.022582922   |
|    average_cost         | 0.0014648438  |
|    clip_fraction        | 0.0228        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.91          |
|    cost_value_loss      | 0.00382       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -8.7          |
|    learning_rate        | 3e-05         |
|    loss                 | -2.02e-05     |
|    mean_cost_advantages | -0.0039800466 |
|    mean_reward_advan... | 0.041586496   |
|    n_updates            | 840           |
|    nu                   | 3.68          |
|    nu_loss              | -0.00529      |
|    policy_gradient_loss | -0.00347      |
|    reward_explained_... | 0.907         |
|    reward_value_loss    | 0.02          |
|    std                  | 0.716         |
|    total_cost           | 15.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 318          |
|    mean_ep_length       | 92.6         |
|    mean_reward          | 90.7         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.12         |
|    forward_reward       | 0.507        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.59        |
|    reward_forward       | 0.507        |
|    reward_survive       | 1            |
|    x_position           | -0.334       |
|    x_velocity           | 0.507        |
|    y_position           | -0.0562      |
|    y_velocity           | 0.475        |
| rollout/                |              |
|    adjusted_reward      | 0.903        |
|    ep_len_mean          | 188          |
|    ep_rew_mean          | 149          |
| time/                   |              |
|    fps                  | 916          |
|    iterations           | 44           |
|    time_elapsed         | 491          |
|    total_timesteps      | 450560       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10223        |
|    mean_motor0          | 0.5905718    |
|    mean_motor1          | 0.5918446    |
|    mean_motor2          | 0.6045245    |
|    mean_motor3          | 0.6466314    |
|    mean_motor4          | 0.6397108    |
|    mean_motor5          | 0.635831     |
|    mean_motor6          | 0.6336715    |
|    mean_motor7          | 0.6084608    |
| train/                  |              |
|    approx_kl            | 0.019326353  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0119       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.909        |
|    cost_value_loss      | 0.00301      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.65        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00978      |
|    mean_cost_advantages | -0.009381244 |
|    mean_reward_advan... | 0.0065581067 |
|    n_updates            | 860          |
|    nu                   | 3.74         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00293     |
|    reward_explained_... | 0.889        |
|    reward_value_loss    | 0.0219       |
|    std                  | 0.711        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 373         |
|    mean_reward          | 689         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.08        |
|    forward_reward       | 0.5         |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.73       |
|    reward_forward       | 0.5         |
|    reward_survive       | 1           |
|    x_position           | 1.52        |
|    x_velocity           | 0.5         |
|    y_position           | 0.501       |
|    y_velocity           | 0.321       |
| rollout/                |             |
|    adjusted_reward      | 0.951       |
|    ep_len_mean          | 211         |
|    ep_rew_mean          | 200         |
| time/                   |             |
|    fps                  | 914         |
|    iterations           | 45          |
|    time_elapsed         | 503         |
|    total_timesteps      | 460800      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10212       |
|    mean_motor0          | 0.5938718   |
|    mean_motor1          | 0.58476305  |
|    mean_motor2          | 0.5950984   |
|    mean_motor3          | 0.62984097  |
|    mean_motor4          | 0.6430756   |
|    mean_motor5          | 0.6287569   |
|    mean_motor6          | 0.6476011   |
|    mean_motor7          | 0.61529076  |
| train/                  |             |
|    approx_kl            | 0.029456222 |
|    average_cost         | 0.03359375  |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.783       |
|    cost_value_loss      | 0.00979     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.6        |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0022     |
|    mean_cost_advantages | 0.020883324 |
|    mean_reward_advan... | 0.023192365 |
|    n_updates            | 880         |
|    nu                   | 3.8         |
|    nu_loss              | -0.126      |
|    policy_gradient_loss | -0.0047     |
|    reward_explained_... | 0.904       |
|    reward_value_loss    | 0.024       |
|    std                  | 0.708       |
|    total_cost           | 344.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 323         |
|    mean_reward          | 547         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.52        |
|    forward_reward       | 0.334       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.6        |
|    reward_forward       | 0.334       |
|    reward_survive       | 1           |
|    x_position           | -0.225      |
|    x_velocity           | 0.334       |
|    y_position           | -1.29       |
|    y_velocity           | 0.481       |
| rollout/                |             |
|    adjusted_reward      | 0.736       |
|    ep_len_mean          | 203         |
|    ep_rew_mean          | 169         |
| time/                   |             |
|    fps                  | 912         |
|    iterations           | 46          |
|    time_elapsed         | 516         |
|    total_timesteps      | 471040      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10209       |
|    mean_motor0          | 0.5849804   |
|    mean_motor1          | 0.5844008   |
|    mean_motor2          | 0.5994957   |
|    mean_motor3          | 0.62452984  |
|    mean_motor4          | 0.64691883  |
|    mean_motor5          | 0.62686646  |
|    mean_motor6          | 0.6346644   |
|    mean_motor7          | 0.61624324  |
| train/                  |             |
|    approx_kl            | 0.023433154 |
|    average_cost         | 0.042089842 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.882       |
|    cost_value_loss      | 0.0116      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.57       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00978     |
|    mean_cost_advantages | 0.02608507  |
|    mean_reward_advan... | 0.025375595 |
|    n_updates            | 900         |
|    nu                   | 3.88        |
|    nu_loss              | -0.16       |
|    policy_gradient_loss | -0.00332    |
|    reward_explained_... | 0.907       |
|    reward_value_loss    | 0.0278      |
|    std                  | 0.704       |
|    total_cost           | 431.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 226         |
|    mean_reward          | 190         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.54        |
|    forward_reward       | 0.312       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.57       |
|    reward_forward       | 0.312       |
|    reward_survive       | 1           |
|    x_position           | 0.63        |
|    x_velocity           | 0.312       |
|    y_position           | 1.14        |
|    y_velocity           | 0.428       |
| rollout/                |             |
|    adjusted_reward      | 0.823       |
|    ep_len_mean          | 215         |
|    ep_rew_mean          | 167         |
| time/                   |             |
|    fps                  | 911         |
|    iterations           | 47          |
|    time_elapsed         | 527         |
|    total_timesteps      | 481280      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10214       |
|    mean_motor0          | 0.5769747   |
|    mean_motor1          | 0.58427054  |
|    mean_motor2          | 0.59945714  |
|    mean_motor3          | 0.6160018   |
|    mean_motor4          | 0.659135    |
|    mean_motor5          | 0.6239381   |
|    mean_motor6          | 0.6524034   |
|    mean_motor7          | 0.6090683   |
| train/                  |             |
|    approx_kl            | 0.022434512 |
|    average_cost         | 0.011621093 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.901       |
|    cost_value_loss      | 0.00659     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.52       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00821     |
|    mean_cost_advantages | 0.004703927 |
|    mean_reward_advan... | 0.01330577  |
|    n_updates            | 920         |
|    nu                   | 3.95        |
|    nu_loss              | -0.0451     |
|    policy_gradient_loss | -0.00345    |
|    reward_explained_... | 0.91        |
|    reward_value_loss    | 0.0228      |
|    std                  | 0.7         |
|    total_cost           | 119.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 689          |
|    mean_ep_length       | 250          |
|    mean_reward          | 97.6         |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2            |
|    forward_reward       | 0.365        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.65        |
|    reward_forward       | 0.365        |
|    reward_survive       | 1            |
|    x_position           | -0.435       |
|    x_velocity           | 0.365        |
|    y_position           | 0.97         |
|    y_velocity           | 0.457        |
| rollout/                |              |
|    adjusted_reward      | 1.24         |
|    ep_len_mean          | 223          |
|    ep_rew_mean          | 218          |
| time/                   |              |
|    fps                  | 910          |
|    iterations           | 48           |
|    time_elapsed         | 539          |
|    total_timesteps      | 491520       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10204        |
|    mean_motor0          | 0.5709856    |
|    mean_motor1          | 0.5851303    |
|    mean_motor2          | 0.5897788    |
|    mean_motor3          | 0.6003183    |
|    mean_motor4          | 0.6574687    |
|    mean_motor5          | 0.62729925   |
|    mean_motor6          | 0.6345457    |
|    mean_motor7          | 0.6084774    |
| train/                  |              |
|    approx_kl            | 0.020457232  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.907        |
|    cost_value_loss      | 0.00231      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.47        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0101       |
|    mean_cost_advantages | -0.007430279 |
|    mean_reward_advan... | 0.018724835  |
|    n_updates            | 940          |
|    nu                   | 4.01         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00334     |
|    reward_explained_... | 0.909        |
|    reward_value_loss    | 0.021        |
|    std                  | 0.695        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 161         |
|    mean_reward          | 187         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.5         |
|    forward_reward       | 0.479       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.59       |
|    reward_forward       | 0.479       |
|    reward_survive       | 1           |
|    x_position           | -0.306      |
|    x_velocity           | 0.479       |
|    y_position           | 0.0432      |
|    y_velocity           | 0.548       |
| rollout/                |             |
|    adjusted_reward      | 1.07        |
|    ep_len_mean          | 230         |
|    ep_rew_mean          | 263         |
| time/                   |             |
|    fps                  | 910         |
|    iterations           | 49          |
|    time_elapsed         | 551         |
|    total_timesteps      | 501760      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10205       |
|    mean_motor0          | 0.5727318   |
|    mean_motor1          | 0.57412076  |
|    mean_motor2          | 0.58183867  |
|    mean_motor3          | 0.5978011   |
|    mean_motor4          | 0.65758073  |
|    mean_motor5          | 0.6203954   |
|    mean_motor6          | 0.62392324  |
|    mean_motor7          | 0.6091541   |
| train/                  |             |
|    approx_kl            | 0.026170855 |
|    average_cost         | 0.024023438 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.892       |
|    cost_value_loss      | 0.011       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.41       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0119      |
|    mean_cost_advantages | 0.011254578 |
|    mean_reward_advan... | 0.047443278 |
|    n_updates            | 960         |
|    nu                   | 4.08        |
|    nu_loss              | -0.0964     |
|    policy_gradient_loss | -0.00383    |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.0327      |
|    std                  | 0.69        |
|    total_cost           | 246.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 190           |
|    mean_reward          | 333           |
| infos/                  |               |
|    cost                 | 0.0202        |
|    distance_from_origin | 1.33          |
|    forward_reward       | 0.371         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.68         |
|    reward_forward       | 0.371         |
|    reward_survive       | 1             |
|    x_position           | -0.586        |
|    x_velocity           | 0.371         |
|    y_position           | -0.898        |
|    y_velocity           | 0.414         |
| rollout/                |               |
|    adjusted_reward      | 1.13          |
|    ep_len_mean          | 252           |
|    ep_rew_mean          | 310           |
| time/                   |               |
|    fps                  | 909           |
|    iterations           | 50            |
|    time_elapsed         | 562           |
|    total_timesteps      | 512000        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10206         |
|    mean_motor0          | 0.5647272     |
|    mean_motor1          | 0.5625526     |
|    mean_motor2          | 0.58600223    |
|    mean_motor3          | 0.59177047    |
|    mean_motor4          | 0.65201175    |
|    mean_motor5          | 0.6267743     |
|    mean_motor6          | 0.60723794    |
|    mean_motor7          | 0.5988871     |
| train/                  |               |
|    approx_kl            | 0.020377522   |
|    average_cost         | 0.0073242188  |
|    clip_fraction        | 0.0191        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.876         |
|    cost_value_loss      | 0.00473       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -8.35         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0343        |
|    mean_cost_advantages | 0.00091825874 |
|    mean_reward_advan... | 0.028837096   |
|    n_updates            | 980           |
|    nu                   | 4.14          |
|    nu_loss              | -0.0299       |
|    policy_gradient_loss | -0.00323      |
|    reward_explained_... | 0.904         |
|    reward_value_loss    | 0.0333        |
|    std                  | 0.685         |
|    total_cost           | 75.0          |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 245         |
|    mean_reward          | 134         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.83        |
|    forward_reward       | 0.208       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.56       |
|    reward_forward       | 0.208       |
|    reward_survive       | 1           |
|    x_position           | 0.227       |
|    x_velocity           | 0.208       |
|    y_position           | 1.1         |
|    y_velocity           | 0.243       |
| rollout/                |             |
|    adjusted_reward      | 1.18        |
|    ep_len_mean          | 252         |
|    ep_rew_mean          | 296         |
| time/                   |             |
|    fps                  | 908         |
|    iterations           | 51          |
|    time_elapsed         | 574         |
|    total_timesteps      | 522240      |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10236       |
|    greater_than_0.5     | 10202       |
|    mean_motor0          | 0.5615376   |
|    mean_motor1          | 0.56415415  |
|    mean_motor2          | 0.58869207  |
|    mean_motor3          | 0.5920993   |
|    mean_motor4          | 0.66144514  |
|    mean_motor5          | 0.6113661   |
|    mean_motor6          | 0.6222446   |
|    mean_motor7          | 0.59431875  |
| train/                  |             |
|    approx_kl            | 0.02381867  |
|    average_cost         | 0.019433593 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.882       |
|    cost_value_loss      | 0.00536     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.3        |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00958     |
|    mean_cost_advantages | 0.012160766 |
|    mean_reward_advan... | 0.037084557 |
|    n_updates            | 1000        |
|    nu                   | 4.21        |
|    nu_loss              | -0.0805     |
|    policy_gradient_loss | -0.00358    |
|    reward_explained_... | 0.921       |
|    reward_value_loss    | 0.0262      |
|    std                  | 0.681       |
|    total_cost           | 199.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 689          |
|    mean_ep_length       | 401          |
|    mean_reward          | 415          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.22         |
|    forward_reward       | 0.398        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.52        |
|    reward_forward       | 0.398        |
|    reward_survive       | 1            |
|    x_position           | 0.557        |
|    x_velocity           | 0.398        |
|    y_position           | 0.467        |
|    y_velocity           | 0.391        |
| rollout/                |              |
|    adjusted_reward      | 1.03         |
|    ep_len_mean          | 228          |
|    ep_rew_mean          | 247          |
| time/                   |              |
|    fps                  | 906          |
|    iterations           | 52           |
|    time_elapsed         | 587          |
|    total_timesteps      | 532480       |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10202        |
|    mean_motor0          | 0.56515396   |
|    mean_motor1          | 0.5541278    |
|    mean_motor2          | 0.5914226    |
|    mean_motor3          | 0.5910762    |
|    mean_motor4          | 0.63692653   |
|    mean_motor5          | 0.6009704    |
|    mean_motor6          | 0.61892927   |
|    mean_motor7          | 0.60976833   |
| train/                  |              |
|    approx_kl            | 0.030986164  |
|    average_cost         | 0.019042969  |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.923        |
|    cost_value_loss      | 0.00971      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.24        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00401      |
|    mean_cost_advantages | 0.0060884813 |
|    mean_reward_advan... | 0.032935407  |
|    n_updates            | 1020         |
|    nu                   | 4.27         |
|    nu_loss              | -0.0801      |
|    policy_gradient_loss | -0.00353     |
|    reward_explained_... | 0.903        |
|    reward_value_loss    | 0.0325       |
|    std                  | 0.675        |
|    total_cost           | 195.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 689          |
|    mean_ep_length       | 240          |
|    mean_reward          | 132          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.51         |
|    forward_reward       | 0.302        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.5         |
|    reward_forward       | 0.302        |
|    reward_survive       | 1            |
|    x_position           | -0.821       |
|    x_velocity           | 0.302        |
|    y_position           | -0.487       |
|    y_velocity           | 0.371        |
| rollout/                |              |
|    adjusted_reward      | 1.03         |
|    ep_len_mean          | 228          |
|    ep_rew_mean          | 240          |
| time/                   |              |
|    fps                  | 905          |
|    iterations           | 53           |
|    time_elapsed         | 599          |
|    total_timesteps      | 542720       |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10198        |
|    mean_motor0          | 0.55843383   |
|    mean_motor1          | 0.5559902    |
|    mean_motor2          | 0.58651215   |
|    mean_motor3          | 0.5888023    |
|    mean_motor4          | 0.64157647   |
|    mean_motor5          | 0.609429     |
|    mean_motor6          | 0.6210966    |
|    mean_motor7          | 0.6170983    |
| train/                  |              |
|    approx_kl            | 0.020777347  |
|    average_cost         | 0.015625     |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.834        |
|    cost_value_loss      | 0.00894      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.19        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00123      |
|    mean_cost_advantages | 0.0039897426 |
|    mean_reward_advan... | 0.025289556  |
|    n_updates            | 1040         |
|    nu                   | 4.34         |
|    nu_loss              | -0.0668      |
|    policy_gradient_loss | -0.00362     |
|    reward_explained_... | 0.889        |
|    reward_value_loss    | 0.0278       |
|    std                  | 0.672        |
|    total_cost           | 160.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 228           |
|    mean_reward          | 235           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.15          |
|    forward_reward       | 0.326         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.5          |
|    reward_forward       | 0.326         |
|    reward_survive       | 1             |
|    x_position           | -0.0273       |
|    x_velocity           | 0.326         |
|    y_position           | -0.741        |
|    y_velocity           | 0.695         |
| rollout/                |               |
|    adjusted_reward      | 1.06          |
|    ep_len_mean          | 245           |
|    ep_rew_mean          | 268           |
| time/                   |               |
|    fps                  | 904           |
|    iterations           | 54            |
|    time_elapsed         | 611           |
|    total_timesteps      | 552960        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10194         |
|    mean_motor0          | 0.56244767    |
|    mean_motor1          | 0.54968363    |
|    mean_motor2          | 0.5704949     |
|    mean_motor3          | 0.58220845    |
|    mean_motor4          | 0.6197587     |
|    mean_motor5          | 0.6035648     |
|    mean_motor6          | 0.61278915    |
|    mean_motor7          | 0.6061193     |
| train/                  |               |
|    approx_kl            | 0.02777778    |
|    average_cost         | 0.00078125    |
|    clip_fraction        | 0.0318        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.825         |
|    cost_value_loss      | 0.00431       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -8.14         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00606       |
|    mean_cost_advantages | -0.0066471077 |
|    mean_reward_advan... | 0.019886574   |
|    n_updates            | 1060          |
|    nu                   | 4.4           |
|    nu_loss              | -0.00339      |
|    policy_gradient_loss | -0.00381      |
|    reward_explained_... | 0.869         |
|    reward_value_loss    | 0.0286        |
|    std                  | 0.667         |
|    total_cost           | 8.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 189         |
|    mean_reward          | 30.5        |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.32        |
|    forward_reward       | 0.496       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.5        |
|    reward_forward       | 0.496       |
|    reward_survive       | 1           |
|    x_position           | 0.178       |
|    x_velocity           | 0.496       |
|    y_position           | -0.0939     |
|    y_velocity           | 0.297       |
| rollout/                |             |
|    adjusted_reward      | 1.07        |
|    ep_len_mean          | 258         |
|    ep_rew_mean          | 271         |
| time/                   |             |
|    fps                  | 904         |
|    iterations           | 55          |
|    time_elapsed         | 622         |
|    total_timesteps      | 563200      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10195       |
|    mean_motor0          | 0.5585624   |
|    mean_motor1          | 0.5405766   |
|    mean_motor2          | 0.5853416   |
|    mean_motor3          | 0.5875163   |
|    mean_motor4          | 0.6232914   |
|    mean_motor5          | 0.61085796  |
|    mean_motor6          | 0.6068171   |
|    mean_motor7          | 0.5934065   |
| train/                  |             |
|    approx_kl            | 0.026691794 |
|    average_cost         | 0.03359375  |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.882       |
|    cost_value_loss      | 0.00632     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -8.09       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0121      |
|    mean_cost_advantages | 0.018902766 |
|    mean_reward_advan... | 0.02321894  |
|    n_updates            | 1080        |
|    nu                   | 4.46        |
|    nu_loss              | -0.148      |
|    policy_gradient_loss | -0.00363    |
|    reward_explained_... | 0.902       |
|    reward_value_loss    | 0.0299      |
|    std                  | 0.663       |
|    total_cost           | 344.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 689          |
|    mean_ep_length       | 294          |
|    mean_reward          | 448          |
| infos/                  |              |
|    cost                 | 0.0206       |
|    distance_from_origin | 1.64         |
|    forward_reward       | 0.332        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.332        |
|    reward_survive       | 1            |
|    x_position           | -0.917       |
|    x_velocity           | 0.332        |
|    y_position           | 0.282        |
|    y_velocity           | 0.474        |
| rollout/                |              |
|    adjusted_reward      | 0.87         |
|    ep_len_mean          | 284          |
|    ep_rew_mean          | 287          |
| time/                   |              |
|    fps                  | 903          |
|    iterations           | 56           |
|    time_elapsed         | 634          |
|    total_timesteps      | 573440       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10190        |
|    mean_motor0          | 0.5493239    |
|    mean_motor1          | 0.537283     |
|    mean_motor2          | 0.5908519    |
|    mean_motor3          | 0.5677011    |
|    mean_motor4          | 0.619838     |
|    mean_motor5          | 0.5984525    |
|    mean_motor6          | 0.58828133   |
|    mean_motor7          | 0.6003612    |
| train/                  |              |
|    approx_kl            | 0.021503672  |
|    average_cost         | 0.0017578125 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.897        |
|    cost_value_loss      | 0.00273      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -8.03        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00446      |
|    mean_cost_advantages | -0.00255661  |
|    mean_reward_advan... | 0.026133161  |
|    n_updates            | 1100         |
|    nu                   | 4.52         |
|    nu_loss              | -0.00784     |
|    policy_gradient_loss | -0.00323     |
|    reward_explained_... | 0.905        |
|    reward_value_loss    | 0.0231       |
|    std                  | 0.658        |
|    total_cost           | 18.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 689            |
|    mean_ep_length       | 291            |
|    mean_reward          | 350            |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 2.02           |
|    forward_reward       | 0.612          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.49          |
|    reward_forward       | 0.612          |
|    reward_survive       | 1              |
|    x_position           | 1.48           |
|    x_velocity           | 0.612          |
|    y_position           | 0.428          |
|    y_velocity           | 0.476          |
| rollout/                |                |
|    adjusted_reward      | 1.14           |
|    ep_len_mean          | 313            |
|    ep_rew_mean          | 328            |
| time/                   |                |
|    fps                  | 902            |
|    iterations           | 57             |
|    time_elapsed         | 646            |
|    total_timesteps      | 583680         |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10184          |
|    mean_motor0          | 0.55654085     |
|    mean_motor1          | 0.54713213     |
|    mean_motor2          | 0.58031005     |
|    mean_motor3          | 0.5712129      |
|    mean_motor4          | 0.6111714      |
|    mean_motor5          | 0.6112764      |
|    mean_motor6          | 0.5896276      |
|    mean_motor7          | 0.5865283      |
| train/                  |                |
|    approx_kl            | 0.026135242    |
|    average_cost         | 0.0034179688   |
|    clip_fraction        | 0.03           |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.849          |
|    cost_value_loss      | 0.00311        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -7.99          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00531        |
|    mean_cost_advantages | -0.00039674575 |
|    mean_reward_advan... | 0.014149566    |
|    n_updates            | 1120           |
|    nu                   | 4.58           |
|    nu_loss              | -0.0155        |
|    policy_gradient_loss | -0.00358       |
|    reward_explained_... | 0.891          |
|    reward_value_loss    | 0.0204         |
|    std                  | 0.655          |
|    total_cost           | 35.0           |
--------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 470         |
|    mean_reward          | 551         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.53        |
|    forward_reward       | 0.617       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.54       |
|    reward_forward       | 0.617       |
|    reward_survive       | 1           |
|    x_position           | -0.0694     |
|    x_velocity           | 0.617       |
|    y_position           | 0.433       |
|    y_velocity           | 0.625       |
| rollout/                |             |
|    adjusted_reward      | 1.16        |
|    ep_len_mean          | 307         |
|    ep_rew_mean          | 323         |
| time/                   |             |
|    fps                  | 900         |
|    iterations           | 58          |
|    time_elapsed         | 659         |
|    total_timesteps      | 593920      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10201       |
|    mean_motor0          | 0.5407864   |
|    mean_motor1          | 0.5423407   |
|    mean_motor2          | 0.5654634   |
|    mean_motor3          | 0.5704823   |
|    mean_motor4          | 0.60780525  |
|    mean_motor5          | 0.6212968   |
|    mean_motor6          | 0.5819899   |
|    mean_motor7          | 0.58576024  |
| train/                  |             |
|    approx_kl            | 0.026809704 |
|    average_cost         | 0.046679687 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.902       |
|    cost_value_loss      | 0.022       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.94       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00541     |
|    mean_cost_advantages | 0.024592733 |
|    mean_reward_advan... | 0.0368356   |
|    n_updates            | 1140        |
|    nu                   | 4.64        |
|    nu_loss              | -0.214      |
|    policy_gradient_loss | -0.00394    |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.0241      |
|    std                  | 0.651       |
|    total_cost           | 478.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 59            |
|    mean_reward          | 26.4          |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 0.921         |
|    forward_reward       | 0.371         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.51         |
|    reward_forward       | 0.371         |
|    reward_survive       | 1             |
|    x_position           | -0.0535       |
|    x_velocity           | 0.371         |
|    y_position           | -0.632        |
|    y_velocity           | 0.546         |
| rollout/                |               |
|    adjusted_reward      | 1.59          |
|    ep_len_mean          | 316           |
|    ep_rew_mean          | 410           |
| time/                   |               |
|    fps                  | 900           |
|    iterations           | 59            |
|    time_elapsed         | 670           |
|    total_timesteps      | 604160        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10188         |
|    mean_motor0          | 0.540892      |
|    mean_motor1          | 0.5372078     |
|    mean_motor2          | 0.57194984    |
|    mean_motor3          | 0.5636808     |
|    mean_motor4          | 0.5962948     |
|    mean_motor5          | 0.6213646     |
|    mean_motor6          | 0.57936716    |
|    mean_motor7          | 0.59433687    |
| train/                  |               |
|    approx_kl            | 0.028211823   |
|    average_cost         | 0.0033203126  |
|    clip_fraction        | 0.0341        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.852         |
|    cost_value_loss      | 0.00618       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.9          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0254        |
|    mean_cost_advantages | -0.0022594677 |
|    mean_reward_advan... | 0.023641193   |
|    n_updates            | 1160          |
|    nu                   | 4.71          |
|    nu_loss              | -0.0154       |
|    policy_gradient_loss | -0.00355      |
|    reward_explained_... | 0.891         |
|    reward_value_loss    | 0.035         |
|    std                  | 0.649         |
|    total_cost           | 34.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 180           |
|    mean_reward          | 97.9          |
| infos/                  |               |
|    cost                 | 0.0126        |
|    distance_from_origin | 1.58          |
|    forward_reward       | 0.451         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.55         |
|    reward_forward       | 0.451         |
|    reward_survive       | 1             |
|    x_position           | -0.294        |
|    x_velocity           | 0.451         |
|    y_position           | 0.206         |
|    y_velocity           | 0.641         |
| rollout/                |               |
|    adjusted_reward      | 0.822         |
|    ep_len_mean          | 313           |
|    ep_rew_mean          | 372           |
| time/                   |               |
|    fps                  | 900           |
|    iterations           | 60            |
|    time_elapsed         | 682           |
|    total_timesteps      | 614400        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10201         |
|    mean_motor0          | 0.52746785    |
|    mean_motor1          | 0.542802      |
|    mean_motor2          | 0.5595404     |
|    mean_motor3          | 0.5722398     |
|    mean_motor4          | 0.6078127     |
|    mean_motor5          | 0.609073      |
|    mean_motor6          | 0.57434404    |
|    mean_motor7          | 0.593589      |
| train/                  |               |
|    approx_kl            | 0.0239616     |
|    average_cost         | 0.0029296875  |
|    clip_fraction        | 0.0238        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.838         |
|    cost_value_loss      | 0.00297       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.87         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0123        |
|    mean_cost_advantages | -0.0014684366 |
|    mean_reward_advan... | 0.05115487    |
|    n_updates            | 1180          |
|    nu                   | 4.76          |
|    nu_loss              | -0.0138       |
|    policy_gradient_loss | -0.0031       |
|    reward_explained_... | 0.9           |
|    reward_value_loss    | 0.0339        |
|    std                  | 0.645         |
|    total_cost           | 30.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 337           |
|    mean_reward          | 385           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.95          |
|    forward_reward       | 0.358         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.51         |
|    reward_forward       | 0.358         |
|    reward_survive       | 1             |
|    x_position           | -0.227        |
|    x_velocity           | 0.358         |
|    y_position           | -0.748        |
|    y_velocity           | 0.412         |
| rollout/                |               |
|    adjusted_reward      | 1.41          |
|    ep_len_mean          | 308           |
|    ep_rew_mean          | 397           |
| time/                   |               |
|    fps                  | 899           |
|    iterations           | 61            |
|    time_elapsed         | 694           |
|    total_timesteps      | 624640        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10183         |
|    mean_motor0          | 0.5374399     |
|    mean_motor1          | 0.5343687     |
|    mean_motor2          | 0.5485393     |
|    mean_motor3          | 0.56569725    |
|    mean_motor4          | 0.61603576    |
|    mean_motor5          | 0.6232935     |
|    mean_motor6          | 0.5751042     |
|    mean_motor7          | 0.60360765    |
| train/                  |               |
|    approx_kl            | 0.027221601   |
|    average_cost         | 0.001171875   |
|    clip_fraction        | 0.0308        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.801         |
|    cost_value_loss      | 0.00145       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.82         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00387       |
|    mean_cost_advantages | -0.0023463876 |
|    mean_reward_advan... | 0.0036238371  |
|    n_updates            | 1200          |
|    nu                   | 4.82          |
|    nu_loss              | -0.00558      |
|    policy_gradient_loss | -0.00339      |
|    reward_explained_... | 0.861         |
|    reward_value_loss    | 0.0258        |
|    std                  | 0.641         |
|    total_cost           | 12.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 689           |
|    mean_ep_length       | 291           |
|    mean_reward          | 399           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.41          |
|    forward_reward       | 0.385         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.52         |
|    reward_forward       | 0.385         |
|    reward_survive       | 1             |
|    x_position           | -0.539        |
|    x_velocity           | 0.385         |
|    y_position           | -0.153        |
|    y_velocity           | 0.338         |
| rollout/                |               |
|    adjusted_reward      | 1.24          |
|    ep_len_mean          | 304           |
|    ep_rew_mean          | 351           |
| time/                   |               |
|    fps                  | 898           |
|    iterations           | 62            |
|    time_elapsed         | 706           |
|    total_timesteps      | 634880        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10181         |
|    mean_motor0          | 0.5426921     |
|    mean_motor1          | 0.53769785    |
|    mean_motor2          | 0.54807407    |
|    mean_motor3          | 0.57375664    |
|    mean_motor4          | 0.5893253     |
|    mean_motor5          | 0.6170208     |
|    mean_motor6          | 0.57901895    |
|    mean_motor7          | 0.5873593     |
| train/                  |               |
|    approx_kl            | 0.02474487    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0297        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.851         |
|    cost_value_loss      | 0.00149       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.77         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0118        |
|    mean_cost_advantages | -0.0018057006 |
|    mean_reward_advan... | 0.032174587   |
|    n_updates            | 1220          |
|    nu                   | 4.86          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00337      |
|    reward_explained_... | 0.887         |
|    reward_value_loss    | 0.0441        |
|    std                  | 0.638         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 689         |
|    mean_ep_length       | 133         |
|    mean_reward          | 128         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.68        |
|    forward_reward       | 0.402       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.51       |
|    reward_forward       | 0.402       |
|    reward_survive       | 1           |
|    x_position           | 0.386       |
|    x_velocity           | 0.402       |
|    y_position           | 0.516       |
|    y_velocity           | 0.446       |
| rollout/                |             |
|    adjusted_reward      | 1.41        |
|    ep_len_mean          | 296         |
|    ep_rew_mean          | 413         |
| time/                   |             |
|    fps                  | 898         |
|    iterations           | 63          |
|    time_elapsed         | 717         |
|    total_timesteps      | 645120      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10192       |
|    mean_motor0          | 0.5311      |
|    mean_motor1          | 0.53923506  |
|    mean_motor2          | 0.5679083   |
|    mean_motor3          | 0.5651803   |
|    mean_motor4          | 0.59741515  |
|    mean_motor5          | 0.60914326  |
|    mean_motor6          | 0.57824147  |
|    mean_motor7          | 0.5900896   |
| train/                  |             |
|    approx_kl            | 0.026395734 |
|    average_cost         | 0.011132812 |
|    clip_fraction        | 0.0247      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.748       |
|    cost_value_loss      | 0.00391     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.75       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0158      |
|    mean_cost_advantages | 0.005651642 |
|    mean_reward_advan... | 0.023758123 |
|    n_updates            | 1240        |
|    nu                   | 4.91        |
|    nu_loss              | -0.0541     |
|    policy_gradient_loss | -0.00288    |
|    reward_explained_... | 0.886       |
|    reward_value_loss    | 0.0353      |
|    std                  | 0.637       |
|    total_cost           | 114.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 693         |
|    mean_ep_length       | 500         |
|    mean_reward          | 693         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.83        |
|    forward_reward       | 0.313       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.49       |
|    reward_forward       | 0.313       |
|    reward_survive       | 1           |
|    x_position           | 0.316       |
|    x_velocity           | 0.313       |
|    y_position           | 0.568       |
|    y_velocity           | 0.327       |
| rollout/                |             |
|    adjusted_reward      | 0.976       |
|    ep_len_mean          | 288         |
|    ep_rew_mean          | 347         |
| time/                   |             |
|    fps                  | 896         |
|    iterations           | 64          |
|    time_elapsed         | 731         |
|    total_timesteps      | 655360      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10187       |
|    mean_motor0          | 0.550897    |
|    mean_motor1          | 0.52919376  |
|    mean_motor2          | 0.571951    |
|    mean_motor3          | 0.55898654  |
|    mean_motor4          | 0.5873524   |
|    mean_motor5          | 0.60158575  |
|    mean_motor6          | 0.57498604  |
|    mean_motor7          | 0.603084    |
| train/                  |             |
|    approx_kl            | 0.028418655 |
|    average_cost         | 0.02890625  |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.783       |
|    cost_value_loss      | 0.0104      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.72       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0119      |
|    mean_cost_advantages | 0.020229483 |
|    mean_reward_advan... | 0.032561194 |
|    n_updates            | 1260        |
|    nu                   | 4.96        |
|    nu_loss              | -0.142      |
|    policy_gradient_loss | -0.00318    |
|    reward_explained_... | 0.902       |
|    reward_value_loss    | 0.0351      |
|    std                  | 0.634       |
|    total_cost           | 296.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 693          |
|    mean_ep_length       | 277          |
|    mean_reward          | 229          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.43         |
|    forward_reward       | 0.29         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.52        |
|    reward_forward       | 0.29         |
|    reward_survive       | 1            |
|    x_position           | 1.28         |
|    x_velocity           | 0.29         |
|    y_position           | -1.55        |
|    y_velocity           | 0.384        |
| rollout/                |              |
|    adjusted_reward      | 1.64         |
|    ep_len_mean          | 308          |
|    ep_rew_mean          | 402          |
| time/                   |              |
|    fps                  | 895          |
|    iterations           | 65           |
|    time_elapsed         | 743          |
|    total_timesteps      | 665600       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10193        |
|    mean_motor0          | 0.5352715    |
|    mean_motor1          | 0.5358762    |
|    mean_motor2          | 0.5646859    |
|    mean_motor3          | 0.5610118    |
|    mean_motor4          | 0.57913846   |
|    mean_motor5          | 0.61789584   |
|    mean_motor6          | 0.57599103   |
|    mean_motor7          | 0.5998307    |
| train/                  |              |
|    approx_kl            | 0.025439644  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.919        |
|    cost_value_loss      | 0.00316      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.68        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00647      |
|    mean_cost_advantages | -0.009454476 |
|    mean_reward_advan... | 0.0020194117 |
|    n_updates            | 1280         |
|    nu                   | 5.01         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00365     |
|    reward_explained_... | 0.886        |
|    reward_value_loss    | 0.0304       |
|    std                  | 0.631        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 693         |
|    mean_ep_length       | 348         |
|    mean_reward          | 552         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.08        |
|    forward_reward       | 0.349       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.47       |
|    reward_forward       | 0.349       |
|    reward_survive       | 1           |
|    x_position           | 0.0392      |
|    x_velocity           | 0.349       |
|    y_position           | 0.306       |
|    y_velocity           | 0.357       |
| rollout/                |             |
|    adjusted_reward      | 1.28        |
|    ep_len_mean          | 296         |
|    ep_rew_mean          | 387         |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 66          |
|    time_elapsed         | 755         |
|    total_timesteps      | 675840      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10198       |
|    mean_motor0          | 0.5255655   |
|    mean_motor1          | 0.53742814  |
|    mean_motor2          | 0.55435014  |
|    mean_motor3          | 0.5551645   |
|    mean_motor4          | 0.5858663   |
|    mean_motor5          | 0.6204237   |
|    mean_motor6          | 0.5700266   |
|    mean_motor7          | 0.5855004   |
| train/                  |             |
|    approx_kl            | 0.031170964 |
|    average_cost         | 0.036132812 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.927       |
|    cost_value_loss      | 0.00885     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.63       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0101      |
|    mean_cost_advantages | 0.0219957   |
|    mean_reward_advan... | 0.046189073 |
|    n_updates            | 1300        |
|    nu                   | 5.07        |
|    nu_loss              | -0.181      |
|    policy_gradient_loss | -0.0041     |
|    reward_explained_... | 0.894       |
|    reward_value_loss    | 0.0371      |
|    std                  | 0.626       |
|    total_cost           | 370.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 693          |
|    mean_ep_length       | 395          |
|    mean_reward          | 386          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.08         |
|    forward_reward       | 0.435        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.54        |
|    reward_forward       | 0.435        |
|    reward_survive       | 1            |
|    x_position           | 0.159        |
|    x_velocity           | 0.435        |
|    y_position           | 1.74         |
|    y_velocity           | 0.511        |
| rollout/                |              |
|    adjusted_reward      | 1.11         |
|    ep_len_mean          | 324          |
|    ep_rew_mean          | 436          |
| time/                   |              |
|    fps                  | 893          |
|    iterations           | 67           |
|    time_elapsed         | 768          |
|    total_timesteps      | 686080       |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10188        |
|    mean_motor0          | 0.53681403   |
|    mean_motor1          | 0.5313343    |
|    mean_motor2          | 0.55636257   |
|    mean_motor3          | 0.56208885   |
|    mean_motor4          | 0.57984746   |
|    mean_motor5          | 0.62119496   |
|    mean_motor6          | 0.573452     |
|    mean_motor7          | 0.58051354   |
| train/                  |              |
|    approx_kl            | 0.032156095  |
|    average_cost         | 0.0001953125 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.855        |
|    cost_value_loss      | 0.0054       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.57        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00397      |
|    mean_cost_advantages | -0.008702722 |
|    mean_reward_advan... | 0.0150834145 |
|    n_updates            | 1320         |
|    nu                   | 5.12         |
|    nu_loss              | -0.00099     |
|    policy_gradient_loss | -0.00328     |
|    reward_explained_... | 0.871        |
|    reward_value_loss    | 0.0419       |
|    std                  | 0.622        |
|    total_cost           | 2.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 693         |
|    mean_ep_length       | 352         |
|    mean_reward          | 396         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2           |
|    forward_reward       | 0.285       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.47       |
|    reward_forward       | 0.285       |
|    reward_survive       | 1           |
|    x_position           | 1.2         |
|    x_velocity           | 0.285       |
|    y_position           | -0.0787     |
|    y_velocity           | 0.436       |
| rollout/                |             |
|    adjusted_reward      | 1.2         |
|    ep_len_mean          | 326         |
|    ep_rew_mean          | 411         |
| time/                   |             |
|    fps                  | 890         |
|    iterations           | 68          |
|    time_elapsed         | 781         |
|    total_timesteps      | 696320      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10172       |
|    mean_motor0          | 0.5294186   |
|    mean_motor1          | 0.5155913   |
|    mean_motor2          | 0.566045    |
|    mean_motor3          | 0.57404983  |
|    mean_motor4          | 0.57064664  |
|    mean_motor5          | 0.6422038   |
|    mean_motor6          | 0.5621147   |
|    mean_motor7          | 0.5886172   |
| train/                  |             |
|    approx_kl            | 0.03392694  |
|    average_cost         | 0.0296875   |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.859       |
|    cost_value_loss      | 0.00982     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.54       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00771     |
|    mean_cost_advantages | 0.01703067  |
|    mean_reward_advan... | 0.015472906 |
|    n_updates            | 1340        |
|    nu                   | 5.18        |
|    nu_loss              | -0.152      |
|    policy_gradient_loss | -0.00377    |
|    reward_explained_... | 0.891       |
|    reward_value_loss    | 0.028       |
|    std                  | 0.62        |
|    total_cost           | 304.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 693           |
|    mean_ep_length       | 341           |
|    mean_reward          | 362           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.95          |
|    forward_reward       | 0.61          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.64         |
|    reward_forward       | 0.61          |
|    reward_survive       | 1             |
|    x_position           | -0.0939       |
|    x_velocity           | 0.61          |
|    y_position           | 0.988         |
|    y_velocity           | 0.844         |
| rollout/                |               |
|    adjusted_reward      | 1.13          |
|    ep_len_mean          | 346           |
|    ep_rew_mean          | 409           |
| time/                   |               |
|    fps                  | 889           |
|    iterations           | 69            |
|    time_elapsed         | 793           |
|    total_timesteps      | 706560        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10193         |
|    mean_motor0          | 0.5320636     |
|    mean_motor1          | 0.5190405     |
|    mean_motor2          | 0.56627893    |
|    mean_motor3          | 0.56573       |
|    mean_motor4          | 0.5739407     |
|    mean_motor5          | 0.6342048     |
|    mean_motor6          | 0.5631131     |
|    mean_motor7          | 0.5822488     |
| train/                  |               |
|    approx_kl            | 0.023589518   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0302        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.755         |
|    cost_value_loss      | 0.000842      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.49         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0051        |
|    mean_cost_advantages | -0.0012725152 |
|    mean_reward_advan... | 0.013024172   |
|    n_updates            | 1360          |
|    nu                   | 5.23          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00326      |
|    reward_explained_... | 0.906         |
|    reward_value_loss    | 0.0295        |
|    std                  | 0.616         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 373           |
|    mean_reward          | 880           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.33          |
|    forward_reward       | 0.404         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.54         |
|    reward_forward       | 0.404         |
|    reward_survive       | 1             |
|    x_position           | -0.0813       |
|    x_velocity           | 0.404         |
|    y_position           | 0.151         |
|    y_velocity           | 0.477         |
| rollout/                |               |
|    adjusted_reward      | 1.45          |
|    ep_len_mean          | 362           |
|    ep_rew_mean          | 459           |
| time/                   |               |
|    fps                  | 888           |
|    iterations           | 70            |
|    time_elapsed         | 806           |
|    total_timesteps      | 716800        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10175         |
|    mean_motor0          | 0.5217185     |
|    mean_motor1          | 0.51689506    |
|    mean_motor2          | 0.56232524    |
|    mean_motor3          | 0.56788486    |
|    mean_motor4          | 0.56745666    |
|    mean_motor5          | 0.64195764    |
|    mean_motor6          | 0.54607546    |
|    mean_motor7          | 0.5755063     |
| train/                  |               |
|    approx_kl            | 0.024830686   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0402        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.908         |
|    cost_value_loss      | 0.00201       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.44         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00777       |
|    mean_cost_advantages | -0.0052610636 |
|    mean_reward_advan... | 0.0109278     |
|    n_updates            | 1380          |
|    nu                   | 5.28          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00344      |
|    reward_explained_... | 0.859         |
|    reward_value_loss    | 0.0293        |
|    std                  | 0.612         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 880         |
|    mean_ep_length       | 290         |
|    mean_reward          | 479         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 1.56        |
|    forward_reward       | 0.409       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.45       |
|    reward_forward       | 0.409       |
|    reward_survive       | 1           |
|    x_position           | 0.613       |
|    x_velocity           | 0.409       |
|    y_position           | 0.481       |
|    y_velocity           | 0.377       |
| rollout/                |             |
|    adjusted_reward      | 1.45        |
|    ep_len_mean          | 356         |
|    ep_rew_mean          | 475         |
| time/                   |             |
|    fps                  | 888         |
|    iterations           | 71          |
|    time_elapsed         | 818         |
|    total_timesteps      | 727040      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10204       |
|    mean_motor0          | 0.5236441   |
|    mean_motor1          | 0.5189578   |
|    mean_motor2          | 0.5748313   |
|    mean_motor3          | 0.568853    |
|    mean_motor4          | 0.5681658   |
|    mean_motor5          | 0.64017946  |
|    mean_motor6          | 0.5496156   |
|    mean_motor7          | 0.60120124  |
| train/                  |             |
|    approx_kl            | 0.031651545 |
|    average_cost         | 0.039257813 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.931       |
|    cost_value_loss      | 0.013       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -7.4        |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0347      |
|    mean_cost_advantages | 0.022441352 |
|    mean_reward_advan... | 0.02794071  |
|    n_updates            | 1400        |
|    nu                   | 5.34        |
|    nu_loss              | -0.207      |
|    policy_gradient_loss | -0.00381    |
|    reward_explained_... | 0.89        |
|    reward_value_loss    | 0.0354      |
|    std                  | 0.61        |
|    total_cost           | 402.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 294          |
|    mean_reward          | 282          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.5          |
|    forward_reward       | 0.691        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.39        |
|    reward_forward       | 0.691        |
|    reward_survive       | 1            |
|    x_position           | 0.672        |
|    x_velocity           | 0.691        |
|    y_position           | 0.0945       |
|    y_velocity           | 0.474        |
| rollout/                |              |
|    adjusted_reward      | 1.4          |
|    ep_len_mean          | 367          |
|    ep_rew_mean          | 516          |
| time/                   |              |
|    fps                  | 887          |
|    iterations           | 72           |
|    time_elapsed         | 830          |
|    total_timesteps      | 737280       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10178        |
|    mean_motor0          | 0.50601965   |
|    mean_motor1          | 0.51434886   |
|    mean_motor2          | 0.5556135    |
|    mean_motor3          | 0.5665051    |
|    mean_motor4          | 0.5596757    |
|    mean_motor5          | 0.6334218    |
|    mean_motor6          | 0.55291474   |
|    mean_motor7          | 0.5916384    |
| train/                  |              |
|    approx_kl            | 0.024727369  |
|    average_cost         | 0.0109375    |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.953        |
|    cost_value_loss      | 0.00601      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.37        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0168       |
|    mean_cost_advantages | -0.001964867 |
|    mean_reward_advan... | 0.024830094  |
|    n_updates            | 1420         |
|    nu                   | 5.4          |
|    nu_loss              | -0.0584      |
|    policy_gradient_loss | -0.00329     |
|    reward_explained_... | 0.888        |
|    reward_value_loss    | 0.0335       |
|    std                  | 0.607        |
|    total_cost           | 112.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 406           |
|    mean_reward          | 448           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.53          |
|    forward_reward       | 0.408         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.51         |
|    reward_forward       | 0.408         |
|    reward_survive       | 1             |
|    x_position           | 1.02          |
|    x_velocity           | 0.408         |
|    y_position           | 0.261         |
|    y_velocity           | 0.414         |
| rollout/                |               |
|    adjusted_reward      | 1.38          |
|    ep_len_mean          | 353           |
|    ep_rew_mean          | 505           |
| time/                   |               |
|    fps                  | 886           |
|    iterations           | 73            |
|    time_elapsed         | 843           |
|    total_timesteps      | 747520        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10176         |
|    mean_motor0          | 0.5148048     |
|    mean_motor1          | 0.518529      |
|    mean_motor2          | 0.5540782     |
|    mean_motor3          | 0.5694417     |
|    mean_motor4          | 0.56573504    |
|    mean_motor5          | 0.63860536    |
|    mean_motor6          | 0.5407405     |
|    mean_motor7          | 0.56774616    |
| train/                  |               |
|    approx_kl            | 0.021062326   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0255        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.767         |
|    cost_value_loss      | 0.000498      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.32         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0159        |
|    mean_cost_advantages | -0.0011498265 |
|    mean_reward_advan... | 0.021017341   |
|    n_updates            | 1440          |
|    nu                   | 5.45          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00267      |
|    reward_explained_... | 0.899         |
|    reward_value_loss    | 0.0344        |
|    std                  | 0.603         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 458          |
|    mean_reward          | 602          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 3.21         |
|    forward_reward       | 0.298        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.47        |
|    reward_forward       | 0.298        |
|    reward_survive       | 1            |
|    x_position           | 1.23         |
|    x_velocity           | 0.298        |
|    y_position           | 0.952        |
|    y_velocity           | 0.37         |
| rollout/                |              |
|    adjusted_reward      | 1.29         |
|    ep_len_mean          | 352          |
|    ep_rew_mean          | 481          |
| time/                   |              |
|    fps                  | 884          |
|    iterations           | 74           |
|    time_elapsed         | 856          |
|    total_timesteps      | 757760       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10182        |
|    mean_motor0          | 0.500066     |
|    mean_motor1          | 0.5318408    |
|    mean_motor2          | 0.55937886   |
|    mean_motor3          | 0.5877672    |
|    mean_motor4          | 0.5616205    |
|    mean_motor5          | 0.6421824    |
|    mean_motor6          | 0.5377084    |
|    mean_motor7          | 0.57270277   |
| train/                  |              |
|    approx_kl            | 0.031783443  |
|    average_cost         | 0.020214844  |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.886        |
|    cost_value_loss      | 0.00643      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.28        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00944      |
|    mean_cost_advantages | 0.0078053996 |
|    mean_reward_advan... | 0.016156603  |
|    n_updates            | 1460         |
|    nu                   | 5.5          |
|    nu_loss              | -0.11        |
|    policy_gradient_loss | -0.00379     |
|    reward_explained_... | 0.887        |
|    reward_value_loss    | 0.0424       |
|    std                  | 0.601        |
|    total_cost           | 207.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 315           |
|    mean_reward          | 475           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.99          |
|    forward_reward       | 0.694         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.59         |
|    reward_forward       | 0.694         |
|    reward_survive       | 1             |
|    x_position           | 1.25          |
|    x_velocity           | 0.694         |
|    y_position           | 0.339         |
|    y_velocity           | 0.608         |
| rollout/                |               |
|    adjusted_reward      | 1.63          |
|    ep_len_mean          | 332           |
|    ep_rew_mean          | 465           |
| time/                   |               |
|    fps                  | 884           |
|    iterations           | 75            |
|    time_elapsed         | 868           |
|    total_timesteps      | 768000        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10176         |
|    mean_motor0          | 0.49654       |
|    mean_motor1          | 0.52619034    |
|    mean_motor2          | 0.54829633    |
|    mean_motor3          | 0.5822595     |
|    mean_motor4          | 0.56003517    |
|    mean_motor5          | 0.63672006    |
|    mean_motor6          | 0.5478714     |
|    mean_motor7          | 0.5985591     |
| train/                  |               |
|    approx_kl            | 0.022998732   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0274        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.799         |
|    cost_value_loss      | 0.000824      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.25         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00909       |
|    mean_cost_advantages | -0.0016403493 |
|    mean_reward_advan... | 0.018834636   |
|    n_updates            | 1480          |
|    nu                   | 5.55          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00276      |
|    reward_explained_... | 0.91          |
|    reward_value_loss    | 0.0324        |
|    std                  | 0.597         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 342           |
|    mean_reward          | 687           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 2.75          |
|    forward_reward       | 0.376         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.49         |
|    reward_forward       | 0.376         |
|    reward_survive       | 1             |
|    x_position           | 1.25          |
|    x_velocity           | 0.376         |
|    y_position           | 1.17          |
|    y_velocity           | 0.421         |
| rollout/                |               |
|    adjusted_reward      | 1.58          |
|    ep_len_mean          | 347           |
|    ep_rew_mean          | 509           |
| time/                   |               |
|    fps                  | 883           |
|    iterations           | 76            |
|    time_elapsed         | 881           |
|    total_timesteps      | 778240        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10166         |
|    mean_motor0          | 0.49474153    |
|    mean_motor1          | 0.52319777    |
|    mean_motor2          | 0.5429448     |
|    mean_motor3          | 0.5799869     |
|    mean_motor4          | 0.5527554     |
|    mean_motor5          | 0.61965203    |
|    mean_motor6          | 0.53392094    |
|    mean_motor7          | 0.58645666    |
| train/                  |               |
|    approx_kl            | 0.0272079     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0372        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.898         |
|    cost_value_loss      | 0.000843      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.19         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0138        |
|    mean_cost_advantages | -0.0028808252 |
|    mean_reward_advan... | 0.025584972   |
|    n_updates            | 1500          |
|    nu                   | 5.6           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00323      |
|    reward_explained_... | 0.897         |
|    reward_value_loss    | 0.0494        |
|    std                  | 0.593         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 144          |
|    mean_reward          | 179          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.72         |
|    forward_reward       | 0.526        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.38        |
|    reward_forward       | 0.526        |
|    reward_survive       | 1            |
|    x_position           | 0.823        |
|    x_velocity           | 0.526        |
|    y_position           | 0.253        |
|    y_velocity           | 0.387        |
| rollout/                |              |
|    adjusted_reward      | 1.67         |
|    ep_len_mean          | 352          |
|    ep_rew_mean          | 563          |
| time/                   |              |
|    fps                  | 883          |
|    iterations           | 77           |
|    time_elapsed         | 892          |
|    total_timesteps      | 788480       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10195        |
|    mean_motor0          | 0.4886478    |
|    mean_motor1          | 0.51424474   |
|    mean_motor2          | 0.53496104   |
|    mean_motor3          | 0.6031338    |
|    mean_motor4          | 0.5476779    |
|    mean_motor5          | 0.63732725   |
|    mean_motor6          | 0.5477861    |
|    mean_motor7          | 0.59907997   |
| train/                  |              |
|    approx_kl            | 0.028696293  |
|    average_cost         | 0.0018554687 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.837        |
|    cost_value_loss      | 0.0016       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.15        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0115       |
|    mean_cost_advantages | -0.000937159 |
|    mean_reward_advan... | 0.027070105  |
|    n_updates            | 1520         |
|    nu                   | 5.64         |
|    nu_loss              | -0.0104      |
|    policy_gradient_loss | -0.00276     |
|    reward_explained_... | 0.916        |
|    reward_value_loss    | 0.0393       |
|    std                  | 0.59         |
|    total_cost           | 19.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 425          |
|    mean_reward          | 721          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.45         |
|    forward_reward       | 0.472        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.55        |
|    reward_forward       | 0.472        |
|    reward_survive       | 1            |
|    x_position           | 1.36         |
|    x_velocity           | 0.472        |
|    y_position           | 1.63         |
|    y_velocity           | 0.517        |
| rollout/                |              |
|    adjusted_reward      | 1.62         |
|    ep_len_mean          | 372          |
|    ep_rew_mean          | 593          |
| time/                   |              |
|    fps                  | 882          |
|    iterations           | 78           |
|    time_elapsed         | 905          |
|    total_timesteps      | 798720       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10167        |
|    mean_motor0          | 0.4947763    |
|    mean_motor1          | 0.51968455   |
|    mean_motor2          | 0.54847133   |
|    mean_motor3          | 0.592366     |
|    mean_motor4          | 0.5463456    |
|    mean_motor5          | 0.6063909    |
|    mean_motor6          | 0.5394505    |
|    mean_motor7          | 0.5784351    |
| train/                  |              |
|    approx_kl            | 0.030598706  |
|    average_cost         | 0.007519531  |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.575        |
|    cost_value_loss      | 0.00251      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -7.09        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0155       |
|    mean_cost_advantages | 0.0047453423 |
|    mean_reward_advan... | 0.028913641  |
|    n_updates            | 1540         |
|    nu                   | 5.68         |
|    nu_loss              | -0.0424      |
|    policy_gradient_loss | -0.00383     |
|    reward_explained_... | 0.892        |
|    reward_value_loss    | 0.0428       |
|    std                  | 0.586        |
|    total_cost           | 77.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 338           |
|    mean_reward          | 668           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 1.83          |
|    forward_reward       | 0.365         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.55         |
|    reward_forward       | 0.365         |
|    reward_survive       | 1             |
|    x_position           | 1.11          |
|    x_velocity           | 0.365         |
|    y_position           | 0.157         |
|    y_velocity           | 0.677         |
| rollout/                |               |
|    adjusted_reward      | 2.08          |
|    ep_len_mean          | 397           |
|    ep_rew_mean          | 689           |
| time/                   |               |
|    fps                  | 881           |
|    iterations           | 79            |
|    time_elapsed         | 917           |
|    total_timesteps      | 808960        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10159         |
|    mean_motor0          | 0.4860808     |
|    mean_motor1          | 0.52569216    |
|    mean_motor2          | 0.53841305    |
|    mean_motor3          | 0.6396929     |
|    mean_motor4          | 0.5319493     |
|    mean_motor5          | 0.5923469     |
|    mean_motor6          | 0.54092896    |
|    mean_motor7          | 0.581504      |
| train/                  |               |
|    approx_kl            | 0.029251164   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0372        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.876         |
|    cost_value_loss      | 0.00117       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -7.04         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00718       |
|    mean_cost_advantages | -0.0042605656 |
|    mean_reward_advan... | 0.023092866   |
|    n_updates            | 1560          |
|    nu                   | 5.72          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00291      |
|    reward_explained_... | 0.906         |
|    reward_value_loss    | 0.0403        |
|    std                  | 0.583         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 430          |
|    mean_reward          | 633          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.17         |
|    forward_reward       | 0.464        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.4         |
|    reward_forward       | 0.464        |
|    reward_survive       | 1            |
|    x_position           | 3.68         |
|    x_velocity           | 0.464        |
|    y_position           | -0.119       |
|    y_velocity           | 0.315        |
| rollout/                |              |
|    adjusted_reward      | 2.01         |
|    ep_len_mean          | 389          |
|    ep_rew_mean          | 688          |
| time/                   |              |
|    fps                  | 880          |
|    iterations           | 80           |
|    time_elapsed         | 930          |
|    total_timesteps      | 819200       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10165        |
|    mean_motor0          | 0.48332554   |
|    mean_motor1          | 0.52575105   |
|    mean_motor2          | 0.5326414    |
|    mean_motor3          | 0.58628595   |
|    mean_motor4          | 0.536046     |
|    mean_motor5          | 0.57122654   |
|    mean_motor6          | 0.54447085   |
|    mean_motor7          | 0.5602795    |
| train/                  |              |
|    approx_kl            | 0.033062592  |
|    average_cost         | 0.0071289064 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.839        |
|    cost_value_loss      | 0.00464      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.99        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00448      |
|    mean_cost_advantages | 0.003546968  |
|    mean_reward_advan... | 0.0492672    |
|    n_updates            | 1580         |
|    nu                   | 5.75         |
|    nu_loss              | -0.0407      |
|    policy_gradient_loss | -0.00331     |
|    reward_explained_... | 0.909        |
|    reward_value_loss    | 0.0492       |
|    std                  | 0.579        |
|    total_cost           | 73.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 330          |
|    mean_reward          | 875          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.27         |
|    forward_reward       | 0.524        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.5         |
|    reward_forward       | 0.524        |
|    reward_survive       | 1            |
|    x_position           | 1.46         |
|    x_velocity           | 0.524        |
|    y_position           | 0.882        |
|    y_velocity           | 0.418        |
| rollout/                |              |
|    adjusted_reward      | 2.57         |
|    ep_len_mean          | 388          |
|    ep_rew_mean          | 813          |
| time/                   |              |
|    fps                  | 879          |
|    iterations           | 81           |
|    time_elapsed         | 942          |
|    total_timesteps      | 829440       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10189        |
|    mean_motor0          | 0.4902842    |
|    mean_motor1          | 0.5308041    |
|    mean_motor2          | 0.5428249    |
|    mean_motor3          | 0.62373734   |
|    mean_motor4          | 0.52246225   |
|    mean_motor5          | 0.57637227   |
|    mean_motor6          | 0.53780043   |
|    mean_motor7          | 0.56123704   |
| train/                  |              |
|    approx_kl            | 0.024739036  |
|    average_cost         | 0.014160156  |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.917        |
|    cost_value_loss      | 0.00758      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.95        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0254       |
|    mean_cost_advantages | 0.0056837304 |
|    mean_reward_advan... | 0.04780274   |
|    n_updates            | 1600         |
|    nu                   | 5.79         |
|    nu_loss              | -0.0815      |
|    policy_gradient_loss | -0.00319     |
|    reward_explained_... | 0.92         |
|    reward_value_loss    | 0.0515       |
|    std                  | 0.576        |
|    total_cost           | 145.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 880          |
|    mean_ep_length       | 180          |
|    mean_reward          | 251          |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.68         |
|    forward_reward       | 0.436        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.49        |
|    reward_forward       | 0.436        |
|    reward_survive       | 1            |
|    x_position           | 1.12         |
|    x_velocity           | 0.436        |
|    y_position           | 0.634        |
|    y_velocity           | 0.464        |
| rollout/                |              |
|    adjusted_reward      | 1.37         |
|    ep_len_mean          | 385          |
|    ep_rew_mean          | 765          |
| time/                   |              |
|    fps                  | 879          |
|    iterations           | 82           |
|    time_elapsed         | 954          |
|    total_timesteps      | 839680       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10177        |
|    mean_motor0          | 0.48517266   |
|    mean_motor1          | 0.5048869    |
|    mean_motor2          | 0.52193606   |
|    mean_motor3          | 0.6247854    |
|    mean_motor4          | 0.53649855   |
|    mean_motor5          | 0.5813684    |
|    mean_motor6          | 0.5256604    |
|    mean_motor7          | 0.55115706   |
| train/                  |              |
|    approx_kl            | 0.028175676  |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.765        |
|    cost_value_loss      | 0.0032       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.91        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00812      |
|    mean_cost_advantages | 0.0004026845 |
|    mean_reward_advan... | 0.048990346  |
|    n_updates            | 1620         |
|    nu                   | 5.83         |
|    nu_loss              | -0.0209      |
|    policy_gradient_loss | -0.00288     |
|    reward_explained_... | 0.891        |
|    reward_value_loss    | 0.0746       |
|    std                  | 0.573        |
|    total_cost           | 37.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 880           |
|    mean_ep_length       | 361           |
|    mean_reward          | 658           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.43          |
|    forward_reward       | 0.371         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.43         |
|    reward_forward       | 0.371         |
|    reward_survive       | 1             |
|    x_position           | 1.25          |
|    x_velocity           | 0.371         |
|    y_position           | 1.03          |
|    y_velocity           | 0.518         |
| rollout/                |               |
|    adjusted_reward      | 2.18          |
|    ep_len_mean          | 366           |
|    ep_rew_mean          | 750           |
| time/                   |               |
|    fps                  | 879           |
|    iterations           | 83            |
|    time_elapsed         | 966           |
|    total_timesteps      | 849920        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10186         |
|    mean_motor0          | 0.49073586    |
|    mean_motor1          | 0.52368116    |
|    mean_motor2          | 0.5528944     |
|    mean_motor3          | 0.5983263     |
|    mean_motor4          | 0.53958684    |
|    mean_motor5          | 0.58168334    |
|    mean_motor6          | 0.52913976    |
|    mean_motor7          | 0.5593971     |
| train/                  |               |
|    approx_kl            | 0.026401717   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0435        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.708         |
|    cost_value_loss      | 0.000196      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.86         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0347        |
|    mean_cost_advantages | -0.0019665605 |
|    mean_reward_advan... | 0.009370122   |
|    n_updates            | 1640          |
|    nu                   | 5.86          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00292      |
|    reward_explained_... | 0.917         |
|    reward_value_loss    | 0.0257        |
|    std                  | 0.569         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 880         |
|    mean_ep_length       | 365         |
|    mean_reward          | 834         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.75        |
|    forward_reward       | 0.514       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.57       |
|    reward_forward       | 0.514       |
|    reward_survive       | 1           |
|    x_position           | 1.1         |
|    x_velocity           | 0.514       |
|    y_position           | 0.689       |
|    y_velocity           | 0.497       |
| rollout/                |             |
|    adjusted_reward      | 1.95        |
|    ep_len_mean          | 381         |
|    ep_rew_mean          | 739         |
| time/                   |             |
|    fps                  | 878         |
|    iterations           | 84          |
|    time_elapsed         | 979         |
|    total_timesteps      | 860160      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10171       |
|    mean_motor0          | 0.49351937  |
|    mean_motor1          | 0.49547276  |
|    mean_motor2          | 0.5462118   |
|    mean_motor3          | 0.58704937  |
|    mean_motor4          | 0.5349224   |
|    mean_motor5          | 0.58860195  |
|    mean_motor6          | 0.5324787   |
|    mean_motor7          | 0.5769926   |
| train/                  |             |
|    approx_kl            | 0.043716967 |
|    average_cost         | 0.071777344 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.864       |
|    cost_value_loss      | 0.0188      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.81       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.018       |
|    mean_cost_advantages | 0.057207454 |
|    mean_reward_advan... | 0.03872797  |
|    n_updates            | 1660        |
|    nu                   | 5.92        |
|    nu_loss              | -0.421      |
|    policy_gradient_loss | -0.00517    |
|    reward_explained_... | 0.901       |
|    reward_value_loss    | 0.0643      |
|    std                  | 0.566       |
|    total_cost           | 735.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.17e+03     |
|    mean_ep_length       | 483          |
|    mean_reward          | 1.17e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 2.5          |
|    forward_reward       | 0.349        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.349        |
|    reward_survive       | 1            |
|    x_position           | 1.56         |
|    x_velocity           | 0.349        |
|    y_position           | 1.75         |
|    y_velocity           | 0.35         |
| rollout/                |              |
|    adjusted_reward      | 2.16         |
|    ep_len_mean          | 390          |
|    ep_rew_mean          | 748          |
| time/                   |              |
|    fps                  | 877          |
|    iterations           | 85           |
|    time_elapsed         | 992          |
|    total_timesteps      | 870400       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10176        |
|    mean_motor0          | 0.5031494    |
|    mean_motor1          | 0.50825197   |
|    mean_motor2          | 0.54118454   |
|    mean_motor3          | 0.5922018    |
|    mean_motor4          | 0.53178394   |
|    mean_motor5          | 0.5701557    |
|    mean_motor6          | 0.53271496   |
|    mean_motor7          | 0.6014983    |
| train/                  |              |
|    approx_kl            | 0.019267809  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.873        |
|    cost_value_loss      | 0.000917     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.78        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0101       |
|    mean_cost_advantages | -0.001837049 |
|    mean_reward_advan... | 0.018376794  |
|    n_updates            | 1680         |
|    nu                   | 5.97         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00265     |
|    reward_explained_... | 0.924        |
|    reward_value_loss    | 0.052        |
|    std                  | 0.564        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.17e+03    |
|    mean_ep_length       | 329         |
|    mean_reward          | 866         |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 3.73        |
|    forward_reward       | 0.552       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.4        |
|    reward_forward       | 0.552       |
|    reward_survive       | 1           |
|    x_position           | 2.44        |
|    x_velocity           | 0.552       |
|    y_position           | 2.58        |
|    y_velocity           | 0.397       |
| rollout/                |             |
|    adjusted_reward      | 1.97        |
|    ep_len_mean          | 372         |
|    ep_rew_mean          | 762         |
| time/                   |             |
|    fps                  | 876         |
|    iterations           | 86          |
|    time_elapsed         | 1004        |
|    total_timesteps      | 880640      |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10174       |
|    mean_motor0          | 0.49702415  |
|    mean_motor1          | 0.4876469   |
|    mean_motor2          | 0.53704655  |
|    mean_motor3          | 0.59384596  |
|    mean_motor4          | 0.5373887   |
|    mean_motor5          | 0.57953966  |
|    mean_motor6          | 0.52104664  |
|    mean_motor7          | 0.58014154  |
| train/                  |             |
|    approx_kl            | 0.038051683 |
|    average_cost         | 0.014941406 |
|    clip_fraction        | 0.067       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.883       |
|    cost_value_loss      | 0.00831     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.74       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0478      |
|    mean_cost_advantages | 0.010182167 |
|    mean_reward_advan... | 0.023032805 |
|    n_updates            | 1700        |
|    nu                   | 6.02        |
|    nu_loss              | -0.0892     |
|    policy_gradient_loss | -0.00389    |
|    reward_explained_... | 0.917       |
|    reward_value_loss    | 0.0589      |
|    std                  | 0.562       |
|    total_cost           | 153.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1.17e+03      |
|    mean_ep_length       | 354           |
|    mean_reward          | 941           |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.64          |
|    forward_reward       | 0.375         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.375         |
|    reward_survive       | 1             |
|    x_position           | 1.4           |
|    x_velocity           | 0.375         |
|    y_position           | 3.04          |
|    y_velocity           | 0.281         |
| rollout/                |               |
|    adjusted_reward      | 2.62          |
|    ep_len_mean          | 406           |
|    ep_rew_mean          | 862           |
| time/                   |               |
|    fps                  | 875           |
|    iterations           | 87            |
|    time_elapsed         | 1017          |
|    total_timesteps      | 890880        |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10162         |
|    mean_motor0          | 0.4779927     |
|    mean_motor1          | 0.51804525    |
|    mean_motor2          | 0.52765214    |
|    mean_motor3          | 0.59117705    |
|    mean_motor4          | 0.53659034    |
|    mean_motor5          | 0.5638792     |
|    mean_motor6          | 0.5269059     |
|    mean_motor7          | 0.5795922     |
| train/                  |               |
|    approx_kl            | 0.029202545   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0358        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.735         |
|    cost_value_loss      | 0.000778      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.7          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0205        |
|    mean_cost_advantages | -0.0014760237 |
|    mean_reward_advan... | 0.010399585   |
|    n_updates            | 1720          |
|    nu                   | 6.07          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0026       |
|    reward_explained_... | 0.902         |
|    reward_value_loss    | 0.0498        |
|    std                  | 0.558         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.56e+03    |
|    mean_ep_length       | 467         |
|    mean_reward          | 1.56e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 4.59        |
|    forward_reward       | 0.45        |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.45       |
|    reward_forward       | 0.45        |
|    reward_survive       | 1           |
|    x_position           | 3.39        |
|    x_velocity           | 0.45        |
|    y_position           | 2.72        |
|    y_velocity           | 0.399       |
| rollout/                |             |
|    adjusted_reward      | 2.92        |
|    ep_len_mean          | 424         |
|    ep_rew_mean          | 991         |
| time/                   |             |
|    fps                  | 874         |
|    iterations           | 88          |
|    time_elapsed         | 1030        |
|    total_timesteps      | 901120      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10165       |
|    mean_motor0          | 0.48872814  |
|    mean_motor1          | 0.5385918   |
|    mean_motor2          | 0.5374651   |
|    mean_motor3          | 0.59812176  |
|    mean_motor4          | 0.543619    |
|    mean_motor5          | 0.5617689   |
|    mean_motor6          | 0.5360893   |
|    mean_motor7          | 0.6106301   |
| train/                  |             |
|    approx_kl            | 0.034772463 |
|    average_cost         | 0.017285157 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.916       |
|    cost_value_loss      | 0.0103      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.67       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0161      |
|    mean_cost_advantages | 0.010573174 |
|    mean_reward_advan... | 0.04829352  |
|    n_updates            | 1740        |
|    nu                   | 6.12        |
|    nu_loss              | -0.105      |
|    policy_gradient_loss | -0.00315    |
|    reward_explained_... | 0.906       |
|    reward_value_loss    | 0.0552      |
|    std                  | 0.557       |
|    total_cost           | 177.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 1.56e+03      |
|    mean_ep_length       | 434           |
|    mean_reward          | 1.27e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.1           |
|    forward_reward       | 0.382         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.53         |
|    reward_forward       | 0.382         |
|    reward_survive       | 1             |
|    x_position           | 2.25          |
|    x_velocity           | 0.382         |
|    y_position           | 3.02          |
|    y_velocity           | 0.403         |
| rollout/                |               |
|    adjusted_reward      | 2.71          |
|    ep_len_mean          | 407           |
|    ep_rew_mean          | 1.02e+03      |
| time/                   |               |
|    fps                  | 873           |
|    iterations           | 89            |
|    time_elapsed         | 1043          |
|    total_timesteps      | 911360        |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10159         |
|    mean_motor0          | 0.4860615     |
|    mean_motor1          | 0.5243406     |
|    mean_motor2          | 0.52817786    |
|    mean_motor3          | 0.589866      |
|    mean_motor4          | 0.5545515     |
|    mean_motor5          | 0.57120264    |
|    mean_motor6          | 0.532608      |
|    mean_motor7          | 0.58913386    |
| train/                  |               |
|    approx_kl            | 0.024874886   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0346        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.893         |
|    cost_value_loss      | 0.00102       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.65         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0458        |
|    mean_cost_advantages | -0.0029712988 |
|    mean_reward_advan... | 0.04631941    |
|    n_updates            | 1760          |
|    nu                   | 6.17          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00247      |
|    reward_explained_... | 0.919         |
|    reward_value_loss    | 0.0728        |
|    std                  | 0.555         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.56e+03    |
|    mean_ep_length       | 418         |
|    mean_reward          | 1.05e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.04        |
|    forward_reward       | 0.396       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.37       |
|    reward_forward       | 0.396       |
|    reward_survive       | 1           |
|    x_position           | 1.88        |
|    x_velocity           | 0.396       |
|    y_position           | 0.378       |
|    y_velocity           | 0.318       |
| rollout/                |             |
|    adjusted_reward      | 2.64        |
|    ep_len_mean          | 427         |
|    ep_rew_mean          | 1.15e+03    |
| time/                   |             |
|    fps                  | 872         |
|    iterations           | 90          |
|    time_elapsed         | 1056        |
|    total_timesteps      | 921600      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10171       |
|    mean_motor0          | 0.4989903   |
|    mean_motor1          | 0.5330292   |
|    mean_motor2          | 0.5458293   |
|    mean_motor3          | 0.5833906   |
|    mean_motor4          | 0.5589877   |
|    mean_motor5          | 0.5568614   |
|    mean_motor6          | 0.5465252   |
|    mean_motor7          | 0.6391336   |
| train/                  |             |
|    approx_kl            | 0.05046467  |
|    average_cost         | 0.043164063 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.916       |
|    cost_value_loss      | 0.00601     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.61       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0411      |
|    mean_cost_advantages | 0.03220572  |
|    mean_reward_advan... | 0.026677877 |
|    n_updates            | 1780        |
|    nu                   | 6.22        |
|    nu_loss              | -0.266      |
|    policy_gradient_loss | -0.0046     |
|    reward_explained_... | 0.915       |
|    reward_value_loss    | 0.0809      |
|    std                  | 0.552       |
|    total_cost           | 442.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.56e+03     |
|    mean_ep_length       | 421          |
|    mean_reward          | 1.54e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 1.49         |
|    forward_reward       | 0.294        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.46        |
|    reward_forward       | 0.294        |
|    reward_survive       | 1            |
|    x_position           | 0.0811       |
|    x_velocity           | 0.294        |
|    y_position           | 1.14         |
|    y_velocity           | 0.363        |
| rollout/                |              |
|    adjusted_reward      | 3.4          |
|    ep_len_mean          | 420          |
|    ep_rew_mean          | 1.24e+03     |
| time/                   |              |
|    fps                  | 871          |
|    iterations           | 91           |
|    time_elapsed         | 1068         |
|    total_timesteps      | 931840       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10183        |
|    mean_motor0          | 0.48801225   |
|    mean_motor1          | 0.54665565   |
|    mean_motor2          | 0.5473565    |
|    mean_motor3          | 0.60192513   |
|    mean_motor4          | 0.555605     |
|    mean_motor5          | 0.5560065    |
|    mean_motor6          | 0.5437795    |
|    mean_motor7          | 0.62337613   |
| train/                  |              |
|    approx_kl            | 0.03552308   |
|    average_cost         | 0.021875     |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.956        |
|    cost_value_loss      | 0.00488      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.59        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0286       |
|    mean_cost_advantages | 0.0100607425 |
|    mean_reward_advan... | 0.017660214  |
|    n_updates            | 1800         |
|    nu                   | 6.29         |
|    nu_loss              | -0.136       |
|    policy_gradient_loss | -0.00335     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 0.0723       |
|    std                  | 0.552        |
|    total_cost           | 224.0        |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 1.58e+03    |
|    mean_ep_length       | 440         |
|    mean_reward          | 1.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 3.72        |
|    forward_reward       | 0.432       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.35       |
|    reward_forward       | 0.432       |
|    reward_survive       | 1           |
|    x_position           | 3.09        |
|    x_velocity           | 0.432       |
|    y_position           | 1.29        |
|    y_velocity           | 0.575       |
| rollout/                |             |
|    adjusted_reward      | 3.2         |
|    ep_len_mean          | 419         |
|    ep_rew_mean          | 1.24e+03    |
| time/                   |             |
|    fps                  | 870         |
|    iterations           | 92          |
|    time_elapsed         | 1081        |
|    total_timesteps      | 942080      |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10191       |
|    mean_motor0          | 0.48528785  |
|    mean_motor1          | 0.59550273  |
|    mean_motor2          | 0.53779334  |
|    mean_motor3          | 0.64285207  |
|    mean_motor4          | 0.5676338   |
|    mean_motor5          | 0.5454084   |
|    mean_motor6          | 0.5594828   |
|    mean_motor7          | 0.6227164   |
| train/                  |             |
|    approx_kl            | 0.038174886 |
|    average_cost         | 0.038085938 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.956       |
|    cost_value_loss      | 0.0259      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.56       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0542      |
|    mean_cost_advantages | 0.019273182 |
|    mean_reward_advan... | 0.051258527 |
|    n_updates            | 1820        |
|    nu                   | 6.36        |
|    nu_loss              | -0.239      |
|    policy_gradient_loss | -0.00373    |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.0905      |
|    std                  | 0.549       |
|    total_cost           | 390.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 1.72e+03     |
|    mean_ep_length       | 422          |
|    mean_reward          | 1.72e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 3.53         |
|    forward_reward       | 0.326        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.59        |
|    reward_forward       | 0.326        |
|    reward_survive       | 1            |
|    x_position           | 1.92         |
|    x_velocity           | 0.326        |
|    y_position           | 2.52         |
|    y_velocity           | 0.394        |
| rollout/                |              |
|    adjusted_reward      | 3.54         |
|    ep_len_mean          | 442          |
|    ep_rew_mean          | 1.4e+03      |
| time/                   |              |
|    fps                  | 870          |
|    iterations           | 93           |
|    time_elapsed         | 1094         |
|    total_timesteps      | 952320       |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10174        |
|    mean_motor0          | 0.494097     |
|    mean_motor1          | 0.5476972    |
|    mean_motor2          | 0.53161246   |
|    mean_motor3          | 0.5875874    |
|    mean_motor4          | 0.5493134    |
|    mean_motor5          | 0.5656057    |
|    mean_motor6          | 0.5483867    |
|    mean_motor7          | 0.6213839    |
| train/                  |              |
|    approx_kl            | 0.028020278  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.722        |
|    cost_value_loss      | 0.00038      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.53        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0862       |
|    mean_cost_advantages | -0.002450176 |
|    mean_reward_advan... | 0.030847276  |
|    n_updates            | 1840         |
|    nu                   | 6.42         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00254     |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 0.0868       |
|    std                  | 0.548        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 1.83e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 1.83e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.68           |
|    forward_reward       | 0.396          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.43          |
|    reward_forward       | 0.396          |
|    reward_survive       | 1              |
|    x_position           | 4.71           |
|    x_velocity           | 0.396          |
|    y_position           | 3.03           |
|    y_velocity           | 0.55           |
| rollout/                |                |
|    adjusted_reward      | 3.48           |
|    ep_len_mean          | 445            |
|    ep_rew_mean          | 1.43e+03       |
| time/                   |                |
|    fps                  | 868            |
|    iterations           | 94             |
|    time_elapsed         | 1107           |
|    total_timesteps      | 962560         |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10176          |
|    mean_motor0          | 0.49493942     |
|    mean_motor1          | 0.5604719      |
|    mean_motor2          | 0.54204327     |
|    mean_motor3          | 0.5927576      |
|    mean_motor4          | 0.54395884     |
|    mean_motor5          | 0.5706752      |
|    mean_motor6          | 0.5471095      |
|    mean_motor7          | 0.6261985      |
| train/                  |                |
|    approx_kl            | 0.03431586     |
|    average_cost         | 0.0036132813   |
|    clip_fraction        | 0.049          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.858          |
|    cost_value_loss      | 0.00669        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -6.5           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0539         |
|    mean_cost_advantages | -0.00045255153 |
|    mean_reward_advan... | 0.041829146    |
|    n_updates            | 1860           |
|    nu                   | 6.48           |
|    nu_loss              | -0.0232        |
|    policy_gradient_loss | -0.00298       |
|    reward_explained_... | 0.893          |
|    reward_value_loss    | 0.0874         |
|    std                  | 0.544          |
|    total_cost           | 37.0           |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 2.27e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.27e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.09         |
|    forward_reward       | 0.284        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.57        |
|    reward_forward       | 0.284        |
|    reward_survive       | 1            |
|    x_position           | 1.54         |
|    x_velocity           | 0.284        |
|    y_position           | 3.61         |
|    y_velocity           | 0.443        |
| rollout/                |              |
|    adjusted_reward      | 3.63         |
|    ep_len_mean          | 440          |
|    ep_rew_mean          | 1.53e+03     |
| time/                   |              |
|    fps                  | 867          |
|    iterations           | 95           |
|    time_elapsed         | 1121         |
|    total_timesteps      | 972800       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10193        |
|    mean_motor0          | 0.51122856   |
|    mean_motor1          | 0.56650984   |
|    mean_motor2          | 0.54045594   |
|    mean_motor3          | 0.5960082    |
|    mean_motor4          | 0.5420355    |
|    mean_motor5          | 0.5830361    |
|    mean_motor6          | 0.5574495    |
|    mean_motor7          | 0.65086687   |
| train/                  |              |
|    approx_kl            | 0.0261858    |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.857        |
|    cost_value_loss      | 0.000596     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.45        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0487       |
|    mean_cost_advantages | -0.002606154 |
|    mean_reward_advan... | 0.024249872  |
|    n_updates            | 1880         |
|    nu                   | 6.53         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00247     |
|    reward_explained_... | 0.923        |
|    reward_value_loss    | 0.0838       |
|    std                  | 0.541        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 2.27e+03     |
|    mean_ep_length       | 444          |
|    mean_reward          | 2.02e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.04         |
|    forward_reward       | 0.378        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.49        |
|    reward_forward       | 0.378        |
|    reward_survive       | 1            |
|    x_position           | 2.53         |
|    x_velocity           | 0.378        |
|    y_position           | 3.47         |
|    y_velocity           | 0.412        |
| rollout/                |              |
|    adjusted_reward      | 4.96         |
|    ep_len_mean          | 445          |
|    ep_rew_mean          | 1.67e+03     |
| time/                   |              |
|    fps                  | 866          |
|    iterations           | 96           |
|    time_elapsed         | 1133         |
|    total_timesteps      | 983040       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10209        |
|    mean_motor0          | 0.50235426   |
|    mean_motor1          | 0.6358491    |
|    mean_motor2          | 0.53882706   |
|    mean_motor3          | 0.6259704    |
|    mean_motor4          | 0.54666424   |
|    mean_motor5          | 0.5591167    |
|    mean_motor6          | 0.55667114   |
|    mean_motor7          | 0.6554415    |
| train/                  |              |
|    approx_kl            | 0.022150848  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.849        |
|    cost_value_loss      | 0.000339     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.42        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0224       |
|    mean_cost_advantages | -0.002318094 |
|    mean_reward_advan... | 0.030644577  |
|    n_updates            | 1900         |
|    nu                   | 6.58         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00241     |
|    reward_explained_... | 0.914        |
|    reward_value_loss    | 0.0993       |
|    std                  | 0.541        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 2.27e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 1.53e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 3.3          |
|    forward_reward       | 0.295        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.58        |
|    reward_forward       | 0.295        |
|    reward_survive       | 1            |
|    x_position           | 0.921        |
|    x_velocity           | 0.295        |
|    y_position           | 2.5          |
|    y_velocity           | 0.427        |
| rollout/                |              |
|    adjusted_reward      | 4.02         |
|    ep_len_mean          | 448          |
|    ep_rew_mean          | 1.77e+03     |
| time/                   |              |
|    fps                  | 865          |
|    iterations           | 97           |
|    time_elapsed         | 1147         |
|    total_timesteps      | 993280       |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10190        |
|    mean_motor0          | 0.5217805    |
|    mean_motor1          | 0.5639479    |
|    mean_motor2          | 0.53559244   |
|    mean_motor3          | 0.57831484   |
|    mean_motor4          | 0.5431477    |
|    mean_motor5          | 0.5849248    |
|    mean_motor6          | 0.5466471    |
|    mean_motor7          | 0.6534174    |
| train/                  |              |
|    approx_kl            | 0.029613703  |
|    average_cost         | 0.0146484375 |
|    clip_fraction        | 0.0428       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.871        |
|    cost_value_loss      | 0.00227      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.4         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0623       |
|    mean_cost_advantages | 0.009103239  |
|    mean_reward_advan... | 0.052616112  |
|    n_updates            | 1920         |
|    nu                   | 6.63         |
|    nu_loss              | -0.0964      |
|    policy_gradient_loss | -0.00274     |
|    reward_explained_... | 0.895        |
|    reward_value_loss    | 0.141        |
|    std                  | 0.538        |
|    total_cost           | 150.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.27e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 1.83e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.84          |
|    forward_reward       | 0.555         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.555         |
|    reward_survive       | 1             |
|    x_position           | 2.75          |
|    x_velocity           | 0.555         |
|    y_position           | 2.39          |
|    y_velocity           | 0.425         |
| rollout/                |               |
|    adjusted_reward      | 4.2           |
|    ep_len_mean          | 445           |
|    ep_rew_mean          | 1.81e+03      |
| time/                   |               |
|    fps                  | 864           |
|    iterations           | 98            |
|    time_elapsed         | 1160          |
|    total_timesteps      | 1003520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10188         |
|    mean_motor0          | 0.517549      |
|    mean_motor1          | 0.5943829     |
|    mean_motor2          | 0.53837657    |
|    mean_motor3          | 0.5867971     |
|    mean_motor4          | 0.54022443    |
|    mean_motor5          | 0.562049      |
|    mean_motor6          | 0.52840275    |
|    mean_motor7          | 0.62844914    |
| train/                  |               |
|    approx_kl            | 0.027135158   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0299        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.914         |
|    cost_value_loss      | 0.000273      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.36         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.107         |
|    mean_cost_advantages | -0.0014340153 |
|    mean_reward_advan... | 0.0325678     |
|    n_updates            | 1940          |
|    nu                   | 6.67          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00201      |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 0.113         |
|    std                  | 0.536         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.27e+03      |
|    mean_ep_length       | 471           |
|    mean_reward          | 1.56e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.07          |
|    forward_reward       | 0.417         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.42         |
|    reward_forward       | 0.417         |
|    reward_survive       | 1             |
|    x_position           | 2.62          |
|    x_velocity           | 0.417         |
|    y_position           | 1.13          |
|    y_velocity           | 0.402         |
| rollout/                |               |
|    adjusted_reward      | 4.63          |
|    ep_len_mean          | 469           |
|    ep_rew_mean          | 2.03e+03      |
| time/                   |               |
|    fps                  | 863           |
|    iterations           | 99            |
|    time_elapsed         | 1173          |
|    total_timesteps      | 1013760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10207         |
|    mean_motor0          | 0.51623386    |
|    mean_motor1          | 0.59636414    |
|    mean_motor2          | 0.54086834    |
|    mean_motor3          | 0.61321926    |
|    mean_motor4          | 0.550576      |
|    mean_motor5          | 0.578397      |
|    mean_motor6          | 0.5390468     |
|    mean_motor7          | 0.6488133     |
| train/                  |               |
|    approx_kl            | 0.035058536   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0695        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.934         |
|    cost_value_loss      | 0.000691      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.34         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0104        |
|    mean_cost_advantages | -0.0047659385 |
|    mean_reward_advan... | 0.021028632   |
|    n_updates            | 1960          |
|    nu                   | 6.71          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00311      |
|    reward_explained_... | 0.921         |
|    reward_value_loss    | 0.119         |
|    std                  | 0.535         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.88e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.88e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.35          |
|    forward_reward       | 0.548         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.64         |
|    reward_forward       | 0.548         |
|    reward_survive       | 1             |
|    x_position           | 1.59          |
|    x_velocity           | 0.548         |
|    y_position           | 3.59          |
|    y_velocity           | 0.441         |
| rollout/                |               |
|    adjusted_reward      | 4.62          |
|    ep_len_mean          | 466           |
|    ep_rew_mean          | 2.04e+03      |
| time/                   |               |
|    fps                  | 862           |
|    iterations           | 100           |
|    time_elapsed         | 1186          |
|    total_timesteps      | 1024000       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10191         |
|    mean_motor0          | 0.5196653     |
|    mean_motor1          | 0.58753264    |
|    mean_motor2          | 0.5659915     |
|    mean_motor3          | 0.6073939     |
|    mean_motor4          | 0.5474242     |
|    mean_motor5          | 0.57879007    |
|    mean_motor6          | 0.53539205    |
|    mean_motor7          | 0.6940641     |
| train/                  |               |
|    approx_kl            | 0.029029056   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0321        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.73          |
|    cost_value_loss      | 0.000144      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.33         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0117        |
|    mean_cost_advantages | -0.0009865797 |
|    mean_reward_advan... | 0.023560468   |
|    n_updates            | 1980          |
|    nu                   | 6.75          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0025       |
|    reward_explained_... | 0.91          |
|    reward_value_loss    | 0.12          |
|    std                  | 0.534         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.88e+03    |
|    mean_ep_length       | 405         |
|    mean_reward          | 1.18e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 4.28        |
|    forward_reward       | 0.469       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.67       |
|    reward_forward       | 0.469       |
|    reward_survive       | 1           |
|    x_position           | 1.66        |
|    x_velocity           | 0.469       |
|    y_position           | 3.64        |
|    y_velocity           | 0.818       |
| rollout/                |             |
|    adjusted_reward      | 5.07        |
|    ep_len_mean          | 470         |
|    ep_rew_mean          | 2.12e+03    |
| time/                   |             |
|    fps                  | 862         |
|    iterations           | 101         |
|    time_elapsed         | 1199        |
|    total_timesteps      | 1034240     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10199       |
|    mean_motor0          | 0.5299548   |
|    mean_motor1          | 0.584164    |
|    mean_motor2          | 0.5454472   |
|    mean_motor3          | 0.60928357  |
|    mean_motor4          | 0.5605837   |
|    mean_motor5          | 0.5678274   |
|    mean_motor6          | 0.53332555  |
|    mean_motor7          | 0.6573972   |
| train/                  |             |
|    approx_kl            | 0.045482147 |
|    average_cost         | 0.02216797  |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.949       |
|    cost_value_loss      | 0.0136      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.3        |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0187      |
|    mean_cost_advantages | 0.011699503 |
|    mean_reward_advan... | 0.028922025 |
|    n_updates            | 2000        |
|    nu                   | 6.79        |
|    nu_loss              | -0.15       |
|    policy_gradient_loss | -0.00429    |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.102       |
|    std                  | 0.531       |
|    total_cost           | 227.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.88e+03      |
|    mean_ep_length       | 431           |
|    mean_reward          | 2.35e+03      |
| infos/                  |               |
|    cost                 | 0.0233        |
|    distance_from_origin | 6.13          |
|    forward_reward       | 0.453         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.58         |
|    reward_forward       | 0.453         |
|    reward_survive       | 1             |
|    x_position           | 2.99          |
|    x_velocity           | 0.453         |
|    y_position           | 4.03          |
|    y_velocity           | 0.489         |
| rollout/                |               |
|    adjusted_reward      | 4.82          |
|    ep_len_mean          | 458           |
|    ep_rew_mean          | 2.13e+03      |
| time/                   |               |
|    fps                  | 861           |
|    iterations           | 102           |
|    time_elapsed         | 1212          |
|    total_timesteps      | 1044480       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10194         |
|    mean_motor0          | 0.5049482     |
|    mean_motor1          | 0.6051979     |
|    mean_motor2          | 0.5454249     |
|    mean_motor3          | 0.62207675    |
|    mean_motor4          | 0.54884255    |
|    mean_motor5          | 0.5691972     |
|    mean_motor6          | 0.5335831     |
|    mean_motor7          | 0.70728797    |
| train/                  |               |
|    approx_kl            | 0.025460362   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0418        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.666         |
|    cost_value_loss      | 0.000184      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.27         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0393        |
|    mean_cost_advantages | -0.0010699149 |
|    mean_reward_advan... | 0.02925931    |
|    n_updates            | 2020          |
|    nu                   | 6.83          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00232      |
|    reward_explained_... | 0.902         |
|    reward_value_loss    | 0.129         |
|    std                  | 0.53          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 2.88e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.5e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 4.89        |
|    forward_reward       | 0.542       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.61       |
|    reward_forward       | 0.542       |
|    reward_survive       | 1           |
|    x_position           | 1.09        |
|    x_velocity           | 0.542       |
|    y_position           | 4.25        |
|    y_velocity           | 0.474       |
| rollout/                |             |
|    adjusted_reward      | 5.61        |
|    ep_len_mean          | 469         |
|    ep_rew_mean          | 2.29e+03    |
| time/                   |             |
|    fps                  | 860         |
|    iterations           | 103         |
|    time_elapsed         | 1225        |
|    total_timesteps      | 1054720     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10215       |
|    mean_motor0          | 0.52099     |
|    mean_motor1          | 0.6286368   |
|    mean_motor2          | 0.55260766  |
|    mean_motor3          | 0.6352945   |
|    mean_motor4          | 0.5398353   |
|    mean_motor5          | 0.57812274  |
|    mean_motor6          | 0.5597767   |
|    mean_motor7          | 0.7134112   |
| train/                  |             |
|    approx_kl            | 0.024288364 |
|    average_cost         | 0.008691406 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.874       |
|    cost_value_loss      | 0.00239     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -6.24       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0372      |
|    mean_cost_advantages | 0.006547886 |
|    mean_reward_advan... | 0.016555745 |
|    n_updates            | 2040        |
|    nu                   | 6.87        |
|    nu_loss              | -0.0594     |
|    policy_gradient_loss | -0.00262    |
|    reward_explained_... | 0.935       |
|    reward_value_loss    | 0.132       |
|    std                  | 0.527       |
|    total_cost           | 89.0        |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.88e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.75e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.61          |
|    forward_reward       | 0.367         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.48         |
|    reward_forward       | 0.367         |
|    reward_survive       | 1             |
|    x_position           | 3.85          |
|    x_velocity           | 0.367         |
|    y_position           | 4.73          |
|    y_velocity           | 0.355         |
| rollout/                |               |
|    adjusted_reward      | 5.33          |
|    ep_len_mean          | 468           |
|    ep_rew_mean          | 2.38e+03      |
| time/                   |               |
|    fps                  | 859           |
|    iterations           | 104           |
|    time_elapsed         | 1239          |
|    total_timesteps      | 1064960       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10214         |
|    mean_motor0          | 0.5271678     |
|    mean_motor1          | 0.6151805     |
|    mean_motor2          | 0.55754244    |
|    mean_motor3          | 0.6359032     |
|    mean_motor4          | 0.5663632     |
|    mean_motor5          | 0.5700798     |
|    mean_motor6          | 0.5362705     |
|    mean_motor7          | 0.71094376    |
| train/                  |               |
|    approx_kl            | 0.03325393    |
|    average_cost         | 0.0022460937  |
|    clip_fraction        | 0.0466        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.875         |
|    cost_value_loss      | 0.000754      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.19         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0574        |
|    mean_cost_advantages | -0.0007241681 |
|    mean_reward_advan... | 0.010908094   |
|    n_updates            | 2060          |
|    nu                   | 6.91          |
|    nu_loss              | -0.0154       |
|    policy_gradient_loss | -0.00232      |
|    reward_explained_... | 0.905         |
|    reward_value_loss    | 0.15          |
|    std                  | 0.524         |
|    total_cost           | 23.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 2.88e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.02e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.33          |
|    forward_reward       | 0.5           |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.63         |
|    reward_forward       | 0.5           |
|    reward_survive       | 1             |
|    x_position           | 0.0709        |
|    x_velocity           | 0.5           |
|    y_position           | 4.17          |
|    y_velocity           | 0.479         |
| rollout/                |               |
|    adjusted_reward      | 4.58          |
|    ep_len_mean          | 455           |
|    ep_rew_mean          | 2.3e+03       |
| time/                   |               |
|    fps                  | 858           |
|    iterations           | 105           |
|    time_elapsed         | 1252          |
|    total_timesteps      | 1075200       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10204         |
|    mean_motor0          | 0.5343661     |
|    mean_motor1          | 0.59082174    |
|    mean_motor2          | 0.548147      |
|    mean_motor3          | 0.60437566    |
|    mean_motor4          | 0.529738      |
|    mean_motor5          | 0.57359225    |
|    mean_motor6          | 0.53998065    |
|    mean_motor7          | 0.7016092     |
| train/                  |               |
|    approx_kl            | 0.033512063   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0471        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.845         |
|    cost_value_loss      | 0.00044       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.15         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0193        |
|    mean_cost_advantages | -0.0022596852 |
|    mean_reward_advan... | 0.01738498    |
|    n_updates            | 2080          |
|    nu                   | 6.94          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00236      |
|    reward_explained_... | 0.922         |
|    reward_value_loss    | 0.133         |
|    std                  | 0.522         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 2.88e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.39e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.94         |
|    forward_reward       | 0.386        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.72        |
|    reward_forward       | 0.386        |
|    reward_survive       | 1            |
|    x_position           | 2.49         |
|    x_velocity           | 0.386        |
|    y_position           | 6.16         |
|    y_velocity           | 0.453        |
| rollout/                |              |
|    adjusted_reward      | 5.29         |
|    ep_len_mean          | 470          |
|    ep_rew_mean          | 2.36e+03     |
| time/                   |              |
|    fps                  | 857          |
|    iterations           | 106          |
|    time_elapsed         | 1265         |
|    total_timesteps      | 1085440      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10198        |
|    mean_motor0          | 0.5137113    |
|    mean_motor1          | 0.5916679    |
|    mean_motor2          | 0.5559511    |
|    mean_motor3          | 0.62851584   |
|    mean_motor4          | 0.53845274   |
|    mean_motor5          | 0.5834258    |
|    mean_motor6          | 0.5401292    |
|    mean_motor7          | 0.68986404   |
| train/                  |              |
|    approx_kl            | 0.024992177  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0357       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.736        |
|    cost_value_loss      | 0.000162     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -6.13        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0414       |
|    mean_cost_advantages | -0.001140611 |
|    mean_reward_advan... | -0.01872223  |
|    n_updates            | 2100         |
|    nu                   | 6.97         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00217     |
|    reward_explained_... | 0.92         |
|    reward_value_loss    | 0.139        |
|    std                  | 0.521        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.03e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.03e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.28           |
|    forward_reward       | 0.553          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.63          |
|    reward_forward       | 0.553          |
|    reward_survive       | 1              |
|    x_position           | 3.15           |
|    x_velocity           | 0.553          |
|    y_position           | 4.14           |
|    y_velocity           | 0.605          |
| rollout/                |                |
|    adjusted_reward      | 5.33           |
|    ep_len_mean          | 467            |
|    ep_rew_mean          | 2.42e+03       |
| time/                   |                |
|    fps                  | 856            |
|    iterations           | 107            |
|    time_elapsed         | 1279           |
|    total_timesteps      | 1095680        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10197          |
|    mean_motor0          | 0.48892155     |
|    mean_motor1          | 0.5993715      |
|    mean_motor2          | 0.5639222      |
|    mean_motor3          | 0.6388577      |
|    mean_motor4          | 0.53808165     |
|    mean_motor5          | 0.5691717      |
|    mean_motor6          | 0.5406508      |
|    mean_motor7          | 0.70633256     |
| train/                  |                |
|    approx_kl            | 0.024749752    |
|    average_cost         | 9.765625e-05   |
|    clip_fraction        | 0.0558         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.746          |
|    cost_value_loss      | 0.000178       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -6.1           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0564         |
|    mean_cost_advantages | -0.00093761337 |
|    mean_reward_advan... | 0.007966235    |
|    n_updates            | 2120           |
|    nu                   | 6.99           |
|    nu_loss              | -0.00068       |
|    policy_gradient_loss | -0.0025        |
|    reward_explained_... | 0.916          |
|    reward_value_loss    | 0.106          |
|    std                  | 0.519          |
|    total_cost           | 1.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.03e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.98e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.4            |
|    forward_reward       | 0.705          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.67          |
|    reward_forward       | 0.705          |
|    reward_survive       | 1              |
|    x_position           | 5.12           |
|    x_velocity           | 0.705          |
|    y_position           | 7.38           |
|    y_velocity           | 0.533          |
| rollout/                |                |
|    adjusted_reward      | 6.31           |
|    ep_len_mean          | 471            |
|    ep_rew_mean          | 2.5e+03        |
| time/                   |                |
|    fps                  | 855            |
|    iterations           | 108            |
|    time_elapsed         | 1292           |
|    total_timesteps      | 1105920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10215          |
|    mean_motor0          | 0.50659364     |
|    mean_motor1          | 0.62973404     |
|    mean_motor2          | 0.5530878      |
|    mean_motor3          | 0.7340783      |
|    mean_motor4          | 0.5468467      |
|    mean_motor5          | 0.57145315     |
|    mean_motor6          | 0.56450593     |
|    mean_motor7          | 0.6881758      |
| train/                  |                |
|    approx_kl            | 0.026325691    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0505         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.622          |
|    cost_value_loss      | 4.97e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -6.07          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0461         |
|    mean_cost_advantages | -0.00042001315 |
|    mean_reward_advan... | 0.0008516401   |
|    n_updates            | 2140           |
|    nu                   | 7.02           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00231       |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 0.119          |
|    std                  | 0.517          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.03e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.22e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.71          |
|    forward_reward       | 0.388         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.58         |
|    reward_forward       | 0.388         |
|    reward_survive       | 1             |
|    x_position           | 3.32          |
|    x_velocity           | 0.388         |
|    y_position           | 5.61          |
|    y_velocity           | 0.296         |
| rollout/                |               |
|    adjusted_reward      | 5.02          |
|    ep_len_mean          | 474           |
|    ep_rew_mean          | 2.52e+03      |
| time/                   |               |
|    fps                  | 854           |
|    iterations           | 109           |
|    time_elapsed         | 1305          |
|    total_timesteps      | 1116160       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10209         |
|    mean_motor0          | 0.5097765     |
|    mean_motor1          | 0.57232463    |
|    mean_motor2          | 0.55858815    |
|    mean_motor3          | 0.72986305    |
|    mean_motor4          | 0.5499271     |
|    mean_motor5          | 0.57666683    |
|    mean_motor6          | 0.5742894     |
|    mean_motor7          | 0.6553792     |
| train/                  |               |
|    approx_kl            | 0.02960954    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0512        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.433         |
|    cost_value_loss      | 3.46e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -6.04         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0321        |
|    mean_cost_advantages | 1.8236497e-05 |
|    mean_reward_advan... | 0.01965049    |
|    n_updates            | 2160          |
|    nu                   | 7.04          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00228      |
|    reward_explained_... | 0.931         |
|    reward_value_loss    | 0.123         |
|    std                  | 0.515         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.42e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.42e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.85           |
|    forward_reward       | 0.386          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.5           |
|    reward_forward       | 0.386          |
|    reward_survive       | 1              |
|    x_position           | 3.02           |
|    x_velocity           | 0.386          |
|    y_position           | 5.88           |
|    y_velocity           | 0.499          |
| rollout/                |                |
|    adjusted_reward      | 5.16           |
|    ep_len_mean          | 484            |
|    ep_rew_mean          | 2.61e+03       |
| time/                   |                |
|    fps                  | 853            |
|    iterations           | 110            |
|    time_elapsed         | 1319           |
|    total_timesteps      | 1126400        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10204          |
|    mean_motor0          | 0.5315572      |
|    mean_motor1          | 0.5557907      |
|    mean_motor2          | 0.56737983     |
|    mean_motor3          | 0.6908254      |
|    mean_motor4          | 0.5998679      |
|    mean_motor5          | 0.58317846     |
|    mean_motor6          | 0.5430981      |
|    mean_motor7          | 0.6717831      |
| train/                  |                |
|    approx_kl            | 0.04873138     |
|    average_cost         | 0.0018554687   |
|    clip_fraction        | 0.0902         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.846          |
|    cost_value_loss      | 0.00201        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -6             |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0316         |
|    mean_cost_advantages | -0.00060599856 |
|    mean_reward_advan... | -0.0073416717  |
|    n_updates            | 2180           |
|    nu                   | 7.06           |
|    nu_loss              | -0.0131        |
|    policy_gradient_loss | -0.00356       |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.107          |
|    std                  | 0.512          |
|    total_cost           | 19.0           |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 3.42e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.29e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.93         |
|    forward_reward       | 0.465        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.66        |
|    reward_forward       | 0.465        |
|    reward_survive       | 1            |
|    x_position           | 2.97         |
|    x_velocity           | 0.465        |
|    y_position           | 3.04         |
|    y_velocity           | 0.517        |
| rollout/                |              |
|    adjusted_reward      | 5.7          |
|    ep_len_mean          | 489          |
|    ep_rew_mean          | 2.64e+03     |
| time/                   |              |
|    fps                  | 852          |
|    iterations           | 111          |
|    time_elapsed         | 1332         |
|    total_timesteps      | 1136640      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10214        |
|    mean_motor0          | 0.52847254   |
|    mean_motor1          | 0.59573144   |
|    mean_motor2          | 0.56577253   |
|    mean_motor3          | 0.7350493    |
|    mean_motor4          | 0.5499676    |
|    mean_motor5          | 0.5847343    |
|    mean_motor6          | 0.5626785    |
|    mean_motor7          | 0.6696649    |
| train/                  |              |
|    approx_kl            | 0.14512017   |
|    average_cost         | 0.03701172   |
|    clip_fraction        | 0.182        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.377        |
|    cost_value_loss      | 0.00396      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.96        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.146        |
|    mean_cost_advantages | 0.034998853  |
|    mean_reward_advan... | 0.0022527862 |
|    n_updates            | 2200         |
|    nu                   | 7.09         |
|    nu_loss              | -0.261       |
|    policy_gradient_loss | -0.00552     |
|    reward_explained_... | 0.938        |
|    reward_value_loss    | 0.0965       |
|    std                  | 0.51         |
|    total_cost           | 379.0        |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.42e+03       |
|    mean_ep_length       | 403            |
|    mean_reward          | 1.38e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.8            |
|    forward_reward       | 0.477          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.47          |
|    reward_forward       | 0.477          |
|    reward_survive       | 1              |
|    x_position           | 1.49           |
|    x_velocity           | 0.477          |
|    y_position           | 4.08           |
|    y_velocity           | 0.504          |
| rollout/                |                |
|    adjusted_reward      | 4.48           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 2.62e+03       |
| time/                   |                |
|    fps                  | 852            |
|    iterations           | 112            |
|    time_elapsed         | 1345           |
|    total_timesteps      | 1146880        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 10202          |
|    mean_motor0          | 0.5169915      |
|    mean_motor1          | 0.5708328      |
|    mean_motor2          | 0.5545521      |
|    mean_motor3          | 0.6656478      |
|    mean_motor4          | 0.55209583     |
|    mean_motor5          | 0.6068158      |
|    mean_motor6          | 0.5526396      |
|    mean_motor7          | 0.6543623      |
| train/                  |                |
|    approx_kl            | 0.030532455    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0487         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.598          |
|    cost_value_loss      | 6.7e-05        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.93          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00506        |
|    mean_cost_advantages | -0.00059830927 |
|    mean_reward_advan... | 0.010893209    |
|    n_updates            | 2220           |
|    nu                   | 7.13           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00228       |
|    reward_explained_... | 0.951          |
|    reward_value_loss    | 0.0907         |
|    std                  | 0.508          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.42e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.21e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.17          |
|    forward_reward       | 0.422         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.68         |
|    reward_forward       | 0.422         |
|    reward_survive       | 1             |
|    x_position           | 3.21          |
|    x_velocity           | 0.422         |
|    y_position           | 7.5           |
|    y_velocity           | 0.455         |
| rollout/                |               |
|    adjusted_reward      | 5.54          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.48e+03      |
| time/                   |               |
|    fps                  | 851           |
|    iterations           | 113           |
|    time_elapsed         | 1358          |
|    total_timesteps      | 1157120       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10217         |
|    mean_motor0          | 0.51296866    |
|    mean_motor1          | 0.6666253     |
|    mean_motor2          | 0.56287515    |
|    mean_motor3          | 0.7347903     |
|    mean_motor4          | 0.5621092     |
|    mean_motor5          | 0.578688      |
|    mean_motor6          | 0.5858109     |
|    mean_motor7          | 0.7275106     |
| train/                  |               |
|    approx_kl            | 0.025161128   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0455        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.831         |
|    cost_value_loss      | 0.00023       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.9          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0322        |
|    mean_cost_advantages | -0.0012946866 |
|    mean_reward_advan... | -0.012460984  |
|    n_updates            | 2240          |
|    nu                   | 7.15          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00227      |
|    reward_explained_... | 0.961         |
|    reward_value_loss    | 0.0825        |
|    std                  | 0.506         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 3.42e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.98e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.6          |
|    forward_reward       | 0.282        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.56        |
|    reward_forward       | 0.282        |
|    reward_survive       | 1            |
|    x_position           | 3.29         |
|    x_velocity           | 0.282        |
|    y_position           | 3.9          |
|    y_velocity           | 0.49         |
| rollout/                |              |
|    adjusted_reward      | 4.71         |
|    ep_len_mean          | 484          |
|    ep_rew_mean          | 2.46e+03     |
| time/                   |              |
|    fps                  | 850          |
|    iterations           | 114          |
|    time_elapsed         | 1371         |
|    total_timesteps      | 1167360      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10203        |
|    mean_motor0          | 0.5052491    |
|    mean_motor1          | 0.58442736   |
|    mean_motor2          | 0.5584357    |
|    mean_motor3          | 0.66201586   |
|    mean_motor4          | 0.5599919    |
|    mean_motor5          | 0.57780826   |
|    mean_motor6          | 0.5462551    |
|    mean_motor7          | 0.67480606   |
| train/                  |              |
|    approx_kl            | 0.031401947  |
|    average_cost         | 0.0071289064 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.811        |
|    cost_value_loss      | 0.00136      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.88        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0649       |
|    mean_cost_advantages | 0.004658346  |
|    mean_reward_advan... | -0.013400072 |
|    n_updates            | 2260         |
|    nu                   | 7.18         |
|    nu_loss              | -0.051       |
|    policy_gradient_loss | -0.00218     |
|    reward_explained_... | 0.95         |
|    reward_value_loss    | 0.102        |
|    std                  | 0.506        |
|    total_cost           | 73.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.42e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.09e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.89          |
|    forward_reward       | 0.57          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.56         |
|    reward_forward       | 0.57          |
|    reward_survive       | 1             |
|    x_position           | 2.21          |
|    x_velocity           | 0.57          |
|    y_position           | 3.96          |
|    y_velocity           | 0.371         |
| rollout/                |               |
|    adjusted_reward      | 6.41          |
|    ep_len_mean          | 484           |
|    ep_rew_mean          | 2.56e+03      |
| time/                   |               |
|    fps                  | 850           |
|    iterations           | 115           |
|    time_elapsed         | 1385          |
|    total_timesteps      | 1177600       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10218         |
|    mean_motor0          | 0.51011676    |
|    mean_motor1          | 0.6716853     |
|    mean_motor2          | 0.59243363    |
|    mean_motor3          | 0.713736      |
|    mean_motor4          | 0.5709618     |
|    mean_motor5          | 0.55290043    |
|    mean_motor6          | 0.57053006    |
|    mean_motor7          | 0.73985887    |
| train/                  |               |
|    approx_kl            | 0.030790757   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0473        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.723         |
|    cost_value_loss      | 0.00016       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.88         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0214        |
|    mean_cost_advantages | -0.0011775937 |
|    mean_reward_advan... | -0.017790373  |
|    n_updates            | 2280          |
|    nu                   | 7.21          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00222      |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 0.0962        |
|    std                  | 0.505         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.42e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.49e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.18           |
|    forward_reward       | 0.426          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.63          |
|    reward_forward       | 0.426          |
|    reward_survive       | 1              |
|    x_position           | 3.14           |
|    x_velocity           | 0.426          |
|    y_position           | 4.02           |
|    y_velocity           | 0.564          |
| rollout/                |                |
|    adjusted_reward      | 5.48           |
|    ep_len_mean          | 479            |
|    ep_rew_mean          | 2.57e+03       |
| time/                   |                |
|    fps                  | 849            |
|    iterations           | 116            |
|    time_elapsed         | 1398           |
|    total_timesteps      | 1187840        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10203          |
|    mean_motor0          | 0.5045619      |
|    mean_motor1          | 0.6111503      |
|    mean_motor2          | 0.5756059      |
|    mean_motor3          | 0.69859964     |
|    mean_motor4          | 0.5710866      |
|    mean_motor5          | 0.5496147      |
|    mean_motor6          | 0.5674826      |
|    mean_motor7          | 0.7488188      |
| train/                  |                |
|    approx_kl            | 0.026052246    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0385         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.726          |
|    cost_value_loss      | 5.07e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.86          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0839         |
|    mean_cost_advantages | -0.00064367644 |
|    mean_reward_advan... | -0.00596915    |
|    n_updates            | 2300           |
|    nu                   | 7.23           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00183       |
|    reward_explained_... | 0.926          |
|    reward_value_loss    | 0.102          |
|    std                  | 0.504          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.51e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.51e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.77          |
|    forward_reward       | 0.411         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.75         |
|    reward_forward       | 0.411         |
|    reward_survive       | 1             |
|    x_position           | 3.91          |
|    x_velocity           | 0.411         |
|    y_position           | 5.21          |
|    y_velocity           | 0.542         |
| rollout/                |               |
|    adjusted_reward      | 6.37          |
|    ep_len_mean          | 479           |
|    ep_rew_mean          | 2.71e+03      |
| time/                   |               |
|    fps                  | 848           |
|    iterations           | 117           |
|    time_elapsed         | 1411          |
|    total_timesteps      | 1198080       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10215         |
|    mean_motor0          | 0.5135911     |
|    mean_motor1          | 0.6466744     |
|    mean_motor2          | 0.5782488     |
|    mean_motor3          | 0.7296504     |
|    mean_motor4          | 0.5699866     |
|    mean_motor5          | 0.53121954    |
|    mean_motor6          | 0.5680433     |
|    mean_motor7          | 0.75731087    |
| train/                  |               |
|    approx_kl            | 0.02484879    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0432        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.713         |
|    cost_value_loss      | 3.13e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.84         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0307        |
|    mean_cost_advantages | -0.0007490609 |
|    mean_reward_advan... | -0.0118031595 |
|    n_updates            | 2320          |
|    nu                   | 7.25          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00205      |
|    reward_explained_... | 0.958         |
|    reward_value_loss    | 0.0843        |
|    std                  | 0.502         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.56e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.56e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.25           |
|    forward_reward       | 0.451          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.57          |
|    reward_forward       | 0.451          |
|    reward_survive       | 1              |
|    x_position           | 3.01           |
|    x_velocity           | 0.451          |
|    y_position           | 5.39           |
|    y_velocity           | 0.429          |
| rollout/                |                |
|    adjusted_reward      | 7.04           |
|    ep_len_mean          | 482            |
|    ep_rew_mean          | 2.89e+03       |
| time/                   |                |
|    fps                  | 847            |
|    iterations           | 118            |
|    time_elapsed         | 1425           |
|    total_timesteps      | 1208320        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10218          |
|    mean_motor0          | 0.5002333      |
|    mean_motor1          | 0.6343298      |
|    mean_motor2          | 0.5777241      |
|    mean_motor3          | 0.7185535      |
|    mean_motor4          | 0.56660974     |
|    mean_motor5          | 0.5310851      |
|    mean_motor6          | 0.5675974      |
|    mean_motor7          | 0.74584883     |
| train/                  |                |
|    approx_kl            | 0.033150014    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0579         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.582          |
|    cost_value_loss      | 3.35e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.81          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0628         |
|    mean_cost_advantages | -0.00050460256 |
|    mean_reward_advan... | -0.0050651086  |
|    n_updates            | 2340           |
|    nu                   | 7.27           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00215       |
|    reward_explained_... | 0.948          |
|    reward_value_loss    | 0.0919         |
|    std                  | 0.5            |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.56e+03       |
|    mean_ep_length       | 429            |
|    mean_reward          | 2.5e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.22           |
|    forward_reward       | 0.465          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.63          |
|    reward_forward       | 0.465          |
|    reward_survive       | 1              |
|    x_position           | 3.94           |
|    x_velocity           | 0.465          |
|    y_position           | 4.67           |
|    y_velocity           | 0.419          |
| rollout/                |                |
|    adjusted_reward      | 5.98           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.08e+03       |
| time/                   |                |
|    fps                  | 847            |
|    iterations           | 119            |
|    time_elapsed         | 1438           |
|    total_timesteps      | 1218560        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10197          |
|    mean_motor0          | 0.5062163      |
|    mean_motor1          | 0.58334506     |
|    mean_motor2          | 0.57258916     |
|    mean_motor3          | 0.645491       |
|    mean_motor4          | 0.5639325      |
|    mean_motor5          | 0.53724456     |
|    mean_motor6          | 0.55078995     |
|    mean_motor7          | 0.71726555     |
| train/                  |                |
|    approx_kl            | 0.026262045    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0503         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.462          |
|    cost_value_loss      | 2.22e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.77          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0339         |
|    mean_cost_advantages | -0.00048280816 |
|    mean_reward_advan... | 0.0065683112   |
|    n_updates            | 2360           |
|    nu                   | 7.29           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00221       |
|    reward_explained_... | 0.943          |
|    reward_value_loss    | 0.0841         |
|    std                  | 0.498          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.56e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.72e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.22           |
|    forward_reward       | 0.494          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.69          |
|    reward_forward       | 0.494          |
|    reward_survive       | 1              |
|    x_position           | 4.37           |
|    x_velocity           | 0.494          |
|    y_position           | 6.58           |
|    y_velocity           | 0.667          |
| rollout/                |                |
|    adjusted_reward      | 6.32           |
|    ep_len_mean          | 487            |
|    ep_rew_mean          | 3.02e+03       |
| time/                   |                |
|    fps                  | 846            |
|    iterations           | 120            |
|    time_elapsed         | 1451           |
|    total_timesteps      | 1228800        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10210          |
|    mean_motor0          | 0.5210494      |
|    mean_motor1          | 0.64933443     |
|    mean_motor2          | 0.5829779      |
|    mean_motor3          | 0.67645323     |
|    mean_motor4          | 0.5491576      |
|    mean_motor5          | 0.5269669      |
|    mean_motor6          | 0.56277424     |
|    mean_motor7          | 0.7802743      |
| train/                  |                |
|    approx_kl            | 0.033510268    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0511         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.36           |
|    cost_value_loss      | 2.12e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.74          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0162         |
|    mean_cost_advantages | -0.00045919503 |
|    mean_reward_advan... | 0.00051339343  |
|    n_updates            | 2380           |
|    nu                   | 7.31           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00235       |
|    reward_explained_... | 0.959          |
|    reward_value_loss    | 0.0626         |
|    std                  | 0.496          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.56e+03      |
|    mean_ep_length       | 434           |
|    mean_reward          | 2.75e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.94          |
|    forward_reward       | 0.488         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.68         |
|    reward_forward       | 0.488         |
|    reward_survive       | 1             |
|    x_position           | 4.95          |
|    x_velocity           | 0.488         |
|    y_position           | 6.19          |
|    y_velocity           | 0.547         |
| rollout/                |               |
|    adjusted_reward      | 7.33          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.19e+03      |
| time/                   |               |
|    fps                  | 846           |
|    iterations           | 121           |
|    time_elapsed         | 1464          |
|    total_timesteps      | 1239040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10216         |
|    mean_motor0          | 0.502027      |
|    mean_motor1          | 0.6639464     |
|    mean_motor2          | 0.5686235     |
|    mean_motor3          | 0.7207623     |
|    mean_motor4          | 0.5786254     |
|    mean_motor5          | 0.5118125     |
|    mean_motor6          | 0.56421334    |
|    mean_motor7          | 0.7968358     |
| train/                  |               |
|    approx_kl            | 0.027198886   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0448        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.399         |
|    cost_value_loss      | 1.54e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.7          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0365        |
|    mean_cost_advantages | -0.0003021618 |
|    mean_reward_advan... | -0.0075975237 |
|    n_updates            | 2400          |
|    nu                   | 7.32          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00198      |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 0.0743        |
|    std                  | 0.493         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.56e+03       |
|    mean_ep_length       | 412            |
|    mean_reward          | 2.05e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 11             |
|    forward_reward       | 0.527          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.78          |
|    reward_forward       | 0.527          |
|    reward_survive       | 1              |
|    x_position           | 7              |
|    x_velocity           | 0.527          |
|    y_position           | 8.22           |
|    y_velocity           | 0.494          |
| rollout/                |                |
|    adjusted_reward      | 6.63           |
|    ep_len_mean          | 483            |
|    ep_rew_mean          | 3.18e+03       |
| time/                   |                |
|    fps                  | 845            |
|    iterations           | 122            |
|    time_elapsed         | 1477           |
|    total_timesteps      | 1249280        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10199          |
|    mean_motor0          | 0.49802107     |
|    mean_motor1          | 0.6615231      |
|    mean_motor2          | 0.5530676      |
|    mean_motor3          | 0.6918918      |
|    mean_motor4          | 0.5780407      |
|    mean_motor5          | 0.5248297      |
|    mean_motor6          | 0.57095796     |
|    mean_motor7          | 0.7232245      |
| train/                  |                |
|    approx_kl            | 0.032302614    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0478         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.408          |
|    cost_value_loss      | 1.08e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.66          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00964        |
|    mean_cost_advantages | -0.00029773783 |
|    mean_reward_advan... | -0.0053465986  |
|    n_updates            | 2420           |
|    nu                   | 7.33           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00218       |
|    reward_explained_... | 0.931          |
|    reward_value_loss    | 0.0845         |
|    std                  | 0.491          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.56e+03      |
|    mean_ep_length       | 440           |
|    mean_reward          | 3e+03         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.86          |
|    forward_reward       | 0.362         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.57         |
|    reward_forward       | 0.362         |
|    reward_survive       | 1             |
|    x_position           | 4.85          |
|    x_velocity           | 0.362         |
|    y_position           | 6.05          |
|    y_velocity           | 0.481         |
| rollout/                |               |
|    adjusted_reward      | 7.66          |
|    ep_len_mean          | 479           |
|    ep_rew_mean          | 3.22e+03      |
| time/                   |               |
|    fps                  | 845           |
|    iterations           | 123           |
|    time_elapsed         | 1490          |
|    total_timesteps      | 1259520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10222         |
|    mean_motor0          | 0.51093537    |
|    mean_motor1          | 0.6723123     |
|    mean_motor2          | 0.5623103     |
|    mean_motor3          | 0.7295042     |
|    mean_motor4          | 0.5589554     |
|    mean_motor5          | 0.53383124    |
|    mean_motor6          | 0.5657946     |
|    mean_motor7          | 0.76550543    |
| train/                  |               |
|    approx_kl            | 0.026655054   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0396        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.552         |
|    cost_value_loss      | 1.92e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.62         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0561        |
|    mean_cost_advantages | -0.0002832193 |
|    mean_reward_advan... | 0.004748511   |
|    n_updates            | 2440          |
|    nu                   | 7.34          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00232      |
|    reward_explained_... | 0.966         |
|    reward_value_loss    | 0.0565        |
|    std                  | 0.488         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.76e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.76e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.7           |
|    forward_reward       | 0.379          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.6           |
|    reward_forward       | 0.379          |
|    reward_survive       | 1              |
|    x_position           | 6.28           |
|    x_velocity           | 0.379          |
|    y_position           | 8.65           |
|    y_velocity           | 0.264          |
| rollout/                |                |
|    adjusted_reward      | 6.75           |
|    ep_len_mean          | 474            |
|    ep_rew_mean          | 3.29e+03       |
| time/                   |                |
|    fps                  | 844            |
|    iterations           | 124            |
|    time_elapsed         | 1503           |
|    total_timesteps      | 1269760        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10206          |
|    mean_motor0          | 0.510177       |
|    mean_motor1          | 0.6463741      |
|    mean_motor2          | 0.5476068      |
|    mean_motor3          | 0.6833645      |
|    mean_motor4          | 0.55962133     |
|    mean_motor5          | 0.54167885     |
|    mean_motor6          | 0.56108713     |
|    mean_motor7          | 0.6992885      |
| train/                  |                |
|    approx_kl            | 0.028405886    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0431         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.458          |
|    cost_value_loss      | 1.07e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.58          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0388         |
|    mean_cost_advantages | -0.00014476266 |
|    mean_reward_advan... | 0.006779642    |
|    n_updates            | 2460           |
|    nu                   | 7.35           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00191       |
|    reward_explained_... | 0.953          |
|    reward_value_loss    | 0.0663         |
|    std                  | 0.486          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.76e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.53e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 11.8           |
|    forward_reward       | 0.446          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.7           |
|    reward_forward       | 0.446          |
|    reward_survive       | 1              |
|    x_position           | 8.27           |
|    x_velocity           | 0.446          |
|    y_position           | 7.93           |
|    y_velocity           | 0.49           |
| rollout/                |                |
|    adjusted_reward      | 6.7            |
|    ep_len_mean          | 479            |
|    ep_rew_mean          | 3.31e+03       |
| time/                   |                |
|    fps                  | 843            |
|    iterations           | 125            |
|    time_elapsed         | 1516           |
|    total_timesteps      | 1280000        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10213          |
|    mean_motor0          | 0.5050804      |
|    mean_motor1          | 0.6320407      |
|    mean_motor2          | 0.5485207      |
|    mean_motor3          | 0.71001107     |
|    mean_motor4          | 0.55667627     |
|    mean_motor5          | 0.5351848      |
|    mean_motor6          | 0.54750764     |
|    mean_motor7          | 0.7389227      |
| train/                  |                |
|    approx_kl            | 0.032938235    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0563         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.501          |
|    cost_value_loss      | 1.19e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.56          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0359         |
|    mean_cost_advantages | -0.00021012314 |
|    mean_reward_advan... | -0.004408934   |
|    n_updates            | 2480           |
|    nu                   | 7.36           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00235       |
|    reward_explained_... | 0.962          |
|    reward_value_loss    | 0.0642         |
|    std                  | 0.485          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.76e+03      |
|    mean_ep_length       | 428           |
|    mean_reward          | 3.05e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.93          |
|    forward_reward       | 0.661         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.73         |
|    reward_forward       | 0.661         |
|    reward_survive       | 1             |
|    x_position           | 4.74          |
|    x_velocity           | 0.661         |
|    y_position           | 6.17          |
|    y_velocity           | 0.521         |
| rollout/                |               |
|    adjusted_reward      | 7.37          |
|    ep_len_mean          | 473           |
|    ep_rew_mean          | 3.3e+03       |
| time/                   |               |
|    fps                  | 843           |
|    iterations           | 126           |
|    time_elapsed         | 1529          |
|    total_timesteps      | 1290240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10215         |
|    mean_motor0          | 0.5104235     |
|    mean_motor1          | 0.67063034    |
|    mean_motor2          | 0.56941575    |
|    mean_motor3          | 0.69190884    |
|    mean_motor4          | 0.5696568     |
|    mean_motor5          | 0.53005975    |
|    mean_motor6          | 0.55745596    |
|    mean_motor7          | 0.7478108     |
| train/                  |               |
|    approx_kl            | 0.030764943   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.055         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.478         |
|    cost_value_loss      | 7.65e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.54         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.035         |
|    mean_cost_advantages | -9.674986e-05 |
|    mean_reward_advan... | -0.026648188  |
|    n_updates            | 2500          |
|    nu                   | 7.37          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00201      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.0836        |
|    std                  | 0.484         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.76e+03      |
|    mean_ep_length       | 470           |
|    mean_reward          | 3.7e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.23          |
|    forward_reward       | 0.559         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.68         |
|    reward_forward       | 0.559         |
|    reward_survive       | 1             |
|    x_position           | 4.98          |
|    x_velocity           | 0.559         |
|    y_position           | 7.42          |
|    y_velocity           | 0.541         |
| rollout/                |               |
|    adjusted_reward      | 7.4           |
|    ep_len_mean          | 477           |
|    ep_rew_mean          | 3.38e+03      |
| time/                   |               |
|    fps                  | 842           |
|    iterations           | 127           |
|    time_elapsed         | 1542          |
|    total_timesteps      | 1300480       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10219         |
|    mean_motor0          | 0.51195437    |
|    mean_motor1          | 0.68448675    |
|    mean_motor2          | 0.56018454    |
|    mean_motor3          | 0.7116219     |
|    mean_motor4          | 0.57151425    |
|    mean_motor5          | 0.53718513    |
|    mean_motor6          | 0.57109845    |
|    mean_motor7          | 0.7278365     |
| train/                  |               |
|    approx_kl            | 0.025761936   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0561        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.304         |
|    cost_value_loss      | 9.39e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.53         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000368      |
|    mean_cost_advantages | -5.591036e-05 |
|    mean_reward_advan... | -0.026092116  |
|    n_updates            | 2520          |
|    nu                   | 7.38          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00229      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.0801        |
|    std                  | 0.484         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.76e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.5e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.21          |
|    forward_reward       | 0.267         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.62         |
|    reward_forward       | 0.267         |
|    reward_survive       | 1             |
|    x_position           | 5.75          |
|    x_velocity           | 0.267         |
|    y_position           | 6.48          |
|    y_velocity           | 0.4           |
| rollout/                |               |
|    adjusted_reward      | 7.68          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.49e+03      |
| time/                   |               |
|    fps                  | 842           |
|    iterations           | 128           |
|    time_elapsed         | 1556          |
|    total_timesteps      | 1310720       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10219         |
|    mean_motor0          | 0.51222444    |
|    mean_motor1          | 0.6848437     |
|    mean_motor2          | 0.5545937     |
|    mean_motor3          | 0.7091608     |
|    mean_motor4          | 0.5867181     |
|    mean_motor5          | 0.5359187     |
|    mean_motor6          | 0.568164      |
|    mean_motor7          | 0.7215222     |
| train/                  |               |
|    approx_kl            | 0.02552352    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0436        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.401         |
|    cost_value_loss      | 7.89e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.51         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00406       |
|    mean_cost_advantages | 2.9527127e-05 |
|    mean_reward_advan... | -0.014266429  |
|    n_updates            | 2540          |
|    nu                   | 7.39          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00192      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.0724        |
|    std                  | 0.482         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 3.76e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.34e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.5          |
|    forward_reward       | 0.349         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.72         |
|    reward_forward       | 0.349         |
|    reward_survive       | 1             |
|    x_position           | 6.61          |
|    x_velocity           | 0.349         |
|    y_position           | 7.82          |
|    y_velocity           | 0.441         |
| rollout/                |               |
|    adjusted_reward      | 6.67          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.42e+03      |
| time/                   |               |
|    fps                  | 841           |
|    iterations           | 129           |
|    time_elapsed         | 1569          |
|    total_timesteps      | 1320960       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10197         |
|    mean_motor0          | 0.5216467     |
|    mean_motor1          | 0.6251045     |
|    mean_motor2          | 0.54452443    |
|    mean_motor3          | 0.6318372     |
|    mean_motor4          | 0.58143985    |
|    mean_motor5          | 0.55035466    |
|    mean_motor6          | 0.58034015    |
|    mean_motor7          | 0.66280335    |
| train/                  |               |
|    approx_kl            | 0.029736707   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0546        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.278         |
|    cost_value_loss      | 6.76e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.49         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0399        |
|    mean_cost_advantages | -4.949239e-05 |
|    mean_reward_advan... | -0.020590436  |
|    n_updates            | 2560          |
|    nu                   | 7.39          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00215      |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.0743        |
|    std                  | 0.481         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 3.76e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.34e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.96           |
|    forward_reward       | 0.651          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.64          |
|    reward_forward       | 0.651          |
|    reward_survive       | 1              |
|    x_position           | 2.69           |
|    x_velocity           | 0.651          |
|    y_position           | 3.84           |
|    y_velocity           | 0.711          |
| rollout/                |                |
|    adjusted_reward      | 7.18           |
|    ep_len_mean          | 474            |
|    ep_rew_mean          | 3.43e+03       |
| time/                   |                |
|    fps                  | 841            |
|    iterations           | 130            |
|    time_elapsed         | 1582           |
|    total_timesteps      | 1331200        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10215          |
|    mean_motor0          | 0.5246798      |
|    mean_motor1          | 0.6547487      |
|    mean_motor2          | 0.5440803      |
|    mean_motor3          | 0.6602782      |
|    mean_motor4          | 0.5679583      |
|    mean_motor5          | 0.54893005     |
|    mean_motor6          | 0.55316883     |
|    mean_motor7          | 0.7292175      |
| train/                  |                |
|    approx_kl            | 0.026743736    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0595         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.429          |
|    cost_value_loss      | 9.18e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.48          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0272         |
|    mean_cost_advantages | -0.00010266663 |
|    mean_reward_advan... | -0.00969252    |
|    n_updates            | 2580           |
|    nu                   | 7.4            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0024        |
|    reward_explained_... | 0.964          |
|    reward_value_loss    | 0.0558         |
|    std                  | 0.48           |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.1e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.78         |
|    forward_reward       | 0.736        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.7         |
|    reward_forward       | 0.736        |
|    reward_survive       | 1            |
|    x_position           | 4.79         |
|    x_velocity           | 0.736        |
|    y_position           | 5.95         |
|    y_velocity           | 0.931        |
| rollout/                |              |
|    adjusted_reward      | 7.55         |
|    ep_len_mean          | 488          |
|    ep_rew_mean          | 3.55e+03     |
| time/                   |              |
|    fps                  | 840          |
|    iterations           | 131          |
|    time_elapsed         | 1595         |
|    total_timesteps      | 1341440      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10210        |
|    mean_motor0          | 0.5210023    |
|    mean_motor1          | 0.63519806   |
|    mean_motor2          | 0.56675565   |
|    mean_motor3          | 0.64695597   |
|    mean_motor4          | 0.5677732    |
|    mean_motor5          | 0.54434186   |
|    mean_motor6          | 0.54001164   |
|    mean_motor7          | 0.7627322    |
| train/                  |              |
|    approx_kl            | 0.029483888  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0624       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.598        |
|    cost_value_loss      | 7.64e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.46        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0825       |
|    mean_cost_advantages | 4.454737e-06 |
|    mean_reward_advan... | -0.027029091 |
|    n_updates            | 2600         |
|    nu                   | 7.4          |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00227     |
|    reward_explained_... | 0.947        |
|    reward_value_loss    | 0.0729       |
|    std                  | 0.479        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.38e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 11.7          |
|    forward_reward       | 0.47          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.65         |
|    reward_forward       | 0.47          |
|    reward_survive       | 1             |
|    x_position           | 7.16          |
|    x_velocity           | 0.47          |
|    y_position           | 8.71          |
|    y_velocity           | 0.308         |
| rollout/                |               |
|    adjusted_reward      | 7.72          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.49e+03      |
| time/                   |               |
|    fps                  | 839           |
|    iterations           | 132           |
|    time_elapsed         | 1609          |
|    total_timesteps      | 1351680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10220         |
|    mean_motor0          | 0.52314925    |
|    mean_motor1          | 0.65266865    |
|    mean_motor2          | 0.5772796     |
|    mean_motor3          | 0.6722287     |
|    mean_motor4          | 0.56789124    |
|    mean_motor5          | 0.54180986    |
|    mean_motor6          | 0.5400159     |
|    mean_motor7          | 0.7663087     |
| train/                  |               |
|    approx_kl            | 0.03769073    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0667        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.266         |
|    cost_value_loss      | 5.61e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.44         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0179        |
|    mean_cost_advantages | 1.0684104e-06 |
|    mean_reward_advan... | -0.022380564  |
|    n_updates            | 2620          |
|    nu                   | 7.41          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00249      |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.0664        |
|    std                  | 0.478         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.39e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.68           |
|    forward_reward       | 0.419          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.59          |
|    reward_forward       | 0.419          |
|    reward_survive       | 1              |
|    x_position           | 2.94           |
|    x_velocity           | 0.419          |
|    y_position           | 5.85           |
|    y_velocity           | 0.453          |
| rollout/                |                |
|    adjusted_reward      | 6.84           |
|    ep_len_mean          | 482            |
|    ep_rew_mean          | 3.47e+03       |
| time/                   |                |
|    fps                  | 839            |
|    iterations           | 133            |
|    time_elapsed         | 1622           |
|    total_timesteps      | 1361920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10199          |
|    mean_motor0          | 0.5216867      |
|    mean_motor1          | 0.620243       |
|    mean_motor2          | 0.5807184      |
|    mean_motor3          | 0.62914884     |
|    mean_motor4          | 0.54839        |
|    mean_motor5          | 0.5526977      |
|    mean_motor6          | 0.5306083      |
|    mean_motor7          | 0.74843913     |
| train/                  |                |
|    approx_kl            | 0.029145394    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0564         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.294          |
|    cost_value_loss      | 3.81e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.41          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00612        |
|    mean_cost_advantages | -3.2118137e-06 |
|    mean_reward_advan... | -0.035821773   |
|    n_updates            | 2640           |
|    nu                   | 7.41           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00225       |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.0776         |
|    std                  | 0.475          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.36e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.68          |
|    forward_reward       | 0.49          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.63         |
|    reward_forward       | 0.49          |
|    reward_survive       | 1             |
|    x_position           | 4.94          |
|    x_velocity           | 0.49          |
|    y_position           | 6.43          |
|    y_velocity           | 0.533         |
| rollout/                |               |
|    adjusted_reward      | 7.54          |
|    ep_len_mean          | 486           |
|    ep_rew_mean          | 3.6e+03       |
| time/                   |               |
|    fps                  | 838           |
|    iterations           | 134           |
|    time_elapsed         | 1636          |
|    total_timesteps      | 1372160       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10216         |
|    mean_motor0          | 0.5281197     |
|    mean_motor1          | 0.6381341     |
|    mean_motor2          | 0.5890246     |
|    mean_motor3          | 0.65104353    |
|    mean_motor4          | 0.52562696    |
|    mean_motor5          | 0.5510196     |
|    mean_motor6          | 0.56001157    |
|    mean_motor7          | 0.7889855     |
| train/                  |               |
|    approx_kl            | 0.028582916   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0582        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.371         |
|    cost_value_loss      | 5.77e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.38         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00994       |
|    mean_cost_advantages | -1.296571e-05 |
|    mean_reward_advan... | -0.05195588   |
|    n_updates            | 2660          |
|    nu                   | 7.42          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00219      |
|    reward_explained_... | 0.92          |
|    reward_value_loss    | 0.0935        |
|    std                  | 0.475         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 418           |
|    mean_reward          | 2.81e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.23          |
|    forward_reward       | 0.622         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.67         |
|    reward_forward       | 0.622         |
|    reward_survive       | 1             |
|    x_position           | 2.99          |
|    x_velocity           | 0.622         |
|    y_position           | 4.68          |
|    y_velocity           | 0.694         |
| rollout/                |               |
|    adjusted_reward      | 7.67          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.64e+03      |
| time/                   |               |
|    fps                  | 838           |
|    iterations           | 135           |
|    time_elapsed         | 1649          |
|    total_timesteps      | 1382400       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10210         |
|    mean_motor0          | 0.5216557     |
|    mean_motor1          | 0.64018166    |
|    mean_motor2          | 0.5898509     |
|    mean_motor3          | 0.651834      |
|    mean_motor4          | 0.54919887    |
|    mean_motor5          | 0.54011214    |
|    mean_motor6          | 0.5545074     |
|    mean_motor7          | 0.78596985    |
| train/                  |               |
|    approx_kl            | 0.025609547   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.057         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.334         |
|    cost_value_loss      | 3.74e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.38         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0284        |
|    mean_cost_advantages | 3.6028934e-05 |
|    mean_reward_advan... | -0.045476537  |
|    n_updates            | 2680          |
|    nu                   | 7.42          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0021       |
|    reward_explained_... | 0.919         |
|    reward_value_loss    | 0.0892        |
|    std                  | 0.474         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 406            |
|    mean_reward          | 3.13e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.48           |
|    forward_reward       | 0.568          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.52          |
|    reward_forward       | 0.568          |
|    reward_survive       | 1              |
|    x_position           | 4.82           |
|    x_velocity           | 0.568          |
|    y_position           | 4.13           |
|    y_velocity           | 0.427          |
| rollout/                |                |
|    adjusted_reward      | 7.18           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 3.61e+03       |
| time/                   |                |
|    fps                  | 837            |
|    iterations           | 136            |
|    time_elapsed         | 1661           |
|    total_timesteps      | 1392640        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10214          |
|    mean_motor0          | 0.53583413     |
|    mean_motor1          | 0.62728673     |
|    mean_motor2          | 0.56919277     |
|    mean_motor3          | 0.61242807     |
|    mean_motor4          | 0.5368383      |
|    mean_motor5          | 0.5442139      |
|    mean_motor6          | 0.5712019      |
|    mean_motor7          | 0.7424322      |
| train/                  |                |
|    approx_kl            | 0.029496148    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0684         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.294          |
|    cost_value_loss      | 3.19e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.36          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0439         |
|    mean_cost_advantages | -2.5294992e-05 |
|    mean_reward_advan... | -0.048221223   |
|    n_updates            | 2700           |
|    nu                   | 7.42           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00205       |
|    reward_explained_... | 0.92           |
|    reward_value_loss    | 0.0888         |
|    std                  | 0.474          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.19e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.29          |
|    forward_reward       | 0.654         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.69         |
|    reward_forward       | 0.654         |
|    reward_survive       | 1             |
|    x_position           | 3.12          |
|    x_velocity           | 0.654         |
|    y_position           | 6.32          |
|    y_velocity           | 0.816         |
| rollout/                |               |
|    adjusted_reward      | 5.57          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.4e+03       |
| time/                   |               |
|    fps                  | 837           |
|    iterations           | 137           |
|    time_elapsed         | 1675          |
|    total_timesteps      | 1402880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10193         |
|    mean_motor0          | 0.52271134    |
|    mean_motor1          | 0.57786053    |
|    mean_motor2          | 0.55145276    |
|    mean_motor3          | 0.55542266    |
|    mean_motor4          | 0.53909373    |
|    mean_motor5          | 0.58139646    |
|    mean_motor6          | 0.55974674    |
|    mean_motor7          | 0.6926404     |
| train/                  |               |
|    approx_kl            | 0.029229611   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0576        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.691         |
|    cost_value_loss      | 5.83e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.35         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0304        |
|    mean_cost_advantages | 0.00010432571 |
|    mean_reward_advan... | -0.044458292  |
|    n_updates            | 2720          |
|    nu                   | 7.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00201      |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 0.0729        |
|    std                  | 0.473         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 426           |
|    mean_reward          | 3.25e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.6           |
|    forward_reward       | 0.483         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.49         |
|    reward_forward       | 0.483         |
|    reward_survive       | 1             |
|    x_position           | 4.5           |
|    x_velocity           | 0.483         |
|    y_position           | 4.75          |
|    y_velocity           | 0.578         |
| rollout/                |               |
|    adjusted_reward      | 7.62          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.49e+03      |
| time/                   |               |
|    fps                  | 837           |
|    iterations           | 138           |
|    time_elapsed         | 1688          |
|    total_timesteps      | 1413120       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10210         |
|    mean_motor0          | 0.53693044    |
|    mean_motor1          | 0.6101691     |
|    mean_motor2          | 0.5644087     |
|    mean_motor3          | 0.6148449     |
|    mean_motor4          | 0.5035704     |
|    mean_motor5          | 0.55770826    |
|    mean_motor6          | 0.56177974    |
|    mean_motor7          | 0.7931553     |
| train/                  |               |
|    approx_kl            | 0.036815017   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0723        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.706         |
|    cost_value_loss      | 6.96e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.34         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0229        |
|    mean_cost_advantages | 1.6111517e-05 |
|    mean_reward_advan... | -0.033222165  |
|    n_updates            | 2740          |
|    nu                   | 7.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00254      |
|    reward_explained_... | 0.968         |
|    reward_value_loss    | 0.0508        |
|    std                  | 0.473         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.63e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.62          |
|    forward_reward       | 0.386         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.53         |
|    reward_forward       | 0.386         |
|    reward_survive       | 1             |
|    x_position           | 3.64          |
|    x_velocity           | 0.386         |
|    y_position           | 4.82          |
|    y_velocity           | 0.326         |
| rollout/                |               |
|    adjusted_reward      | 7.14          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.46e+03      |
| time/                   |               |
|    fps                  | 836           |
|    iterations           | 139           |
|    time_elapsed         | 1701          |
|    total_timesteps      | 1423360       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10212         |
|    mean_motor0          | 0.55231935    |
|    mean_motor1          | 0.5582137     |
|    mean_motor2          | 0.5597727     |
|    mean_motor3          | 0.5905788     |
|    mean_motor4          | 0.516456      |
|    mean_motor5          | 0.56954956    |
|    mean_motor6          | 0.5650673     |
|    mean_motor7          | 0.7327008     |
| train/                  |               |
|    approx_kl            | 0.026505625   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0479        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.367         |
|    cost_value_loss      | 5.06e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.33         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0184        |
|    mean_cost_advantages | 6.9268135e-05 |
|    mean_reward_advan... | -0.038203448  |
|    n_updates            | 2760          |
|    nu                   | 7.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00165      |
|    reward_explained_... | 0.945         |
|    reward_value_loss    | 0.0645        |
|    std                  | 0.471         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 409           |
|    mean_reward          | 2.94e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.36          |
|    forward_reward       | 0.424         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.5          |
|    reward_forward       | 0.424         |
|    reward_survive       | 1             |
|    x_position           | 2.88          |
|    x_velocity           | 0.424         |
|    y_position           | 6.64          |
|    y_velocity           | 0.61          |
| rollout/                |               |
|    adjusted_reward      | 6.94          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.38e+03      |
| time/                   |               |
|    fps                  | 836           |
|    iterations           | 140           |
|    time_elapsed         | 1714          |
|    total_timesteps      | 1433600       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10207         |
|    mean_motor0          | 0.5321266     |
|    mean_motor1          | 0.5870817     |
|    mean_motor2          | 0.5862263     |
|    mean_motor3          | 0.54270065    |
|    mean_motor4          | 0.52306026    |
|    mean_motor5          | 0.59234524    |
|    mean_motor6          | 0.55252564    |
|    mean_motor7          | 0.7216194     |
| train/                  |               |
|    approx_kl            | 0.027079608   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0554        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.443         |
|    cost_value_loss      | 6.53e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.3          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0254        |
|    mean_cost_advantages | 3.1309373e-05 |
|    mean_reward_advan... | -0.019385891  |
|    n_updates            | 2780          |
|    nu                   | 7.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00244      |
|    reward_explained_... | 0.97          |
|    reward_value_loss    | 0.0491        |
|    std                  | 0.469         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.86e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.03         |
|    forward_reward       | 0.305        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.54        |
|    reward_forward       | 0.305        |
|    reward_survive       | 1            |
|    x_position           | 1.75         |
|    x_velocity           | 0.305        |
|    y_position           | 4.37         |
|    y_velocity           | 0.376        |
| rollout/                |              |
|    adjusted_reward      | 7.44         |
|    ep_len_mean          | 489          |
|    ep_rew_mean          | 3.42e+03     |
| time/                   |              |
|    fps                  | 835          |
|    iterations           | 141          |
|    time_elapsed         | 1727         |
|    total_timesteps      | 1443840      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10214        |
|    mean_motor0          | 0.5455001    |
|    mean_motor1          | 0.56180966   |
|    mean_motor2          | 0.5753307    |
|    mean_motor3          | 0.6022332    |
|    mean_motor4          | 0.507877     |
|    mean_motor5          | 0.587585     |
|    mean_motor6          | 0.569463     |
|    mean_motor7          | 0.7208737    |
| train/                  |              |
|    approx_kl            | 0.03037881   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.577        |
|    cost_value_loss      | 5.19e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.27        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00951      |
|    mean_cost_advantages | 2.755463e-05 |
|    mean_reward_advan... | -0.025918221 |
|    n_updates            | 2800         |
|    nu                   | 7.43         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00244     |
|    reward_explained_... | 0.953        |
|    reward_value_loss    | 0.0573       |
|    std                  | 0.468        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.85e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.29         |
|    forward_reward       | 0.283        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.7         |
|    reward_forward       | 0.283        |
|    reward_survive       | 1            |
|    x_position           | 3.39         |
|    x_velocity           | 0.283        |
|    y_position           | 4.79         |
|    y_velocity           | 0.498        |
| rollout/                |              |
|    adjusted_reward      | 6.44         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 3.39e+03     |
| time/                   |              |
|    fps                  | 835          |
|    iterations           | 142          |
|    time_elapsed         | 1741         |
|    total_timesteps      | 1454080      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10204        |
|    mean_motor0          | 0.5347376    |
|    mean_motor1          | 0.6074266    |
|    mean_motor2          | 0.5971547    |
|    mean_motor3          | 0.5656357    |
|    mean_motor4          | 0.54347765   |
|    mean_motor5          | 0.56537026   |
|    mean_motor6          | 0.5522791    |
|    mean_motor7          | 0.74462974   |
| train/                  |              |
|    approx_kl            | 0.02649923   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0627       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.391        |
|    cost_value_loss      | 3.65e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.25        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0315       |
|    mean_cost_advantages | 4.480604e-05 |
|    mean_reward_advan... | -0.021455277 |
|    n_updates            | 2820         |
|    nu                   | 7.44         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00229     |
|    reward_explained_... | 0.954        |
|    reward_value_loss    | 0.061        |
|    std                  | 0.467        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.18e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.7          |
|    forward_reward       | 0.535        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.7         |
|    reward_forward       | 0.535        |
|    reward_survive       | 1            |
|    x_position           | 2.53         |
|    x_velocity           | 0.535        |
|    y_position           | 3.61         |
|    y_velocity           | 0.818        |
| rollout/                |              |
|    adjusted_reward      | 7.72         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 3.47e+03     |
| time/                   |              |
|    fps                  | 834          |
|    iterations           | 143          |
|    time_elapsed         | 1754         |
|    total_timesteps      | 1464320      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10223        |
|    mean_motor0          | 0.5244154    |
|    mean_motor1          | 0.62405646   |
|    mean_motor2          | 0.5916469    |
|    mean_motor3          | 0.6168599    |
|    mean_motor4          | 0.49390513   |
|    mean_motor5          | 0.57361174   |
|    mean_motor6          | 0.57148564   |
|    mean_motor7          | 0.7792252    |
| train/                  |              |
|    approx_kl            | 0.035020083  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0643       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.457        |
|    cost_value_loss      | 3.42e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.22        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0154       |
|    mean_cost_advantages | 4.234965e-05 |
|    mean_reward_advan... | -0.047892682 |
|    n_updates            | 2840         |
|    nu                   | 7.44         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00221     |
|    reward_explained_... | 0.948        |
|    reward_value_loss    | 0.0767       |
|    std                  | 0.465        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.88e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.74         |
|    forward_reward       | 0.406        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.61        |
|    reward_forward       | 0.406        |
|    reward_survive       | 1            |
|    x_position           | 3.35         |
|    x_velocity           | 0.406        |
|    y_position           | 4.42         |
|    y_velocity           | 0.423        |
| rollout/                |              |
|    adjusted_reward      | 6.64         |
|    ep_len_mean          | 482          |
|    ep_rew_mean          | 3.4e+03      |
| time/                   |              |
|    fps                  | 834          |
|    iterations           | 144          |
|    time_elapsed         | 1767         |
|    total_timesteps      | 1474560      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10216        |
|    mean_motor0          | 0.5128737    |
|    mean_motor1          | 0.5962808    |
|    mean_motor2          | 0.5777973    |
|    mean_motor3          | 0.61008847   |
|    mean_motor4          | 0.50159514   |
|    mean_motor5          | 0.59663665   |
|    mean_motor6          | 0.5757011    |
|    mean_motor7          | 0.7219536    |
| train/                  |              |
|    approx_kl            | 0.028038274  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0532       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.221        |
|    cost_value_loss      | 2.07e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.18        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00952      |
|    mean_cost_advantages | 2.478436e-05 |
|    mean_reward_advan... | -0.05412627  |
|    n_updates            | 2860         |
|    nu                   | 7.44         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00194     |
|    reward_explained_... | 0.894        |
|    reward_value_loss    | 0.0801       |
|    std                  | 0.463        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.12e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.52          |
|    forward_reward       | 0.306         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.306         |
|    reward_survive       | 1             |
|    x_position           | 4.22          |
|    x_velocity           | 0.306         |
|    y_position           | 4.52          |
|    y_velocity           | 0.256         |
| rollout/                |               |
|    adjusted_reward      | 7.33          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 3.41e+03      |
| time/                   |               |
|    fps                  | 833           |
|    iterations           | 145           |
|    time_elapsed         | 1781          |
|    total_timesteps      | 1484800       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10206         |
|    mean_motor0          | 0.532709      |
|    mean_motor1          | 0.5841381     |
|    mean_motor2          | 0.5787674     |
|    mean_motor3          | 0.5917492     |
|    mean_motor4          | 0.51381254    |
|    mean_motor5          | 0.5853908     |
|    mean_motor6          | 0.56414145    |
|    mean_motor7          | 0.70442665    |
| train/                  |               |
|    approx_kl            | 0.033292886   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0639        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.417         |
|    cost_value_loss      | 3e-06         |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.16         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0288        |
|    mean_cost_advantages | 4.3900087e-05 |
|    mean_reward_advan... | -0.05957217   |
|    n_updates            | 2880          |
|    nu                   | 7.44          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00191      |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 0.0759        |
|    std                  | 0.462         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.65e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.14          |
|    forward_reward       | 0.441         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.441         |
|    reward_survive       | 1             |
|    x_position           | 3.52          |
|    x_velocity           | 0.441         |
|    y_position           | 4.46          |
|    y_velocity           | 0.291         |
| rollout/                |               |
|    adjusted_reward      | 6.25          |
|    ep_len_mean          | 488           |
|    ep_rew_mean          | 3.38e+03      |
| time/                   |               |
|    fps                  | 833           |
|    iterations           | 146           |
|    time_elapsed         | 1794          |
|    total_timesteps      | 1495040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10209         |
|    mean_motor0          | 0.51466715    |
|    mean_motor1          | 0.5766626     |
|    mean_motor2          | 0.563961      |
|    mean_motor3          | 0.5630272     |
|    mean_motor4          | 0.53581345    |
|    mean_motor5          | 0.60223633    |
|    mean_motor6          | 0.5551003     |
|    mean_motor7          | 0.6640042     |
| train/                  |               |
|    approx_kl            | 0.03344502    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0808        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.458         |
|    cost_value_loss      | 3.41e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.14         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.016         |
|    mean_cost_advantages | -5.275792e-05 |
|    mean_reward_advan... | -0.039666362  |
|    n_updates            | 2900          |
|    nu                   | 7.44          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00246      |
|    reward_explained_... | 0.935         |
|    reward_value_loss    | 0.0595        |
|    std                  | 0.461         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.39e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.01          |
|    forward_reward       | 0.661         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.67         |
|    reward_forward       | 0.661         |
|    reward_survive       | 1             |
|    x_position           | 4.88          |
|    x_velocity           | 0.661         |
|    y_position           | 6.13          |
|    y_velocity           | 0.757         |
| rollout/                |               |
|    adjusted_reward      | 6.91          |
|    ep_len_mean          | 485           |
|    ep_rew_mean          | 3.36e+03      |
| time/                   |               |
|    fps                  | 832           |
|    iterations           | 147           |
|    time_elapsed         | 1807          |
|    total_timesteps      | 1505280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10207         |
|    mean_motor0          | 0.530517      |
|    mean_motor1          | 0.5685505     |
|    mean_motor2          | 0.58144605    |
|    mean_motor3          | 0.5756986     |
|    mean_motor4          | 0.5339685     |
|    mean_motor5          | 0.58519524    |
|    mean_motor6          | 0.57190716    |
|    mean_motor7          | 0.6949092     |
| train/                  |               |
|    approx_kl            | 0.031059152   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0854        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.494         |
|    cost_value_loss      | 4.31e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.13         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0238        |
|    mean_cost_advantages | -5.670594e-06 |
|    mean_reward_advan... | -0.030576725  |
|    n_updates            | 2920          |
|    nu                   | 7.44          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00285      |
|    reward_explained_... | 0.96          |
|    reward_value_loss    | 0.0453        |
|    std                  | 0.46          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 430          |
|    mean_reward          | 3.81e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.72         |
|    forward_reward       | 0.47         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.6         |
|    reward_forward       | 0.47         |
|    reward_survive       | 1            |
|    x_position           | 4.43         |
|    x_velocity           | 0.47         |
|    y_position           | 7.3          |
|    y_velocity           | 0.554        |
| rollout/                |              |
|    adjusted_reward      | 6.78         |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 3.29e+03     |
| time/                   |              |
|    fps                  | 832          |
|    iterations           | 148          |
|    time_elapsed         | 1820         |
|    total_timesteps      | 1515520      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10206        |
|    mean_motor0          | 0.5184643    |
|    mean_motor1          | 0.566584     |
|    mean_motor2          | 0.55971396   |
|    mean_motor3          | 0.55628526   |
|    mean_motor4          | 0.56808054   |
|    mean_motor5          | 0.5843097    |
|    mean_motor6          | 0.53029287   |
|    mean_motor7          | 0.67461807   |
| train/                  |              |
|    approx_kl            | 0.028777784  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0666       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.505        |
|    cost_value_loss      | 3.33e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -5.09        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0164       |
|    mean_cost_advantages | 1.810798e-05 |
|    mean_reward_advan... | -0.032130994 |
|    n_updates            | 2940         |
|    nu                   | 7.44         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00217     |
|    reward_explained_... | 0.944        |
|    reward_value_loss    | 0.0584       |
|    std                  | 0.457        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.45e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.04          |
|    forward_reward       | 0.426         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.69         |
|    reward_forward       | 0.426         |
|    reward_survive       | 1             |
|    x_position           | 3.65          |
|    x_velocity           | 0.426         |
|    y_position           | 8.18          |
|    y_velocity           | 0.499         |
| rollout/                |               |
|    adjusted_reward      | 7.28          |
|    ep_len_mean          | 486           |
|    ep_rew_mean          | 3.32e+03      |
| time/                   |               |
|    fps                  | 831           |
|    iterations           | 149           |
|    time_elapsed         | 1834          |
|    total_timesteps      | 1525760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10221         |
|    mean_motor0          | 0.54751766    |
|    mean_motor1          | 0.59130585    |
|    mean_motor2          | 0.5826074     |
|    mean_motor3          | 0.57816845    |
|    mean_motor4          | 0.5559403     |
|    mean_motor5          | 0.5900914     |
|    mean_motor6          | 0.5627474     |
|    mean_motor7          | 0.7241241     |
| train/                  |               |
|    approx_kl            | 0.030350503   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0685        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.818         |
|    cost_value_loss      | 4.73e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -5.06         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00268       |
|    mean_cost_advantages | 0.00015467919 |
|    mean_reward_advan... | -0.023936534  |
|    n_updates            | 2960          |
|    nu                   | 7.44          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00251      |
|    reward_explained_... | 0.953         |
|    reward_value_loss    | 0.0483        |
|    std                  | 0.456         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 364         |
|    mean_reward          | 1.87e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 7.01        |
|    forward_reward       | 0.488       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.59       |
|    reward_forward       | 0.488       |
|    reward_survive       | 1           |
|    x_position           | 4.15        |
|    x_velocity           | 0.488       |
|    y_position           | 5.48        |
|    y_velocity           | 0.364       |
| rollout/                |             |
|    adjusted_reward      | 5.68        |
|    ep_len_mean          | 478         |
|    ep_rew_mean          | 3.17e+03    |
| time/                   |             |
|    fps                  | 831         |
|    iterations           | 150         |
|    time_elapsed         | 1846        |
|    total_timesteps      | 1536000     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10205       |
|    mean_motor0          | 0.5548552   |
|    mean_motor1          | 0.54781306  |
|    mean_motor2          | 0.5764558   |
|    mean_motor3          | 0.55934185  |
|    mean_motor4          | 0.53509176  |
|    mean_motor5          | 0.59243524  |
|    mean_motor6          | 0.55965996  |
|    mean_motor7          | 0.6728382   |
| train/                  |             |
|    approx_kl            | 0.17194885  |
|    average_cost         | 0.026464844 |
|    clip_fraction        | 0.233       |
|    clip_range           | 0.4         |
|    cost_explained_va... | -3          |
|    cost_value_loss      | 0.00394     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -5.03       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00154     |
|    mean_cost_advantages | 0.032315016 |
|    mean_reward_advan... | -0.03786656 |
|    n_updates            | 2980        |
|    nu                   | 7.46        |
|    nu_loss              | -0.197      |
|    policy_gradient_loss | -0.00732    |
|    reward_explained_... | 0.925       |
|    reward_value_loss    | 0.0634      |
|    std                  | 0.454       |
|    total_cost           | 271.0       |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.36e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.29           |
|    forward_reward       | 0.527          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.59          |
|    reward_forward       | 0.527          |
|    reward_survive       | 1              |
|    x_position           | 3.21           |
|    x_velocity           | 0.527          |
|    y_position           | 4.15           |
|    y_velocity           | 0.66           |
| rollout/                |                |
|    adjusted_reward      | 6.66           |
|    ep_len_mean          | 477            |
|    ep_rew_mean          | 3.2e+03        |
| time/                   |                |
|    fps                  | 831            |
|    iterations           | 151            |
|    time_elapsed         | 1860           |
|    total_timesteps      | 1546240        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10204          |
|    mean_motor0          | 0.5512183      |
|    mean_motor1          | 0.5211346      |
|    mean_motor2          | 0.6054638      |
|    mean_motor3          | 0.58972764     |
|    mean_motor4          | 0.5117716      |
|    mean_motor5          | 0.6043979      |
|    mean_motor6          | 0.5698752      |
|    mean_motor7          | 0.7079441      |
| train/                  |                |
|    approx_kl            | 0.030194065    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0614         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.459          |
|    cost_value_loss      | 2.19e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -5.01          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0444         |
|    mean_cost_advantages | -0.00010054731 |
|    mean_reward_advan... | -0.05782358    |
|    n_updates            | 3000           |
|    nu                   | 7.47           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00183       |
|    reward_explained_... | 0.942          |
|    reward_value_loss    | 0.077          |
|    std                  | 0.454          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 3.67e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 6.01        |
|    forward_reward       | 0.732       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.62       |
|    reward_forward       | 0.732       |
|    reward_survive       | 1           |
|    x_position           | 3.58        |
|    x_velocity           | 0.732       |
|    y_position           | 4.77        |
|    y_velocity           | 0.746       |
| rollout/                |             |
|    adjusted_reward      | 6.42        |
|    ep_len_mean          | 472         |
|    ep_rew_mean          | 3.07e+03    |
| time/                   |             |
|    fps                  | 830         |
|    iterations           | 152         |
|    time_elapsed         | 1873        |
|    total_timesteps      | 1556480     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10205       |
|    mean_motor0          | 0.5313783   |
|    mean_motor1          | 0.51735175  |
|    mean_motor2          | 0.60253173  |
|    mean_motor3          | 0.57349634  |
|    mean_motor4          | 0.5172819   |
|    mean_motor5          | 0.584128    |
|    mean_motor6          | 0.5674133   |
|    mean_motor7          | 0.6817327   |
| train/                  |             |
|    approx_kl            | 0.030119121 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.613       |
|    cost_value_loss      | 1.37e-05    |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -5          |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0694      |
|    mean_cost_advantages | 6.66174e-05 |
|    mean_reward_advan... | -0.05826954 |
|    n_updates            | 3020        |
|    nu                   | 7.48        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00192    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0776      |
|    std                  | 0.453       |
|    total_cost           | 0.0         |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.79e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.97          |
|    forward_reward       | 0.595         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.595         |
|    reward_survive       | 1             |
|    x_position           | 5.73          |
|    x_velocity           | 0.595         |
|    y_position           | 5.46          |
|    y_velocity           | 0.478         |
| rollout/                |               |
|    adjusted_reward      | 6.46          |
|    ep_len_mean          | 471           |
|    ep_rew_mean          | 3.01e+03      |
| time/                   |               |
|    fps                  | 830           |
|    iterations           | 153           |
|    time_elapsed         | 1886          |
|    total_timesteps      | 1566720       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10206         |
|    mean_motor0          | 0.53679025    |
|    mean_motor1          | 0.5251516     |
|    mean_motor2          | 0.57738054    |
|    mean_motor3          | 0.55254567    |
|    mean_motor4          | 0.5211822     |
|    mean_motor5          | 0.62839323    |
|    mean_motor6          | 0.5389391     |
|    mean_motor7          | 0.6749197     |
| train/                  |               |
|    approx_kl            | 0.02611504    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0656        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.657         |
|    cost_value_loss      | 8.59e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.98         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00738       |
|    mean_cost_advantages | -4.048011e-06 |
|    mean_reward_advan... | -0.045219667  |
|    n_updates            | 3040          |
|    nu                   | 7.49          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00198      |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 0.0587        |
|    std                  | 0.451         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.62e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.75          |
|    forward_reward       | 0.379         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.379         |
|    reward_survive       | 1             |
|    x_position           | 2.98          |
|    x_velocity           | 0.379         |
|    y_position           | 4.5           |
|    y_velocity           | 0.739         |
| rollout/                |               |
|    adjusted_reward      | 5.8           |
|    ep_len_mean          | 476           |
|    ep_rew_mean          | 2.93e+03      |
| time/                   |               |
|    fps                  | 829           |
|    iterations           | 154           |
|    time_elapsed         | 1900          |
|    total_timesteps      | 1576960       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10202         |
|    mean_motor0          | 0.5672299     |
|    mean_motor1          | 0.50828993    |
|    mean_motor2          | 0.5751069     |
|    mean_motor3          | 0.57792383    |
|    mean_motor4          | 0.4953828     |
|    mean_motor5          | 0.6414329     |
|    mean_motor6          | 0.544562      |
|    mean_motor7          | 0.68535584    |
| train/                  |               |
|    approx_kl            | 0.03438613    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0784        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.761         |
|    cost_value_loss      | 8.01e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.96         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0032        |
|    mean_cost_advantages | 3.8724163e-05 |
|    mean_reward_advan... | -0.029975489  |
|    n_updates            | 3060          |
|    nu                   | 7.5           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00262      |
|    reward_explained_... | 0.926         |
|    reward_value_loss    | 0.0557        |
|    std                  | 0.45          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.98e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.76          |
|    forward_reward       | 0.448         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.448         |
|    reward_survive       | 1             |
|    x_position           | 1.59          |
|    x_velocity           | 0.448         |
|    y_position           | 3.37          |
|    y_velocity           | 0.534         |
| rollout/                |               |
|    adjusted_reward      | 5.93          |
|    ep_len_mean          | 485           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 829           |
|    iterations           | 155           |
|    time_elapsed         | 1913          |
|    total_timesteps      | 1587200       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10192         |
|    mean_motor0          | 0.5248595     |
|    mean_motor1          | 0.5086398     |
|    mean_motor2          | 0.57934654    |
|    mean_motor3          | 0.56595004    |
|    mean_motor4          | 0.53458226    |
|    mean_motor5          | 0.6193716     |
|    mean_motor6          | 0.51926744    |
|    mean_motor7          | 0.68778855    |
| train/                  |               |
|    approx_kl            | 0.027329843   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0626        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.722         |
|    cost_value_loss      | 8.03e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.94         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0474        |
|    mean_cost_advantages | 3.7066817e-05 |
|    mean_reward_advan... | -0.03929111   |
|    n_updates            | 3080          |
|    nu                   | 7.51          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00211      |
|    reward_explained_... | 0.955         |
|    reward_value_loss    | 0.0493        |
|    std                  | 0.45          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 444            |
|    mean_reward          | 2.09e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.86           |
|    forward_reward       | 0.589          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.59          |
|    reward_forward       | 0.589          |
|    reward_survive       | 1              |
|    x_position           | 2.98           |
|    x_velocity           | 0.589          |
|    y_position           | 4.98           |
|    y_velocity           | 0.699          |
| rollout/                |                |
|    adjusted_reward      | 5.84           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 2.99e+03       |
| time/                   |                |
|    fps                  | 829            |
|    iterations           | 156            |
|    time_elapsed         | 1926           |
|    total_timesteps      | 1597440        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10205          |
|    mean_motor0          | 0.51981515     |
|    mean_motor1          | 0.5158359      |
|    mean_motor2          | 0.5891741      |
|    mean_motor3          | 0.55765146     |
|    mean_motor4          | 0.54448444     |
|    mean_motor5          | 0.65471995     |
|    mean_motor6          | 0.5169938      |
|    mean_motor7          | 0.71026796     |
| train/                  |                |
|    approx_kl            | 0.028947428    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0571         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.59           |
|    cost_value_loss      | 4.93e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.94          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0177         |
|    mean_cost_advantages | -0.00015705687 |
|    mean_reward_advan... | -0.0407228     |
|    n_updates            | 3100           |
|    nu                   | 7.52           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00208       |
|    reward_explained_... | 0.945          |
|    reward_value_loss    | 0.053          |
|    std                  | 0.45           |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.3e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7              |
|    forward_reward       | 0.34           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.42          |
|    reward_forward       | 0.34           |
|    reward_survive       | 1              |
|    x_position           | 3.72           |
|    x_velocity           | 0.34           |
|    y_position           | 5.69           |
|    y_velocity           | 0.372          |
| rollout/                |                |
|    adjusted_reward      | 6.84           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 3.02e+03       |
| time/                   |                |
|    fps                  | 828            |
|    iterations           | 157            |
|    time_elapsed         | 1939           |
|    total_timesteps      | 1607680        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10211          |
|    mean_motor0          | 0.5397439      |
|    mean_motor1          | 0.5337485      |
|    mean_motor2          | 0.604043       |
|    mean_motor3          | 0.5755629      |
|    mean_motor4          | 0.49880823     |
|    mean_motor5          | 0.6452133      |
|    mean_motor6          | 0.5354835      |
|    mean_motor7          | 0.67708623     |
| train/                  |                |
|    approx_kl            | 0.030681267    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.062          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.508          |
|    cost_value_loss      | 3.73e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.93          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0144         |
|    mean_cost_advantages | -0.00017080728 |
|    mean_reward_advan... | -0.030194795   |
|    n_updates            | 3120           |
|    nu                   | 7.53           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00186       |
|    reward_explained_... | 0.947          |
|    reward_value_loss    | 0.0474         |
|    std                  | 0.449          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 466            |
|    mean_reward          | 3.63e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9              |
|    forward_reward       | 0.363          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.67          |
|    reward_forward       | 0.363          |
|    reward_survive       | 1              |
|    x_position           | 5.33           |
|    x_velocity           | 0.363          |
|    y_position           | 7.18           |
|    y_velocity           | 0.267          |
| rollout/                |                |
|    adjusted_reward      | 6.49           |
|    ep_len_mean          | 491            |
|    ep_rew_mean          | 3.01e+03       |
| time/                   |                |
|    fps                  | 828            |
|    iterations           | 158            |
|    time_elapsed         | 1953           |
|    total_timesteps      | 1617920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10207          |
|    mean_motor0          | 0.5367778      |
|    mean_motor1          | 0.53407323     |
|    mean_motor2          | 0.5802938      |
|    mean_motor3          | 0.5683172      |
|    mean_motor4          | 0.53228366     |
|    mean_motor5          | 0.6356139      |
|    mean_motor6          | 0.52058756     |
|    mean_motor7          | 0.6725224      |
| train/                  |                |
|    approx_kl            | 0.028228734    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0591         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.524          |
|    cost_value_loss      | 3.38e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.9           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0243         |
|    mean_cost_advantages | -0.00020267749 |
|    mean_reward_advan... | -0.030918997   |
|    n_updates            | 3140           |
|    nu                   | 7.53           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00222       |
|    reward_explained_... | 0.902          |
|    reward_value_loss    | 0.0568         |
|    std                  | 0.446          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.74e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.38           |
|    forward_reward       | 0.538          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.46          |
|    reward_forward       | 0.538          |
|    reward_survive       | 1              |
|    x_position           | 5.89           |
|    x_velocity           | 0.538          |
|    y_position           | 7.24           |
|    y_velocity           | 0.218          |
| rollout/                |                |
|    adjusted_reward      | 6.66           |
|    ep_len_mean          | 486            |
|    ep_rew_mean          | 3.07e+03       |
| time/                   |                |
|    fps                  | 828            |
|    iterations           | 159            |
|    time_elapsed         | 1966           |
|    total_timesteps      | 1628160        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10212          |
|    mean_motor0          | 0.5673177      |
|    mean_motor1          | 0.5187119      |
|    mean_motor2          | 0.5989572      |
|    mean_motor3          | 0.55412394     |
|    mean_motor4          | 0.5190882      |
|    mean_motor5          | 0.6879083      |
|    mean_motor6          | 0.5082014      |
|    mean_motor7          | 0.6969023      |
| train/                  |                |
|    approx_kl            | 0.03226892     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0658         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.492          |
|    cost_value_loss      | 2.88e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.86          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0143         |
|    mean_cost_advantages | -0.00023213573 |
|    mean_reward_advan... | -0.0212642     |
|    n_updates            | 3160           |
|    nu                   | 7.54           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00227       |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 0.0388         |
|    std                  | 0.445          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 330           |
|    mean_reward          | 2.07e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.64          |
|    forward_reward       | 0.336         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.41         |
|    reward_forward       | 0.336         |
|    reward_survive       | 1             |
|    x_position           | 4.29          |
|    x_velocity           | 0.336         |
|    y_position           | 3.75          |
|    y_velocity           | 0.164         |
| rollout/                |               |
|    adjusted_reward      | 6.43          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.13e+03      |
| time/                   |               |
|    fps                  | 828           |
|    iterations           | 160           |
|    time_elapsed         | 1978          |
|    total_timesteps      | 1638400       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10196         |
|    mean_motor0          | 0.53622186    |
|    mean_motor1          | 0.5016575     |
|    mean_motor2          | 0.6032176     |
|    mean_motor3          | 0.5639528     |
|    mean_motor4          | 0.47736496    |
|    mean_motor5          | 0.6519524     |
|    mean_motor6          | 0.52238864    |
|    mean_motor7          | 0.6975514     |
| train/                  |               |
|    approx_kl            | 0.02947501    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0633        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.41          |
|    cost_value_loss      | 2.56e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.83         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00951       |
|    mean_cost_advantages | -0.0001715548 |
|    mean_reward_advan... | -0.027853364  |
|    n_updates            | 3180          |
|    nu                   | 7.54          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00222      |
|    reward_explained_... | 0.922         |
|    reward_value_loss    | 0.052         |
|    std                  | 0.443         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.33e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.97          |
|    forward_reward       | 0.29          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.61         |
|    reward_forward       | 0.29          |
|    reward_survive       | 1             |
|    x_position           | 5.85          |
|    x_velocity           | 0.29          |
|    y_position           | 5.17          |
|    y_velocity           | 0.314         |
| rollout/                |               |
|    adjusted_reward      | 5.16          |
|    ep_len_mean          | 486           |
|    ep_rew_mean          | 3.05e+03      |
| time/                   |               |
|    fps                  | 827           |
|    iterations           | 161           |
|    time_elapsed         | 1992          |
|    total_timesteps      | 1648640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10206         |
|    mean_motor0          | 0.5344507     |
|    mean_motor1          | 0.4938447     |
|    mean_motor2          | 0.5871917     |
|    mean_motor3          | 0.52919286    |
|    mean_motor4          | 0.54093635    |
|    mean_motor5          | 0.6999129     |
|    mean_motor6          | 0.49622402    |
|    mean_motor7          | 0.7010698     |
| train/                  |               |
|    approx_kl            | 0.047773875   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0837        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.819         |
|    cost_value_loss      | 3.2e-06       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.8          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0269        |
|    mean_cost_advantages | -8.226656e-05 |
|    mean_reward_advan... | -0.033210892  |
|    n_updates            | 3200          |
|    nu                   | 7.55          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00263      |
|    reward_explained_... | 0.936         |
|    reward_value_loss    | 0.0523        |
|    std                  | 0.442         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.64e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.33          |
|    forward_reward       | 0.208         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.74         |
|    reward_forward       | 0.208         |
|    reward_survive       | 1             |
|    x_position           | 4.87          |
|    x_velocity           | 0.208         |
|    y_position           | 6.51          |
|    y_velocity           | 0.296         |
| rollout/                |               |
|    adjusted_reward      | 6.23          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 827           |
|    iterations           | 162           |
|    time_elapsed         | 2005          |
|    total_timesteps      | 1658880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10202         |
|    mean_motor0          | 0.5314847     |
|    mean_motor1          | 0.47884044    |
|    mean_motor2          | 0.63225603    |
|    mean_motor3          | 0.55220866    |
|    mean_motor4          | 0.527032      |
|    mean_motor5          | 0.66844195    |
|    mean_motor6          | 0.5063847     |
|    mean_motor7          | 0.76508504    |
| train/                  |               |
|    approx_kl            | 0.033611424   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0755        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.843         |
|    cost_value_loss      | 4.13e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.78         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00486       |
|    mean_cost_advantages | 7.8566314e-05 |
|    mean_reward_advan... | -0.035071544  |
|    n_updates            | 3220          |
|    nu                   | 7.55          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00223      |
|    reward_explained_... | 0.959         |
|    reward_value_loss    | 0.0467        |
|    std                  | 0.441         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | 4.1e+03         |
|    mean_ep_length       | 500             |
|    mean_reward          | 3.08e+03        |
| infos/                  |                 |
|    cost                 | 0               |
|    distance_from_origin | 7.65            |
|    forward_reward       | 0.227           |
|    reward_contact       | 0               |
|    reward_ctrl          | -1.63           |
|    reward_forward       | 0.227           |
|    reward_survive       | 1               |
|    x_position           | 4.75            |
|    x_velocity           | 0.227           |
|    y_position           | 5.64            |
|    y_velocity           | 0.321           |
| rollout/                |                 |
|    adjusted_reward      | 6.19            |
|    ep_len_mean          | 484             |
|    ep_rew_mean          | 2.93e+03        |
| time/                   |                 |
|    fps                  | 826             |
|    iterations           | 163             |
|    time_elapsed         | 2019            |
|    total_timesteps      | 1669120         |
| torque/                 |                 |
|    greater_than_0.25    | 10240           |
|    greater_than_0.3     | 10239           |
|    greater_than_0.5     | 10214           |
|    mean_motor0          | 0.55519897      |
|    mean_motor1          | 0.50810754      |
|    mean_motor2          | 0.6312376       |
|    mean_motor3          | 0.5398683       |
|    mean_motor4          | 0.5241269       |
|    mean_motor5          | 0.6862011       |
|    mean_motor6          | 0.50948495      |
|    mean_motor7          | 0.7788239       |
| train/                  |                 |
|    approx_kl            | 0.025559362     |
|    average_cost         | 0.0             |
|    clip_fraction        | 0.0443          |
|    clip_range           | 0.4             |
|    cost_explained_va... | 0.494           |
|    cost_value_loss      | 2.18e-06        |
|    early_stop_epoch     | 20              |
|    entropy_loss         | -4.76           |
|    learning_rate        | 3e-05           |
|    loss                 | 0.00121         |
|    mean_cost_advantages | -0.000118597236 |
|    mean_reward_advan... | -0.038571402    |
|    n_updates            | 3240            |
|    nu                   | 7.56            |
|    nu_loss              | -0              |
|    policy_gradient_loss | -0.00154        |
|    reward_explained_... | 0.927           |
|    reward_value_loss    | 0.0548          |
|    std                  | 0.439           |
|    total_cost           | 0.0             |
---------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.18e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.49           |
|    forward_reward       | 0.281          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.6           |
|    reward_forward       | 0.281          |
|    reward_survive       | 1              |
|    x_position           | 5.71           |
|    x_velocity           | 0.281          |
|    y_position           | 7.41           |
|    y_velocity           | 0.238          |
| rollout/                |                |
|    adjusted_reward      | 6.66           |
|    ep_len_mean          | 483            |
|    ep_rew_mean          | 2.91e+03       |
| time/                   |                |
|    fps                  | 826            |
|    iterations           | 164            |
|    time_elapsed         | 2032           |
|    total_timesteps      | 1679360        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10219          |
|    mean_motor0          | 0.53498405     |
|    mean_motor1          | 0.5002543      |
|    mean_motor2          | 0.6442944      |
|    mean_motor3          | 0.55120355     |
|    mean_motor4          | 0.5281897      |
|    mean_motor5          | 0.6680517      |
|    mean_motor6          | 0.5261668      |
|    mean_motor7          | 0.8100853      |
| train/                  |                |
|    approx_kl            | 0.036380433    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0805         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.68           |
|    cost_value_loss      | 2.64e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.74          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0347         |
|    mean_cost_advantages | -0.00010605224 |
|    mean_reward_advan... | -0.04069338    |
|    n_updates            | 3260           |
|    nu                   | 7.56           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00216       |
|    reward_explained_... | 0.899          |
|    reward_value_loss    | 0.059          |
|    std                  | 0.439          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 488           |
|    mean_reward          | 3.3e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.4           |
|    forward_reward       | 0.268         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.6          |
|    reward_forward       | 0.268         |
|    reward_survive       | 1             |
|    x_position           | 1.69          |
|    x_velocity           | 0.268         |
|    y_position           | 2.63          |
|    y_velocity           | 0.374         |
| rollout/                |               |
|    adjusted_reward      | 6.09          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.02e+03      |
| time/                   |               |
|    fps                  | 825           |
|    iterations           | 165           |
|    time_elapsed         | 2045          |
|    total_timesteps      | 1689600       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10208         |
|    mean_motor0          | 0.5459124     |
|    mean_motor1          | 0.51888466    |
|    mean_motor2          | 0.62523764    |
|    mean_motor3          | 0.5556325     |
|    mean_motor4          | 0.5190958     |
|    mean_motor5          | 0.64111435    |
|    mean_motor6          | 0.5244572     |
|    mean_motor7          | 0.76711696    |
| train/                  |               |
|    approx_kl            | 0.026828116   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.057         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.614         |
|    cost_value_loss      | 2.05e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.73         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0913        |
|    mean_cost_advantages | -0.0001277262 |
|    mean_reward_advan... | -0.028997475  |
|    n_updates            | 3280          |
|    nu                   | 7.56          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00181      |
|    reward_explained_... | 0.9           |
|    reward_value_loss    | 0.0527        |
|    std                  | 0.438         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.85e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.23          |
|    forward_reward       | 0.474         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.57         |
|    reward_forward       | 0.474         |
|    reward_survive       | 1             |
|    x_position           | 4.37          |
|    x_velocity           | 0.474         |
|    y_position           | 3.95          |
|    y_velocity           | 0.545         |
| rollout/                |               |
|    adjusted_reward      | 6.21          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 3.03e+03      |
| time/                   |               |
|    fps                  | 825           |
|    iterations           | 166           |
|    time_elapsed         | 2059          |
|    total_timesteps      | 1699840       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10220         |
|    mean_motor0          | 0.54582524    |
|    mean_motor1          | 0.5177482     |
|    mean_motor2          | 0.62380826    |
|    mean_motor3          | 0.5440037     |
|    mean_motor4          | 0.4923033     |
|    mean_motor5          | 0.64231074    |
|    mean_motor6          | 0.54440355    |
|    mean_motor7          | 0.76053524    |
| train/                  |               |
|    approx_kl            | 0.032892544   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0648        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.541         |
|    cost_value_loss      | 2.73e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.73         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0399        |
|    mean_cost_advantages | -6.990893e-05 |
|    mean_reward_advan... | -0.02056561   |
|    n_updates            | 3300          |
|    nu                   | 7.57          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00189      |
|    reward_explained_... | 0.934         |
|    reward_value_loss    | 0.0437        |
|    std                  | 0.438         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.44e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.04           |
|    forward_reward       | 0.496          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.67          |
|    reward_forward       | 0.496          |
|    reward_survive       | 1              |
|    x_position           | 3.89           |
|    x_velocity           | 0.496          |
|    y_position           | 6.39           |
|    y_velocity           | 0.386          |
| rollout/                |                |
|    adjusted_reward      | 6.61           |
|    ep_len_mean          | 485            |
|    ep_rew_mean          | 3.08e+03       |
| time/                   |                |
|    fps                  | 825            |
|    iterations           | 167            |
|    time_elapsed         | 2072           |
|    total_timesteps      | 1710080        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10212          |
|    mean_motor0          | 0.56893766     |
|    mean_motor1          | 0.51689714     |
|    mean_motor2          | 0.66048205     |
|    mean_motor3          | 0.55208313     |
|    mean_motor4          | 0.47318324     |
|    mean_motor5          | 0.6434741      |
|    mean_motor6          | 0.5349932      |
|    mean_motor7          | 0.7741395      |
| train/                  |                |
|    approx_kl            | 0.040983368    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0849         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.714          |
|    cost_value_loss      | 4.67e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.7           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0147         |
|    mean_cost_advantages | -1.1507218e-05 |
|    mean_reward_advan... | -0.015319569   |
|    n_updates            | 3320           |
|    nu                   | 7.57           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0028        |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.0327         |
|    std                  | 0.436          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
---------------------------------------------
| eval/                   |                 |
|    best_mean_reward     | 4.1e+03         |
|    mean_ep_length       | 500             |
|    mean_reward          | 2.82e+03        |
| infos/                  |                 |
|    cost                 | 0               |
|    distance_from_origin | 6.6             |
|    forward_reward       | 0.364           |
|    reward_contact       | 0               |
|    reward_ctrl          | -1.52           |
|    reward_forward       | 0.364           |
|    reward_survive       | 1               |
|    x_position           | 3.69            |
|    x_velocity           | 0.364           |
|    y_position           | 5.15            |
|    y_velocity           | 0.433           |
| rollout/                |                 |
|    adjusted_reward      | 6.06            |
|    ep_len_mean          | 487             |
|    ep_rew_mean          | 3.07e+03        |
| time/                   |                 |
|    fps                  | 824             |
|    iterations           | 168             |
|    time_elapsed         | 2086            |
|    total_timesteps      | 1720320         |
| torque/                 |                 |
|    greater_than_0.25    | 10240           |
|    greater_than_0.3     | 10240           |
|    greater_than_0.5     | 10214           |
|    mean_motor0          | 0.57531697      |
|    mean_motor1          | 0.52098167      |
|    mean_motor2          | 0.6424267       |
|    mean_motor3          | 0.54821855      |
|    mean_motor4          | 0.48158407      |
|    mean_motor5          | 0.63903356      |
|    mean_motor6          | 0.50059646      |
|    mean_motor7          | 0.80576867      |
| train/                  |                 |
|    approx_kl            | 0.031227103     |
|    average_cost         | 0.0             |
|    clip_fraction        | 0.0663          |
|    clip_range           | 0.4             |
|    cost_explained_va... | 0.605           |
|    cost_value_loss      | 3.27e-06        |
|    early_stop_epoch     | 20              |
|    entropy_loss         | -4.66           |
|    learning_rate        | 3e-05           |
|    loss                 | 0.0346          |
|    mean_cost_advantages | -0.000105845626 |
|    mean_reward_advan... | -0.02127099     |
|    n_updates            | 3340            |
|    nu                   | 7.57            |
|    nu_loss              | -0              |
|    policy_gradient_loss | -0.00209        |
|    reward_explained_... | 0.898           |
|    reward_value_loss    | 0.0415          |
|    std                  | 0.434           |
|    total_cost           | 0.0             |
---------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.5e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.12          |
|    forward_reward       | 0.415         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.57         |
|    reward_forward       | 0.415         |
|    reward_survive       | 1             |
|    x_position           | 5.13          |
|    x_velocity           | 0.415         |
|    y_position           | 5.95          |
|    y_velocity           | 0.467         |
| rollout/                |               |
|    adjusted_reward      | 6.88          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.05e+03      |
| time/                   |               |
|    fps                  | 824           |
|    iterations           | 169           |
|    time_elapsed         | 2099          |
|    total_timesteps      | 1730560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10215         |
|    mean_motor0          | 0.5476129     |
|    mean_motor1          | 0.5006605     |
|    mean_motor2          | 0.662883      |
|    mean_motor3          | 0.567515      |
|    mean_motor4          | 0.47459826    |
|    mean_motor5          | 0.6539928     |
|    mean_motor6          | 0.4931926     |
|    mean_motor7          | 0.8503976     |
| train/                  |               |
|    approx_kl            | 0.0359404     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0696        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.786         |
|    cost_value_loss      | 2.21e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.64         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.022         |
|    mean_cost_advantages | 1.7769727e-05 |
|    mean_reward_advan... | -0.023424799  |
|    n_updates            | 3360          |
|    nu                   | 7.57          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00232      |
|    reward_explained_... | 0.922         |
|    reward_value_loss    | 0.0364        |
|    std                  | 0.433         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.74e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.36          |
|    forward_reward       | 0.287         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.56         |
|    reward_forward       | 0.287         |
|    reward_survive       | 1             |
|    x_position           | 3.17          |
|    x_velocity           | 0.287         |
|    y_position           | 5.14          |
|    y_velocity           | 0.477         |
| rollout/                |               |
|    adjusted_reward      | 6.65          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.15e+03      |
| time/                   |               |
|    fps                  | 823           |
|    iterations           | 170           |
|    time_elapsed         | 2112          |
|    total_timesteps      | 1740800       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10222         |
|    mean_motor0          | 0.52131593    |
|    mean_motor1          | 0.4867639     |
|    mean_motor2          | 0.63378245    |
|    mean_motor3          | 0.57038015    |
|    mean_motor4          | 0.49117607    |
|    mean_motor5          | 0.640237      |
|    mean_motor6          | 0.4832542     |
|    mean_motor7          | 0.83807915    |
| train/                  |               |
|    approx_kl            | 0.028570134   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0616        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.451         |
|    cost_value_loss      | 1.34e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.62         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0166        |
|    mean_cost_advantages | -0.0001079029 |
|    mean_reward_advan... | -0.023144184  |
|    n_updates            | 3380          |
|    nu                   | 7.57          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00199      |
|    reward_explained_... | 0.838         |
|    reward_value_loss    | 0.0486        |
|    std                  | 0.432         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.88e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.78           |
|    forward_reward       | 0.308          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.51          |
|    reward_forward       | 0.308          |
|    reward_survive       | 1              |
|    x_position           | 3.81           |
|    x_velocity           | 0.308          |
|    y_position           | 4.84           |
|    y_velocity           | 0.531          |
| rollout/                |                |
|    adjusted_reward      | 6.97           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 3.27e+03       |
| time/                   |                |
|    fps                  | 823            |
|    iterations           | 171            |
|    time_elapsed         | 2126           |
|    total_timesteps      | 1751040        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10222          |
|    mean_motor0          | 0.51595515     |
|    mean_motor1          | 0.50220823     |
|    mean_motor2          | 0.64081794     |
|    mean_motor3          | 0.5585309      |
|    mean_motor4          | 0.4837912      |
|    mean_motor5          | 0.6151292      |
|    mean_motor6          | 0.4956196      |
|    mean_motor7          | 0.8030036      |
| train/                  |                |
|    approx_kl            | 0.027589176    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0539         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.608          |
|    cost_value_loss      | 1.45e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.6           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0173         |
|    mean_cost_advantages | -2.7290045e-05 |
|    mean_reward_advan... | -0.027655447   |
|    n_updates            | 3400           |
|    nu                   | 7.58           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00172       |
|    reward_explained_... | 0.895          |
|    reward_value_loss    | 0.0506         |
|    std                  | 0.431          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.9e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.84          |
|    forward_reward       | 0.553         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.58         |
|    reward_forward       | 0.553         |
|    reward_survive       | 1             |
|    x_position           | 2.15          |
|    x_velocity           | 0.553         |
|    y_position           | 6.43          |
|    y_velocity           | 0.59          |
| rollout/                |               |
|    adjusted_reward      | 5.9           |
|    ep_len_mean          | 489           |
|    ep_rew_mean          | 3.18e+03      |
| time/                   |               |
|    fps                  | 823           |
|    iterations           | 172           |
|    time_elapsed         | 2139          |
|    total_timesteps      | 1761280       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10199         |
|    mean_motor0          | 0.5150363     |
|    mean_motor1          | 0.49465623    |
|    mean_motor2          | 0.61233914    |
|    mean_motor3          | 0.5345416     |
|    mean_motor4          | 0.50913185    |
|    mean_motor5          | 0.61444414    |
|    mean_motor6          | 0.49302906    |
|    mean_motor7          | 0.7713287     |
| train/                  |               |
|    approx_kl            | 0.03138861    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0756        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.463         |
|    cost_value_loss      | 1.36e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.57         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00583       |
|    mean_cost_advantages | -9.455537e-05 |
|    mean_reward_advan... | -0.016146222  |
|    n_updates            | 3420          |
|    nu                   | 7.58          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00247      |
|    reward_explained_... | 0.862         |
|    reward_value_loss    | 0.0409        |
|    std                  | 0.429         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.59e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.84           |
|    forward_reward       | 0.45           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.6           |
|    reward_forward       | 0.45           |
|    reward_survive       | 1              |
|    x_position           | 3.56           |
|    x_velocity           | 0.45           |
|    y_position           | 6.83           |
|    y_velocity           | 0.48           |
| rollout/                |                |
|    adjusted_reward      | 7.04           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.29e+03       |
| time/                   |                |
|    fps                  | 822            |
|    iterations           | 173            |
|    time_elapsed         | 2153           |
|    total_timesteps      | 1771520        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10213          |
|    mean_motor0          | 0.532447       |
|    mean_motor1          | 0.47371978     |
|    mean_motor2          | 0.62684953     |
|    mean_motor3          | 0.5219803      |
|    mean_motor4          | 0.4897152      |
|    mean_motor5          | 0.6287806      |
|    mean_motor6          | 0.4980348      |
|    mean_motor7          | 0.79517066     |
| train/                  |                |
|    approx_kl            | 0.032930892    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0745         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.621          |
|    cost_value_loss      | 1.77e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.56          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0178         |
|    mean_cost_advantages | -5.4834432e-05 |
|    mean_reward_advan... | -0.021600826   |
|    n_updates            | 3440           |
|    nu                   | 7.58           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00191       |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.0364         |
|    std                  | 0.429          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 462          |
|    mean_reward          | 2.22e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.02         |
|    forward_reward       | 0.688        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.688        |
|    reward_survive       | 1            |
|    x_position           | 1.87         |
|    x_velocity           | 0.688        |
|    y_position           | 5.56         |
|    y_velocity           | 0.48         |
| rollout/                |              |
|    adjusted_reward      | 6.26         |
|    ep_len_mean          | 489          |
|    ep_rew_mean          | 3.21e+03     |
| time/                   |              |
|    fps                  | 822          |
|    iterations           | 174          |
|    time_elapsed         | 2166         |
|    total_timesteps      | 1781760      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10188        |
|    mean_motor0          | 0.5192911    |
|    mean_motor1          | 0.46323258   |
|    mean_motor2          | 0.6060573    |
|    mean_motor3          | 0.523046     |
|    mean_motor4          | 0.4814822    |
|    mean_motor5          | 0.615571     |
|    mean_motor6          | 0.50207376   |
|    mean_motor7          | 0.7471017    |
| train/                  |              |
|    approx_kl            | 0.031156873  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0712       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.332        |
|    cost_value_loss      | 9.96e-07     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -4.55        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00829      |
|    mean_cost_advantages | -0.000142798 |
|    mean_reward_advan... | -0.01456455  |
|    n_updates            | 3460         |
|    nu                   | 7.58         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00194     |
|    reward_explained_... | 0.89         |
|    reward_value_loss    | 0.0363       |
|    std                  | 0.429        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.34e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.24           |
|    forward_reward       | 0.265          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.51          |
|    reward_forward       | 0.265          |
|    reward_survive       | 1              |
|    x_position           | 4.16           |
|    x_velocity           | 0.265          |
|    y_position           | 5.74           |
|    y_velocity           | 0.159          |
| rollout/                |                |
|    adjusted_reward      | 7.05           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 3.26e+03       |
| time/                   |                |
|    fps                  | 822            |
|    iterations           | 175            |
|    time_elapsed         | 2179           |
|    total_timesteps      | 1792000        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10193          |
|    mean_motor0          | 0.5404965      |
|    mean_motor1          | 0.45771074     |
|    mean_motor2          | 0.6260003      |
|    mean_motor3          | 0.5236323      |
|    mean_motor4          | 0.45988995     |
|    mean_motor5          | 0.61363256     |
|    mean_motor6          | 0.5205939      |
|    mean_motor7          | 0.7528836      |
| train/                  |                |
|    approx_kl            | 0.03153034     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0746         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.313          |
|    cost_value_loss      | 1.55e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.54          |
|    learning_rate        | 3e-05          |
|    loss                 | -0.00341       |
|    mean_cost_advantages | -0.00011021646 |
|    mean_reward_advan... | -0.018557046   |
|    n_updates            | 3480           |
|    nu                   | 7.58           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00224       |
|    reward_explained_... | 0.928          |
|    reward_value_loss    | 0.0377         |
|    std                  | 0.427          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.97e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.6            |
|    forward_reward       | 0.391          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.57          |
|    reward_forward       | 0.391          |
|    reward_survive       | 1              |
|    x_position           | 4.99           |
|    x_velocity           | 0.391          |
|    y_position           | 8.02           |
|    y_velocity           | 0.313          |
| rollout/                |                |
|    adjusted_reward      | 6.37           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 3.21e+03       |
| time/                   |                |
|    fps                  | 821            |
|    iterations           | 176            |
|    time_elapsed         | 2192           |
|    total_timesteps      | 1802240        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10198          |
|    mean_motor0          | 0.55577695     |
|    mean_motor1          | 0.47321215     |
|    mean_motor2          | 0.62928075     |
|    mean_motor3          | 0.5183193      |
|    mean_motor4          | 0.47183973     |
|    mean_motor5          | 0.5964757      |
|    mean_motor6          | 0.5122417      |
|    mean_motor7          | 0.758043       |
| train/                  |                |
|    approx_kl            | 0.03749022     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0809         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.45           |
|    cost_value_loss      | 1.28e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.51          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0213         |
|    mean_cost_advantages | -0.00010742955 |
|    mean_reward_advan... | -0.014688812   |
|    n_updates            | 3500           |
|    nu                   | 7.58           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00256       |
|    reward_explained_... | 0.894          |
|    reward_value_loss    | 0.0392         |
|    std                  | 0.426          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 413            |
|    mean_reward          | 1.99e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.99           |
|    forward_reward       | 0.342          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.45          |
|    reward_forward       | 0.342          |
|    reward_survive       | 1              |
|    x_position           | 4.67           |
|    x_velocity           | 0.342          |
|    y_position           | 4.82           |
|    y_velocity           | 0.257          |
| rollout/                |                |
|    adjusted_reward      | 6.21           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 3.23e+03       |
| time/                   |                |
|    fps                  | 821            |
|    iterations           | 177            |
|    time_elapsed         | 2205           |
|    total_timesteps      | 1812480        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10186          |
|    mean_motor0          | 0.5284316      |
|    mean_motor1          | 0.44700462     |
|    mean_motor2          | 0.63343465     |
|    mean_motor3          | 0.4959631      |
|    mean_motor4          | 0.46665692     |
|    mean_motor5          | 0.58124864     |
|    mean_motor6          | 0.5167366      |
|    mean_motor7          | 0.7027639      |
| train/                  |                |
|    approx_kl            | 0.036521114    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.072          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.427          |
|    cost_value_loss      | 1.41e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.49          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0102         |
|    mean_cost_advantages | -8.6745604e-05 |
|    mean_reward_advan... | -0.0101189185  |
|    n_updates            | 3520           |
|    nu                   | 7.58           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00223       |
|    reward_explained_... | 0.932          |
|    reward_value_loss    | 0.0336         |
|    std                  | 0.425          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.95e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.23          |
|    forward_reward       | 0.489         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.49         |
|    reward_forward       | 0.489         |
|    reward_survive       | 1             |
|    x_position           | 5.56          |
|    x_velocity           | 0.489         |
|    y_position           | 5.84          |
|    y_velocity           | 0.408         |
| rollout/                |               |
|    adjusted_reward      | 6.38          |
|    ep_len_mean          | 493           |
|    ep_rew_mean          | 3.16e+03      |
| time/                   |               |
|    fps                  | 821           |
|    iterations           | 178           |
|    time_elapsed         | 2218          |
|    total_timesteps      | 1822720       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10207         |
|    mean_motor0          | 0.5422688     |
|    mean_motor1          | 0.4522903     |
|    mean_motor2          | 0.6530254     |
|    mean_motor3          | 0.5192233     |
|    mean_motor4          | 0.44593406    |
|    mean_motor5          | 0.58053243    |
|    mean_motor6          | 0.52803135    |
|    mean_motor7          | 0.7781055     |
| train/                  |               |
|    approx_kl            | 0.03612005    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0833        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.482         |
|    cost_value_loss      | 2.2e-06       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.48         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00538       |
|    mean_cost_advantages | -5.958524e-05 |
|    mean_reward_advan... | -0.0191795    |
|    n_updates            | 3540          |
|    nu                   | 7.58          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00231      |
|    reward_explained_... | 0.951         |
|    reward_value_loss    | 0.0409        |
|    std                  | 0.425         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.65e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.75           |
|    forward_reward       | 0.363          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.55          |
|    reward_forward       | 0.363          |
|    reward_survive       | 1              |
|    x_position           | 3.52           |
|    x_velocity           | 0.363          |
|    y_position           | 5.39           |
|    y_velocity           | 0.349          |
| rollout/                |                |
|    adjusted_reward      | 6.12           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 3.24e+03       |
| time/                   |                |
|    fps                  | 821            |
|    iterations           | 179            |
|    time_elapsed         | 2232           |
|    total_timesteps      | 1832960        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10194          |
|    mean_motor0          | 0.5336152      |
|    mean_motor1          | 0.472084       |
|    mean_motor2          | 0.62480056     |
|    mean_motor3          | 0.5288319      |
|    mean_motor4          | 0.45522094     |
|    mean_motor5          | 0.59019697     |
|    mean_motor6          | 0.5262759      |
|    mean_motor7          | 0.7221217      |
| train/                  |                |
|    approx_kl            | 0.0297959      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0594         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.558          |
|    cost_value_loss      | 1.4e-06        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.46          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0262         |
|    mean_cost_advantages | -1.1354884e-05 |
|    mean_reward_advan... | -0.01122025    |
|    n_updates            | 3560           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00202       |
|    reward_explained_... | 0.93           |
|    reward_value_loss    | 0.0368         |
|    std                  | 0.423          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.16e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.25         |
|    forward_reward       | 0.342        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.39        |
|    reward_forward       | 0.342        |
|    reward_survive       | 1            |
|    x_position           | 2.36         |
|    x_velocity           | 0.342        |
|    y_position           | 6.62         |
|    y_velocity           | 0.695        |
| rollout/                |              |
|    adjusted_reward      | 6.59         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 3.17e+03     |
| time/                   |              |
|    fps                  | 820          |
|    iterations           | 180          |
|    time_elapsed         | 2245         |
|    total_timesteps      | 1843200      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10202        |
|    mean_motor0          | 0.52544725   |
|    mean_motor1          | 0.46889296   |
|    mean_motor2          | 0.63236123   |
|    mean_motor3          | 0.5207279    |
|    mean_motor4          | 0.4557765    |
|    mean_motor5          | 0.57665205   |
|    mean_motor6          | 0.5407144    |
|    mean_motor7          | 0.748854     |
| train/                  |              |
|    approx_kl            | 0.027885502  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0604       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.874        |
|    cost_value_loss      | 2.15e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -4.44        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00955      |
|    mean_cost_advantages | 6.957319e-05 |
|    mean_reward_advan... | -0.008096107 |
|    n_updates            | 3580         |
|    nu                   | 7.59         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00218     |
|    reward_explained_... | 0.945        |
|    reward_value_loss    | 0.0314       |
|    std                  | 0.423        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.86e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.96          |
|    forward_reward       | 0.278         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.278         |
|    reward_survive       | 1             |
|    x_position           | 2.82          |
|    x_velocity           | 0.278         |
|    y_position           | 5.08          |
|    y_velocity           | 0.539         |
| rollout/                |               |
|    adjusted_reward      | 7.41          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 3.25e+03      |
| time/                   |               |
|    fps                  | 820           |
|    iterations           | 181           |
|    time_elapsed         | 2258          |
|    total_timesteps      | 1853440       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10206         |
|    mean_motor0          | 0.53654706    |
|    mean_motor1          | 0.47201866    |
|    mean_motor2          | 0.64810795    |
|    mean_motor3          | 0.5152503     |
|    mean_motor4          | 0.44718748    |
|    mean_motor5          | 0.5758327     |
|    mean_motor6          | 0.5278752     |
|    mean_motor7          | 0.77770776    |
| train/                  |               |
|    approx_kl            | 0.032105215   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0696        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.789         |
|    cost_value_loss      | 2.22e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.43         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0141        |
|    mean_cost_advantages | 4.5150246e-05 |
|    mean_reward_advan... | -7.028822e-05 |
|    n_updates            | 3600          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0025       |
|    reward_explained_... | 0.938         |
|    reward_value_loss    | 0.0286        |
|    std                  | 0.422         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 407           |
|    mean_reward          | 2.93e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.5           |
|    forward_reward       | 0.325         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.31         |
|    reward_forward       | 0.325         |
|    reward_survive       | 1             |
|    x_position           | 3.65          |
|    x_velocity           | 0.325         |
|    y_position           | 6.38          |
|    y_velocity           | 0.449         |
| rollout/                |               |
|    adjusted_reward      | 6.18          |
|    ep_len_mean          | 486           |
|    ep_rew_mean          | 3.21e+03      |
| time/                   |               |
|    fps                  | 820           |
|    iterations           | 182           |
|    time_elapsed         | 2271          |
|    total_timesteps      | 1863680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10204         |
|    mean_motor0          | 0.531395      |
|    mean_motor1          | 0.4602739     |
|    mean_motor2          | 0.6320274     |
|    mean_motor3          | 0.49920768    |
|    mean_motor4          | 0.45205194    |
|    mean_motor5          | 0.583439      |
|    mean_motor6          | 0.49005237    |
|    mean_motor7          | 0.79064643    |
| train/                  |               |
|    approx_kl            | 0.029449891   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0557        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.367         |
|    cost_value_loss      | 1.08e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.41         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00483       |
|    mean_cost_advantages | 3.2067633e-06 |
|    mean_reward_advan... | -0.0027816624 |
|    n_updates            | 3620          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00184      |
|    reward_explained_... | 0.894         |
|    reward_value_loss    | 0.0375        |
|    std                  | 0.421         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.03e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.69         |
|    forward_reward       | 0.273        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.37        |
|    reward_forward       | 0.273        |
|    reward_survive       | 1            |
|    x_position           | 1.75         |
|    x_velocity           | 0.273        |
|    y_position           | 4.22         |
|    y_velocity           | 0.549        |
| rollout/                |              |
|    adjusted_reward      | 6.43         |
|    ep_len_mean          | 482          |
|    ep_rew_mean          | 3.16e+03     |
| time/                   |              |
|    fps                  | 820          |
|    iterations           | 183          |
|    time_elapsed         | 2285         |
|    total_timesteps      | 1873920      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10206        |
|    mean_motor0          | 0.5359226    |
|    mean_motor1          | 0.447256     |
|    mean_motor2          | 0.6284772    |
|    mean_motor3          | 0.5330723    |
|    mean_motor4          | 0.4499249    |
|    mean_motor5          | 0.5748563    |
|    mean_motor6          | 0.5065551    |
|    mean_motor7          | 0.8237627    |
| train/                  |              |
|    approx_kl            | 0.033382278  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.787        |
|    cost_value_loss      | 1.76e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -4.4         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00821      |
|    mean_cost_advantages | 7.410809e-05 |
|    mean_reward_advan... | -0.026921874 |
|    n_updates            | 3640         |
|    nu                   | 7.59         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00222     |
|    reward_explained_... | 0.926        |
|    reward_value_loss    | 0.0392       |
|    std                  | 0.42         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 411           |
|    mean_reward          | 2.71e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.49          |
|    forward_reward       | 0.393         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.393         |
|    reward_survive       | 1             |
|    x_position           | 3.49          |
|    x_velocity           | 0.393         |
|    y_position           | 6.43          |
|    y_velocity           | 0.593         |
| rollout/                |               |
|    adjusted_reward      | 6.13          |
|    ep_len_mean          | 477           |
|    ep_rew_mean          | 3.11e+03      |
| time/                   |               |
|    fps                  | 819           |
|    iterations           | 184           |
|    time_elapsed         | 2298          |
|    total_timesteps      | 1884160       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10193         |
|    mean_motor0          | 0.50793624    |
|    mean_motor1          | 0.46587586    |
|    mean_motor2          | 0.58952904    |
|    mean_motor3          | 0.5061968     |
|    mean_motor4          | 0.45742935    |
|    mean_motor5          | 0.5635766     |
|    mean_motor6          | 0.5079535     |
|    mean_motor7          | 0.80623233    |
| train/                  |               |
|    approx_kl            | 0.035988927   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0867        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.426         |
|    cost_value_loss      | 1.33e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.37         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0269        |
|    mean_cost_advantages | -1.538007e-05 |
|    mean_reward_advan... | -0.03240968   |
|    n_updates            | 3660          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00226      |
|    reward_explained_... | 0.881         |
|    reward_value_loss    | 0.0467        |
|    std                  | 0.418         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.38e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.24          |
|    forward_reward       | 0.357         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.357         |
|    reward_survive       | 1             |
|    x_position           | 1.89          |
|    x_velocity           | 0.357         |
|    y_position           | 5.53          |
|    y_velocity           | 0.448         |
| rollout/                |               |
|    adjusted_reward      | 7.4           |
|    ep_len_mean          | 472           |
|    ep_rew_mean          | 3.13e+03      |
| time/                   |               |
|    fps                  | 819           |
|    iterations           | 185           |
|    time_elapsed         | 2311          |
|    total_timesteps      | 1894400       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10211         |
|    mean_motor0          | 0.54395634    |
|    mean_motor1          | 0.4527584     |
|    mean_motor2          | 0.62823427    |
|    mean_motor3          | 0.5364144     |
|    mean_motor4          | 0.44550252    |
|    mean_motor5          | 0.5727908     |
|    mean_motor6          | 0.53636384    |
|    mean_motor7          | 0.8885864     |
| train/                  |               |
|    approx_kl            | 0.03127999    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0724        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.262         |
|    cost_value_loss      | 1.05e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.35         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0119        |
|    mean_cost_advantages | -8.011317e-06 |
|    mean_reward_advan... | -0.014738825  |
|    n_updates            | 3680          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00209      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.0329        |
|    std                  | 0.418         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.01e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.02           |
|    forward_reward       | 0.404          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.58          |
|    reward_forward       | 0.404          |
|    reward_survive       | 1              |
|    x_position           | 1.37           |
|    x_velocity           | 0.404          |
|    y_position           | 6.76           |
|    y_velocity           | 0.773          |
| rollout/                |                |
|    adjusted_reward      | 6.24           |
|    ep_len_mean          | 477            |
|    ep_rew_mean          | 3.12e+03       |
| time/                   |                |
|    fps                  | 819            |
|    iterations           | 186            |
|    time_elapsed         | 2324           |
|    total_timesteps      | 1904640        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10197          |
|    mean_motor0          | 0.5254053      |
|    mean_motor1          | 0.47319898     |
|    mean_motor2          | 0.583878       |
|    mean_motor3          | 0.4986201      |
|    mean_motor4          | 0.47224775     |
|    mean_motor5          | 0.5650861      |
|    mean_motor6          | 0.5093291      |
|    mean_motor7          | 0.7976556      |
| train/                  |                |
|    approx_kl            | 0.0346777      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0621         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.212          |
|    cost_value_loss      | 7.13e-07       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.34          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0476         |
|    mean_cost_advantages | -1.9993016e-05 |
|    mean_reward_advan... | -0.024623951   |
|    n_updates            | 3700           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00172       |
|    reward_explained_... | 0.808          |
|    reward_value_loss    | 0.0486         |
|    std                  | 0.417          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 426           |
|    mean_reward          | 2.47e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.64          |
|    forward_reward       | 0.259         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.49         |
|    reward_forward       | 0.259         |
|    reward_survive       | 1             |
|    x_position           | 3.41          |
|    x_velocity           | 0.259         |
|    y_position           | 5.59          |
|    y_velocity           | 0.472         |
| rollout/                |               |
|    adjusted_reward      | 6.51          |
|    ep_len_mean          | 486           |
|    ep_rew_mean          | 3.19e+03      |
| time/                   |               |
|    fps                  | 819           |
|    iterations           | 187           |
|    time_elapsed         | 2337          |
|    total_timesteps      | 1914880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10210         |
|    mean_motor0          | 0.53672403    |
|    mean_motor1          | 0.45824847    |
|    mean_motor2          | 0.58495       |
|    mean_motor3          | 0.50752676    |
|    mean_motor4          | 0.47613844    |
|    mean_motor5          | 0.56964105    |
|    mean_motor6          | 0.52794445    |
|    mean_motor7          | 0.85697204    |
| train/                  |               |
|    approx_kl            | 0.03563485    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0842        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.715         |
|    cost_value_loss      | 2.56e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.33         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0141        |
|    mean_cost_advantages | 4.0220253e-05 |
|    mean_reward_advan... | -0.022432085  |
|    n_updates            | 3720          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00219      |
|    reward_explained_... | 0.932         |
|    reward_value_loss    | 0.0373        |
|    std                  | 0.417         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.67e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.96           |
|    forward_reward       | 0.332          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.32          |
|    reward_forward       | 0.332          |
|    reward_survive       | 1              |
|    x_position           | 3.25           |
|    x_velocity           | 0.332          |
|    y_position           | 7.2            |
|    y_velocity           | 0.223          |
| rollout/                |                |
|    adjusted_reward      | 5.89           |
|    ep_len_mean          | 485            |
|    ep_rew_mean          | 3.14e+03       |
| time/                   |                |
|    fps                  | 818            |
|    iterations           | 188            |
|    time_elapsed         | 2350           |
|    total_timesteps      | 1925120        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10194          |
|    mean_motor0          | 0.5201307      |
|    mean_motor1          | 0.43900055     |
|    mean_motor2          | 0.5851636      |
|    mean_motor3          | 0.49531102     |
|    mean_motor4          | 0.4653836      |
|    mean_motor5          | 0.598642       |
|    mean_motor6          | 0.50386095     |
|    mean_motor7          | 0.793386       |
| train/                  |                |
|    approx_kl            | 0.03181959     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0603         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.864          |
|    cost_value_loss      | 1.49e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.31          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0118         |
|    mean_cost_advantages | 0.000104521365 |
|    mean_reward_advan... | -0.023567703   |
|    n_updates            | 3740           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00199       |
|    reward_explained_... | 0.933          |
|    reward_value_loss    | 0.044          |
|    std                  | 0.415          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 326          |
|    mean_reward          | 2.16e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 10.9         |
|    forward_reward       | 0.196        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.5         |
|    reward_forward       | 0.196        |
|    reward_survive       | 1            |
|    x_position           | 3.43         |
|    x_velocity           | 0.196        |
|    y_position           | 10.3         |
|    y_velocity           | 0.258        |
| rollout/                |              |
|    adjusted_reward      | 6.89         |
|    ep_len_mean          | 485          |
|    ep_rew_mean          | 3.16e+03     |
| time/                   |              |
|    fps                  | 818          |
|    iterations           | 189          |
|    time_elapsed         | 2363         |
|    total_timesteps      | 1935360      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10206        |
|    mean_motor0          | 0.51118916   |
|    mean_motor1          | 0.45210472   |
|    mean_motor2          | 0.6088141    |
|    mean_motor3          | 0.50711864   |
|    mean_motor4          | 0.4672475    |
|    mean_motor5          | 0.5993439    |
|    mean_motor6          | 0.53116745   |
|    mean_motor7          | 0.81874275   |
| train/                  |              |
|    approx_kl            | 0.032296997  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0786       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.562        |
|    cost_value_loss      | 3.43e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -4.28        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00939      |
|    mean_cost_advantages | 6.082461e-05 |
|    mean_reward_advan... | -0.01885714  |
|    n_updates            | 3760         |
|    nu                   | 7.59         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00231     |
|    reward_explained_... | 0.949        |
|    reward_value_loss    | 0.0347       |
|    std                  | 0.414        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.06e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.35           |
|    forward_reward       | 0.174          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.45          |
|    reward_forward       | 0.174          |
|    reward_survive       | 1              |
|    x_position           | 3.25           |
|    x_velocity           | 0.174          |
|    y_position           | 8.56           |
|    y_velocity           | 0.226          |
| rollout/                |                |
|    adjusted_reward      | 6.98           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 3.15e+03       |
| time/                   |                |
|    fps                  | 818            |
|    iterations           | 190            |
|    time_elapsed         | 2376           |
|    total_timesteps      | 1945600        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10196          |
|    mean_motor0          | 0.5153759      |
|    mean_motor1          | 0.457244       |
|    mean_motor2          | 0.61256486     |
|    mean_motor3          | 0.49218923     |
|    mean_motor4          | 0.46345288     |
|    mean_motor5          | 0.5799968      |
|    mean_motor6          | 0.52506727     |
|    mean_motor7          | 0.8657859      |
| train/                  |                |
|    approx_kl            | 0.03222079     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0686         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.214          |
|    cost_value_loss      | 9e-07          |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.26          |
|    learning_rate        | 3e-05          |
|    loss                 | -0.000223      |
|    mean_cost_advantages | -3.3235323e-05 |
|    mean_reward_advan... | -0.0062388107  |
|    n_updates            | 3780           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00217       |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 0.0309         |
|    std                  | 0.413          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.49e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.04           |
|    forward_reward       | 0.157          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.5           |
|    reward_forward       | 0.157          |
|    reward_survive       | 1              |
|    x_position           | 3.42           |
|    x_velocity           | 0.157          |
|    y_position           | 8.03           |
|    y_velocity           | 0.164          |
| rollout/                |                |
|    adjusted_reward      | 6.41           |
|    ep_len_mean          | 480            |
|    ep_rew_mean          | 3.11e+03       |
| time/                   |                |
|    fps                  | 818            |
|    iterations           | 191            |
|    time_elapsed         | 2389           |
|    total_timesteps      | 1955840        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10192          |
|    mean_motor0          | 0.5066266      |
|    mean_motor1          | 0.4677523      |
|    mean_motor2          | 0.5985015      |
|    mean_motor3          | 0.4938871      |
|    mean_motor4          | 0.46019936     |
|    mean_motor5          | 0.58786833     |
|    mean_motor6          | 0.49948144     |
|    mean_motor7          | 0.799722       |
| train/                  |                |
|    approx_kl            | 0.035666123    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0706         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.401          |
|    cost_value_loss      | 1.55e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.25          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0161         |
|    mean_cost_advantages | -1.2460619e-05 |
|    mean_reward_advan... | -0.011782513   |
|    n_updates            | 3800           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00207       |
|    reward_explained_... | 0.91           |
|    reward_value_loss    | 0.0366         |
|    std                  | 0.413          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.72e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.52          |
|    forward_reward       | 0.258         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.5          |
|    reward_forward       | 0.258         |
|    reward_survive       | 1             |
|    x_position           | 3.18          |
|    x_velocity           | 0.258         |
|    y_position           | 7.76          |
|    y_velocity           | 0.195         |
| rollout/                |               |
|    adjusted_reward      | 6.46          |
|    ep_len_mean          | 484           |
|    ep_rew_mean          | 3.17e+03      |
| time/                   |               |
|    fps                  | 818           |
|    iterations           | 192           |
|    time_elapsed         | 2403          |
|    total_timesteps      | 1966080       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10199         |
|    mean_motor0          | 0.50553024    |
|    mean_motor1          | 0.46355334    |
|    mean_motor2          | 0.6087421     |
|    mean_motor3          | 0.497411      |
|    mean_motor4          | 0.45616618    |
|    mean_motor5          | 0.59765905    |
|    mean_motor6          | 0.49552828    |
|    mean_motor7          | 0.7966413     |
| train/                  |               |
|    approx_kl            | 0.03896851    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0831        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.714         |
|    cost_value_loss      | 2.34e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.23         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00393       |
|    mean_cost_advantages | -1.613241e-05 |
|    mean_reward_advan... | -0.018815652  |
|    n_updates            | 3820          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00215      |
|    reward_explained_... | 0.95          |
|    reward_value_loss    | 0.0424        |
|    std                  | 0.411         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.59e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.31          |
|    forward_reward       | 0.228         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.54         |
|    reward_forward       | 0.228         |
|    reward_survive       | 1             |
|    x_position           | 3.92          |
|    x_velocity           | 0.228         |
|    y_position           | 7.29          |
|    y_velocity           | 0.297         |
| rollout/                |               |
|    adjusted_reward      | 6.43          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.23e+03      |
| time/                   |               |
|    fps                  | 817           |
|    iterations           | 193           |
|    time_elapsed         | 2416          |
|    total_timesteps      | 1976320       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10194         |
|    mean_motor0          | 0.51191485    |
|    mean_motor1          | 0.43990174    |
|    mean_motor2          | 0.60868543    |
|    mean_motor3          | 0.50120825    |
|    mean_motor4          | 0.45841938    |
|    mean_motor5          | 0.58641714    |
|    mean_motor6          | 0.49714977    |
|    mean_motor7          | 0.7617923     |
| train/                  |               |
|    approx_kl            | 0.039320074   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0878        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.84          |
|    cost_value_loss      | 3.53e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.2          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00436       |
|    mean_cost_advantages | -3.321021e-05 |
|    mean_reward_advan... | -0.012910282  |
|    n_updates            | 3840          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00242      |
|    reward_explained_... | 0.944         |
|    reward_value_loss    | 0.0372        |
|    std                  | 0.41          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.71e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.77          |
|    forward_reward       | 0.356         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.356         |
|    reward_survive       | 1             |
|    x_position           | 2.31          |
|    x_velocity           | 0.356         |
|    y_position           | 6.28          |
|    y_velocity           | 0.412         |
| rollout/                |               |
|    adjusted_reward      | 7.25          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.24e+03      |
| time/                   |               |
|    fps                  | 817           |
|    iterations           | 194           |
|    time_elapsed         | 2429          |
|    total_timesteps      | 1986560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10214         |
|    mean_motor0          | 0.5630969     |
|    mean_motor1          | 0.46061277    |
|    mean_motor2          | 0.61512697    |
|    mean_motor3          | 0.51442194    |
|    mean_motor4          | 0.46344027    |
|    mean_motor5          | 0.5856458     |
|    mean_motor6          | 0.5202455     |
|    mean_motor7          | 0.8225099     |
| train/                  |               |
|    approx_kl            | 0.03664425    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0767        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.559         |
|    cost_value_loss      | 1.16e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.18         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00379       |
|    mean_cost_advantages | 4.8435282e-05 |
|    mean_reward_advan... | -0.011622159  |
|    n_updates            | 3860          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00258      |
|    reward_explained_... | 0.931         |
|    reward_value_loss    | 0.031         |
|    std                  | 0.409         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.48e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.51          |
|    forward_reward       | 0.512         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.61         |
|    reward_forward       | 0.512         |
|    reward_survive       | 1             |
|    x_position           | 1.32          |
|    x_velocity           | 0.512         |
|    y_position           | 4.13          |
|    y_velocity           | 1.44          |
| rollout/                |               |
|    adjusted_reward      | 6.41          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.22e+03      |
| time/                   |               |
|    fps                  | 817           |
|    iterations           | 195           |
|    time_elapsed         | 2443          |
|    total_timesteps      | 1996800       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10199         |
|    mean_motor0          | 0.5661502     |
|    mean_motor1          | 0.46983415    |
|    mean_motor2          | 0.59307986    |
|    mean_motor3          | 0.5131081     |
|    mean_motor4          | 0.47647285    |
|    mean_motor5          | 0.58634245    |
|    mean_motor6          | 0.5116898     |
|    mean_motor7          | 0.7756595     |
| train/                  |               |
|    approx_kl            | 0.042238913   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.444         |
|    cost_value_loss      | 7.24e-07      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.15         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0181        |
|    mean_cost_advantages | -2.689825e-05 |
|    mean_reward_advan... | -0.02498362   |
|    n_updates            | 3880          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0025       |
|    reward_explained_... | 0.849         |
|    reward_value_loss    | 0.0494        |
|    std                  | 0.408         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.97e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.02           |
|    forward_reward       | 0.58           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.49          |
|    reward_forward       | 0.58           |
|    reward_survive       | 1              |
|    x_position           | 1.27           |
|    x_velocity           | 0.58           |
|    y_position           | 5.74           |
|    y_velocity           | 1.09           |
| rollout/                |                |
|    adjusted_reward      | 6.51           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.27e+03       |
| time/                   |                |
|    fps                  | 816            |
|    iterations           | 196            |
|    time_elapsed         | 2456           |
|    total_timesteps      | 2007040        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10199          |
|    mean_motor0          | 0.5369991      |
|    mean_motor1          | 0.44294113     |
|    mean_motor2          | 0.5830757      |
|    mean_motor3          | 0.5191657      |
|    mean_motor4          | 0.45067698     |
|    mean_motor5          | 0.5785583      |
|    mean_motor6          | 0.50674987     |
|    mean_motor7          | 0.8066948      |
| train/                  |                |
|    approx_kl            | 0.034936983    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.076          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.563          |
|    cost_value_loss      | 1.12e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.12          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0456         |
|    mean_cost_advantages | -9.4641855e-06 |
|    mean_reward_advan... | -0.016510217   |
|    n_updates            | 3900           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00248       |
|    reward_explained_... | 0.92           |
|    reward_value_loss    | 0.0361         |
|    std                  | 0.405          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 411           |
|    mean_reward          | 2.94e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.53          |
|    forward_reward       | 0.19          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.4          |
|    reward_forward       | 0.19          |
|    reward_survive       | 1             |
|    x_position           | 2.63          |
|    x_velocity           | 0.19          |
|    y_position           | 7.01          |
|    y_velocity           | 0.289         |
| rollout/                |               |
|    adjusted_reward      | 7.65          |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 3.37e+03      |
| time/                   |               |
|    fps                  | 816           |
|    iterations           | 197           |
|    time_elapsed         | 2469          |
|    total_timesteps      | 2017280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10210         |
|    mean_motor0          | 0.552528      |
|    mean_motor1          | 0.43760896    |
|    mean_motor2          | 0.5971652     |
|    mean_motor3          | 0.54438275    |
|    mean_motor4          | 0.44290924    |
|    mean_motor5          | 0.57858336    |
|    mean_motor6          | 0.51749694    |
|    mean_motor7          | 0.8625761     |
| train/                  |               |
|    approx_kl            | 0.033683427   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0652        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.509         |
|    cost_value_loss      | 8.31e-07      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -4.08         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00817       |
|    mean_cost_advantages | -5.877841e-05 |
|    mean_reward_advan... | -0.022605663  |
|    n_updates            | 3920          |
|    nu                   | 7.59          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00202      |
|    reward_explained_... | 0.935         |
|    reward_value_loss    | 0.0365        |
|    std                  | 0.404         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 500            |
|    mean_reward          | 3e+03          |
| infos/                  |                |
|    cost                 | 0.0111         |
|    distance_from_origin | 7.07           |
|    forward_reward       | 0.559          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.49          |
|    reward_forward       | 0.559          |
|    reward_survive       | 1              |
|    x_position           | 1.6            |
|    x_velocity           | 0.559          |
|    y_position           | 6.41           |
|    y_velocity           | 0.722          |
| rollout/                |                |
|    adjusted_reward      | 7.19           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.45e+03       |
| time/                   |                |
|    fps                  | 816            |
|    iterations           | 198            |
|    time_elapsed         | 2482           |
|    total_timesteps      | 2027520        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10205          |
|    mean_motor0          | 0.5399445      |
|    mean_motor1          | 0.46425563     |
|    mean_motor2          | 0.5738563      |
|    mean_motor3          | 0.5586928      |
|    mean_motor4          | 0.44418544     |
|    mean_motor5          | 0.576312       |
|    mean_motor6          | 0.51495695     |
|    mean_motor7          | 0.81078655     |
| train/                  |                |
|    approx_kl            | 0.03130856     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0815         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.25           |
|    cost_value_loss      | 4.78e-07       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.05          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00735        |
|    mean_cost_advantages | -7.7944074e-05 |
|    mean_reward_advan... | -0.013667325   |
|    n_updates            | 3940           |
|    nu                   | 7.59           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00184       |
|    reward_explained_... | 0.863          |
|    reward_value_loss    | 0.0389         |
|    std                  | 0.403          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.9e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.05         |
|    forward_reward       | 0.238        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.37        |
|    reward_forward       | 0.238        |
|    reward_survive       | 1            |
|    x_position           | 1.29         |
|    x_velocity           | 0.238        |
|    y_position           | 7.68         |
|    y_velocity           | 0.431        |
| rollout/                |              |
|    adjusted_reward      | 7.01         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 3.39e+03     |
| time/                   |              |
|    fps                  | 816          |
|    iterations           | 199          |
|    time_elapsed         | 2496         |
|    total_timesteps      | 2037760      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10198        |
|    mean_motor0          | 0.5366788    |
|    mean_motor1          | 0.47265777   |
|    mean_motor2          | 0.5682805    |
|    mean_motor3          | 0.52345085   |
|    mean_motor4          | 0.4577915    |
|    mean_motor5          | 0.5979881    |
|    mean_motor6          | 0.49073252   |
|    mean_motor7          | 0.7390105    |
| train/                  |              |
|    approx_kl            | 0.04267266   |
|    average_cost         | 0.005566406  |
|    clip_fraction        | 0.0964       |
|    clip_range           | 0.4          |
|    cost_explained_va... | -40.7        |
|    cost_value_loss      | 0.00156      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -4.03        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00944      |
|    mean_cost_advantages | 0.007883063  |
|    mean_reward_advan... | -0.018272325 |
|    n_updates            | 3960         |
|    nu                   | 7.59         |
|    nu_loss              | -0.0423      |
|    policy_gradient_loss | -0.00281     |
|    reward_explained_... | 0.923        |
|    reward_value_loss    | 0.0469       |
|    std                  | 0.402        |
|    total_cost           | 57.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 414            |
|    mean_reward          | 3.3e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.57           |
|    forward_reward       | 0.125          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.52          |
|    reward_forward       | 0.125          |
|    reward_survive       | 1              |
|    x_position           | 1.39           |
|    x_velocity           | 0.125          |
|    y_position           | 7.02           |
|    y_velocity           | 0.206          |
| rollout/                |                |
|    adjusted_reward      | 6.88           |
|    ep_len_mean          | 485            |
|    ep_rew_mean          | 3.42e+03       |
| time/                   |                |
|    fps                  | 816            |
|    iterations           | 200            |
|    time_elapsed         | 2509           |
|    total_timesteps      | 2048000        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10204          |
|    mean_motor0          | 0.5538351      |
|    mean_motor1          | 0.46491766     |
|    mean_motor2          | 0.5637399      |
|    mean_motor3          | 0.53586614     |
|    mean_motor4          | 0.47453412     |
|    mean_motor5          | 0.61890405     |
|    mean_motor6          | 0.48286623     |
|    mean_motor7          | 0.7584572      |
| train/                  |                |
|    approx_kl            | 0.032709323    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0704         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.725          |
|    cost_value_loss      | 9.99e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.02          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0121         |
|    mean_cost_advantages | -0.00064145704 |
|    mean_reward_advan... | -0.012596577   |
|    n_updates            | 3980           |
|    nu                   | 7.6            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00221       |
|    reward_explained_... | 0.942          |
|    reward_value_loss    | 0.0407         |
|    std                  | 0.402          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 466            |
|    mean_reward          | 3.38e+03       |
| infos/                  |                |
|    cost                 | 0.0315         |
|    distance_from_origin | 9.6            |
|    forward_reward       | 0.234          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.43          |
|    reward_forward       | 0.234          |
|    reward_survive       | 1              |
|    x_position           | 1.58           |
|    x_velocity           | 0.234          |
|    y_position           | 8.99           |
|    y_velocity           | 0.377          |
| rollout/                |                |
|    adjusted_reward      | 7.69           |
|    ep_len_mean          | 485            |
|    ep_rew_mean          | 3.52e+03       |
| time/                   |                |
|    fps                  | 816            |
|    iterations           | 201            |
|    time_elapsed         | 2522           |
|    total_timesteps      | 2058240        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10202          |
|    mean_motor0          | 0.55820215     |
|    mean_motor1          | 0.50279605     |
|    mean_motor2          | 0.5791867      |
|    mean_motor3          | 0.56271434     |
|    mean_motor4          | 0.4608074      |
|    mean_motor5          | 0.5915925      |
|    mean_motor6          | 0.47236443     |
|    mean_motor7          | 0.7919506      |
| train/                  |                |
|    approx_kl            | 0.037255853    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0918         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.865          |
|    cost_value_loss      | 1.29e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -4.01          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0265         |
|    mean_cost_advantages | -0.00027974055 |
|    mean_reward_advan... | -0.025538212   |
|    n_updates            | 4000           |
|    nu                   | 7.6            |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00254       |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.0439         |
|    std                  | 0.401          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 422         |
|    mean_reward          | 2.6e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 9.85        |
|    forward_reward       | 0.175       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.57       |
|    reward_forward       | 0.175       |
|    reward_survive       | 1           |
|    x_position           | -0.567      |
|    x_velocity           | 0.175       |
|    y_position           | 9.77        |
|    y_velocity           | 0.397       |
| rollout/                |             |
|    adjusted_reward      | 6.26        |
|    ep_len_mean          | 477         |
|    ep_rew_mean          | 3.34e+03    |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 202         |
|    time_elapsed         | 2535        |
|    total_timesteps      | 2068480     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10196       |
|    mean_motor0          | 0.5580722   |
|    mean_motor1          | 0.42317587  |
|    mean_motor2          | 0.6134757   |
|    mean_motor3          | 0.4858481   |
|    mean_motor4          | 0.47272095  |
|    mean_motor5          | 0.5964469   |
|    mean_motor6          | 0.50245035  |
|    mean_motor7          | 0.70434463  |
| train/                  |             |
|    approx_kl            | 0.4636859   |
|    average_cost         | 0.04121094  |
|    clip_fraction        | 0.377       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.682       |
|    cost_value_loss      | 0.0175      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -4          |
|    learning_rate        | 3e-05       |
|    loss                 | 0.016       |
|    mean_cost_advantages | 0.052154772 |
|    mean_reward_advan... | -0.02105169 |
|    n_updates            | 4020        |
|    nu                   | 7.63        |
|    nu_loss              | -0.313      |
|    policy_gradient_loss | -0.0113     |
|    reward_explained_... | 0.909       |
|    reward_value_loss    | 0.0572      |
|    std                  | 0.401       |
|    total_cost           | 422.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.66e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 10.2        |
|    forward_reward       | 0.412       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.44       |
|    reward_forward       | 0.412       |
|    reward_survive       | 1           |
|    x_position           | 4.28        |
|    x_velocity           | 0.412       |
|    y_position           | 9.16        |
|    y_velocity           | 0.406       |
| rollout/                |             |
|    adjusted_reward      | 8.58        |
|    ep_len_mean          | 473         |
|    ep_rew_mean          | 3.47e+03    |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 203         |
|    time_elapsed         | 2548        |
|    total_timesteps      | 2078720     |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10200       |
|    mean_motor0          | 0.49775964  |
|    mean_motor1          | 0.46489674  |
|    mean_motor2          | 0.61932147  |
|    mean_motor3          | 0.522993    |
|    mean_motor4          | 0.44735384  |
|    mean_motor5          | 0.6332512   |
|    mean_motor6          | 0.46914166  |
|    mean_motor7          | 0.7759897   |
| train/                  |             |
|    approx_kl            | 0.7055136   |
|    average_cost         | 0.039941408 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.765       |
|    cost_value_loss      | 0.025       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -4          |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0258     |
|    mean_cost_advantages | 0.045092758 |
|    mean_reward_advan... | -0.03234134 |
|    n_updates            | 4040        |
|    nu                   | 7.67        |
|    nu_loss              | -0.305      |
|    policy_gradient_loss | -0.0123     |
|    reward_explained_... | 0.936       |
|    reward_value_loss    | 0.0471      |
|    std                  | 0.4         |
|    total_cost           | 409.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.8e+03     |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.04        |
|    forward_reward       | 0.22        |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.26       |
|    reward_forward       | 0.22        |
|    reward_survive       | 1           |
|    x_position           | 0.468       |
|    x_velocity           | 0.22        |
|    y_position           | 4.8         |
|    y_velocity           | 0.167       |
| rollout/                |             |
|    adjusted_reward      | 7.56        |
|    ep_len_mean          | 474         |
|    ep_rew_mean          | 3.55e+03    |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 204         |
|    time_elapsed         | 2562        |
|    total_timesteps      | 2088960     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10185       |
|    mean_motor0          | 0.53735477  |
|    mean_motor1          | 0.43925944  |
|    mean_motor2          | 0.58235383  |
|    mean_motor3          | 0.47271895  |
|    mean_motor4          | 0.47505203  |
|    mean_motor5          | 0.64739674  |
|    mean_motor6          | 0.4921283   |
|    mean_motor7          | 0.7238498   |
| train/                  |             |
|    approx_kl            | 0.054421544 |
|    average_cost         | 0.011523438 |
|    clip_fraction        | 0.0836      |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.808       |
|    cost_value_loss      | 0.00667     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -3.97       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.00636    |
|    mean_cost_advantages | 0.011672126 |
|    mean_reward_advan... | 0.040082864 |
|    n_updates            | 4060        |
|    nu                   | 7.72        |
|    nu_loss              | -0.0884     |
|    policy_gradient_loss | -0.00339    |
|    reward_explained_... | 0.941       |
|    reward_value_loss    | 0.0256      |
|    std                  | 0.398       |
|    total_cost           | 118.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.1e+03     |
|    mean_ep_length       | 500         |
|    mean_reward          | 3.69e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 7.48        |
|    forward_reward       | 0.253       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.47       |
|    reward_forward       | 0.253       |
|    reward_survive       | 1           |
|    x_position           | 2.39        |
|    x_velocity           | 0.253       |
|    y_position           | 6.52        |
|    y_velocity           | 0.86        |
| rollout/                |             |
|    adjusted_reward      | 5.86        |
|    ep_len_mean          | 479         |
|    ep_rew_mean          | 3.44e+03    |
| time/                   |             |
|    fps                  | 815         |
|    iterations           | 205         |
|    time_elapsed         | 2575        |
|    total_timesteps      | 2099200     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10176       |
|    mean_motor0          | 0.5488385   |
|    mean_motor1          | 0.4562672   |
|    mean_motor2          | 0.6215328   |
|    mean_motor3          | 0.4957549   |
|    mean_motor4          | 0.45807773  |
|    mean_motor5          | 0.61303747  |
|    mean_motor6          | 0.48849258  |
|    mean_motor7          | 0.68650067  |
| train/                  |             |
|    approx_kl            | 0.153546    |
|    average_cost         | 0.04042969  |
|    clip_fraction        | 0.181       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.908       |
|    cost_value_loss      | 0.0123      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -3.95       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00395     |
|    mean_cost_advantages | 0.0434876   |
|    mean_reward_advan... | 0.032806225 |
|    n_updates            | 4080        |
|    nu                   | 7.79        |
|    nu_loss              | -0.312      |
|    policy_gradient_loss | -0.00744    |
|    reward_explained_... | 0.926       |
|    reward_value_loss    | 0.0306      |
|    std                  | 0.398       |
|    total_cost           | 414.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.1e+03      |
|    mean_ep_length       | 409          |
|    mean_reward          | 2.34e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.96         |
|    forward_reward       | 0.31         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.59        |
|    reward_forward       | 0.31         |
|    reward_survive       | 1            |
|    x_position           | 2.77         |
|    x_velocity           | 0.31         |
|    y_position           | 6.09         |
|    y_velocity           | 0.701        |
| rollout/                |              |
|    adjusted_reward      | 6.29         |
|    ep_len_mean          | 475          |
|    ep_rew_mean          | 3.29e+03     |
| time/                   |              |
|    fps                  | 815          |
|    iterations           | 206          |
|    time_elapsed         | 2588         |
|    total_timesteps      | 2109440      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10187        |
|    mean_motor0          | 0.56783307   |
|    mean_motor1          | 0.4294178    |
|    mean_motor2          | 0.596031     |
|    mean_motor3          | 0.48758656   |
|    mean_motor4          | 0.46705452   |
|    mean_motor5          | 0.62307847   |
|    mean_motor6          | 0.49922055   |
|    mean_motor7          | 0.71608865   |
| train/                  |              |
|    approx_kl            | 0.049574412  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0877       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.894        |
|    cost_value_loss      | 0.003        |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.93        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00863      |
|    mean_cost_advantages | -0.004339179 |
|    mean_reward_advan... | -0.009536701 |
|    n_updates            | 4100         |
|    nu                   | 7.85         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0037      |
|    reward_explained_... | 0.938        |
|    reward_value_loss    | 0.0258       |
|    std                  | 0.397        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 413           |
|    mean_reward          | 2.43e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.14          |
|    forward_reward       | 0.347         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.56         |
|    reward_forward       | 0.347         |
|    reward_survive       | 1             |
|    x_position           | 1.99          |
|    x_velocity           | 0.347         |
|    y_position           | 8.83          |
|    y_velocity           | 0.508         |
| rollout/                |               |
|    adjusted_reward      | 7.78          |
|    ep_len_mean          | 479           |
|    ep_rew_mean          | 3.43e+03      |
| time/                   |               |
|    fps                  | 814           |
|    iterations           | 207           |
|    time_elapsed         | 2601          |
|    total_timesteps      | 2119680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10189         |
|    mean_motor0          | 0.54258883    |
|    mean_motor1          | 0.4208251     |
|    mean_motor2          | 0.63838744    |
|    mean_motor3          | 0.50382024    |
|    mean_motor4          | 0.44644985    |
|    mean_motor5          | 0.6357712     |
|    mean_motor6          | 0.4675304     |
|    mean_motor7          | 0.76177716    |
| train/                  |               |
|    approx_kl            | 0.044402048   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0774        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.95          |
|    cost_value_loss      | 0.000802      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.91         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00131       |
|    mean_cost_advantages | -0.0038775373 |
|    mean_reward_advan... | -0.0038514812 |
|    n_updates            | 4120          |
|    nu                   | 7.91          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00273      |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.029         |
|    std                  | 0.396         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.1e+03        |
|    mean_ep_length       | 425            |
|    mean_reward          | 2.81e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.59           |
|    forward_reward       | 0.38           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.43          |
|    reward_forward       | 0.38           |
|    reward_survive       | 1              |
|    x_position           | 1.12           |
|    x_velocity           | 0.38           |
|    y_position           | 4.21           |
|    y_velocity           | 0.455          |
| rollout/                |                |
|    adjusted_reward      | 6.89           |
|    ep_len_mean          | 474            |
|    ep_rew_mean          | 3.22e+03       |
| time/                   |                |
|    fps                  | 814            |
|    iterations           | 208            |
|    time_elapsed         | 2613           |
|    total_timesteps      | 2129920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10185          |
|    mean_motor0          | 0.5362222      |
|    mean_motor1          | 0.42288318     |
|    mean_motor2          | 0.6147164      |
|    mean_motor3          | 0.47950405     |
|    mean_motor4          | 0.4560334      |
|    mean_motor5          | 0.61396426     |
|    mean_motor6          | 0.4756612      |
|    mean_motor7          | 0.71254957     |
| train/                  |                |
|    approx_kl            | 0.037152894    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.07           |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.809          |
|    cost_value_loss      | 0.000266       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.88          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0218         |
|    mean_cost_advantages | -0.00065395294 |
|    mean_reward_advan... | 0.00010977984  |
|    n_updates            | 4140           |
|    nu                   | 7.96           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00202       |
|    reward_explained_... | 0.905          |
|    reward_value_loss    | 0.0363         |
|    std                  | 0.394          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.1e+03       |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.99e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.47          |
|    forward_reward       | 0.438         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.438         |
|    reward_survive       | 1             |
|    x_position           | 2.13          |
|    x_velocity           | 0.438         |
|    y_position           | 3.85          |
|    y_velocity           | 0.618         |
| rollout/                |               |
|    adjusted_reward      | 6.59          |
|    ep_len_mean          | 478           |
|    ep_rew_mean          | 3.25e+03      |
| time/                   |               |
|    fps                  | 814           |
|    iterations           | 209           |
|    time_elapsed         | 2627          |
|    total_timesteps      | 2140160       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10193         |
|    mean_motor0          | 0.55141973    |
|    mean_motor1          | 0.41797018    |
|    mean_motor2          | 0.6045099     |
|    mean_motor3          | 0.48147726    |
|    mean_motor4          | 0.44744864    |
|    mean_motor5          | 0.62507796    |
|    mean_motor6          | 0.4813342     |
|    mean_motor7          | 0.72488767    |
| train/                  |               |
|    approx_kl            | 0.04041912    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0894        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.829         |
|    cost_value_loss      | 0.000278      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.85         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0138        |
|    mean_cost_advantages | -0.0013557606 |
|    mean_reward_advan... | -0.017791033  |
|    n_updates            | 4160          |
|    nu                   | 8             |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0027       |
|    reward_explained_... | 0.904         |
|    reward_value_loss    | 0.0427        |
|    std                  | 0.393         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.24e+03       |
|    mean_ep_length       | 482            |
|    mean_reward          | 4.24e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.98           |
|    forward_reward       | 0.382          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.47          |
|    reward_forward       | 0.382          |
|    reward_survive       | 1              |
|    x_position           | 1.96           |
|    x_velocity           | 0.382          |
|    y_position           | 6.67           |
|    y_velocity           | 0.654          |
| rollout/                |                |
|    adjusted_reward      | 7.74           |
|    ep_len_mean          | 474            |
|    ep_rew_mean          | 3.37e+03       |
| time/                   |                |
|    fps                  | 814            |
|    iterations           | 210            |
|    time_elapsed         | 2640           |
|    total_timesteps      | 2150400        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10187          |
|    mean_motor0          | 0.5497522      |
|    mean_motor1          | 0.43898758     |
|    mean_motor2          | 0.64463997     |
|    mean_motor3          | 0.49241218     |
|    mean_motor4          | 0.449826       |
|    mean_motor5          | 0.6280021      |
|    mean_motor6          | 0.46334648     |
|    mean_motor7          | 0.7301574      |
| train/                  |                |
|    approx_kl            | 0.032337446    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0602         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.633          |
|    cost_value_loss      | 1.63e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.82          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0116         |
|    mean_cost_advantages | -0.00027988577 |
|    mean_reward_advan... | -0.010372651   |
|    n_updates            | 4180           |
|    nu                   | 8.04           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00215       |
|    reward_explained_... | 0.93           |
|    reward_value_loss    | 0.0305         |
|    std                  | 0.391          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.24e+03      |
|    mean_ep_length       | 484           |
|    mean_reward          | 3.67e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.61          |
|    forward_reward       | 0.313         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.313         |
|    reward_survive       | 1             |
|    x_position           | 0.947         |
|    x_velocity           | 0.313         |
|    y_position           | 5.42          |
|    y_velocity           | 0.543         |
| rollout/                |               |
|    adjusted_reward      | 7.89          |
|    ep_len_mean          | 469           |
|    ep_rew_mean          | 3.48e+03      |
| time/                   |               |
|    fps                  | 814           |
|    iterations           | 211           |
|    time_elapsed         | 2653          |
|    total_timesteps      | 2160640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10195         |
|    mean_motor0          | 0.5392177     |
|    mean_motor1          | 0.41064858    |
|    mean_motor2          | 0.66284317    |
|    mean_motor3          | 0.46916193    |
|    mean_motor4          | 0.47816172    |
|    mean_motor5          | 0.65567887    |
|    mean_motor6          | 0.47377104    |
|    mean_motor7          | 0.7179848     |
| train/                  |               |
|    approx_kl            | 0.1435026     |
|    average_cost         | 0.027832031   |
|    clip_fraction        | 0.204         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.93          |
|    cost_value_loss      | 0.0178        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.81         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.0119       |
|    mean_cost_advantages | 0.025970137   |
|    mean_reward_advan... | -0.0047935946 |
|    n_updates            | 4200          |
|    nu                   | 8.09          |
|    nu_loss              | -0.224        |
|    policy_gradient_loss | -0.00725      |
|    reward_explained_... | 0.894         |
|    reward_value_loss    | 0.035         |
|    std                  | 0.391         |
|    total_cost           | 285.0         |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.45e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.63         |
|    forward_reward       | 0.241        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.241        |
|    reward_survive       | 1            |
|    x_position           | 1.37         |
|    x_velocity           | 0.241        |
|    y_position           | 6.34         |
|    y_velocity           | 0.561        |
| rollout/                |              |
|    adjusted_reward      | 7.65         |
|    ep_len_mean          | 478          |
|    ep_rew_mean          | 3.54e+03     |
| time/                   |              |
|    fps                  | 813          |
|    iterations           | 212          |
|    time_elapsed         | 2667         |
|    total_timesteps      | 2170880      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10186        |
|    mean_motor0          | 0.50704944   |
|    mean_motor1          | 0.40628928   |
|    mean_motor2          | 0.627476     |
|    mean_motor3          | 0.47981897   |
|    mean_motor4          | 0.4469522    |
|    mean_motor5          | 0.61174625   |
|    mean_motor6          | 0.4969173    |
|    mean_motor7          | 0.70792353   |
| train/                  |              |
|    approx_kl            | 0.17178991   |
|    average_cost         | 0.038769532  |
|    clip_fraction        | 0.196        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.932        |
|    cost_value_loss      | 0.0622       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.79        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0171       |
|    mean_cost_advantages | 0.028476402  |
|    mean_reward_advan... | -0.019309847 |
|    n_updates            | 4220         |
|    nu                   | 8.16         |
|    nu_loss              | -0.314       |
|    policy_gradient_loss | -0.00771     |
|    reward_explained_... | 0.904        |
|    reward_value_loss    | 0.0559       |
|    std                  | 0.39         |
|    total_cost           | 397.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.77e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.39         |
|    forward_reward       | 0.179        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.179        |
|    reward_survive       | 1            |
|    x_position           | 2.65         |
|    x_velocity           | 0.179        |
|    y_position           | 6.67         |
|    y_velocity           | 0.272        |
| rollout/                |              |
|    adjusted_reward      | 6.67         |
|    ep_len_mean          | 486          |
|    ep_rew_mean          | 3.61e+03     |
| time/                   |              |
|    fps                  | 813          |
|    iterations           | 213          |
|    time_elapsed         | 2680         |
|    total_timesteps      | 2181120      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10184        |
|    mean_motor0          | 0.51712453   |
|    mean_motor1          | 0.40703446   |
|    mean_motor2          | 0.59042555   |
|    mean_motor3          | 0.49367142   |
|    mean_motor4          | 0.46244925   |
|    mean_motor5          | 0.6373188    |
|    mean_motor6          | 0.48740143   |
|    mean_motor7          | 0.66999894   |
| train/                  |              |
|    approx_kl            | 0.11362927   |
|    average_cost         | 0.02421875   |
|    clip_fraction        | 0.187        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.939        |
|    cost_value_loss      | 0.0365       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.77        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0581       |
|    mean_cost_advantages | 0.015274254  |
|    mean_reward_advan... | -0.030989802 |
|    n_updates            | 4240         |
|    nu                   | 8.24         |
|    nu_loss              | -0.198       |
|    policy_gradient_loss | -0.00561     |
|    reward_explained_... | 0.902        |
|    reward_value_loss    | 0.0611       |
|    std                  | 0.389        |
|    total_cost           | 248.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.98e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.84         |
|    forward_reward       | 0.407        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.5         |
|    reward_forward       | 0.407        |
|    reward_survive       | 1            |
|    x_position           | 1.17         |
|    x_velocity           | 0.407        |
|    y_position           | 7.73         |
|    y_velocity           | 0.752        |
| rollout/                |              |
|    adjusted_reward      | 6.7          |
|    ep_len_mean          | 483          |
|    ep_rew_mean          | 3.54e+03     |
| time/                   |              |
|    fps                  | 813          |
|    iterations           | 214          |
|    time_elapsed         | 2693         |
|    total_timesteps      | 2191360      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10160        |
|    mean_motor0          | 0.4962035    |
|    mean_motor1          | 0.40262875   |
|    mean_motor2          | 0.6249535    |
|    mean_motor3          | 0.48352343   |
|    mean_motor4          | 0.46894306   |
|    mean_motor5          | 0.63163626   |
|    mean_motor6          | 0.4989918    |
|    mean_motor7          | 0.6482892    |
| train/                  |              |
|    approx_kl            | 0.05524981   |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.128        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.911        |
|    cost_value_loss      | 0.0155       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.76        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00243      |
|    mean_cost_advantages | 0.0031152451 |
|    mean_reward_advan... | -0.018257603 |
|    n_updates            | 4260         |
|    nu                   | 8.31         |
|    nu_loss              | -0.0402      |
|    policy_gradient_loss | -0.00432     |
|    reward_explained_... | 0.939        |
|    reward_value_loss    | 0.0352       |
|    std                  | 0.389        |
|    total_cost           | 50.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.51e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.5            |
|    forward_reward       | 0.307          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.3           |
|    reward_forward       | 0.307          |
|    reward_survive       | 1              |
|    x_position           | 0.377          |
|    x_velocity           | 0.307          |
|    y_position           | 7.32           |
|    y_velocity           | 0.664          |
| rollout/                |                |
|    adjusted_reward      | 7.08           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.56e+03       |
| time/                   |                |
|    fps                  | 813            |
|    iterations           | 215            |
|    time_elapsed         | 2707           |
|    total_timesteps      | 2201600        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10173          |
|    mean_motor0          | 0.5195473      |
|    mean_motor1          | 0.39547336     |
|    mean_motor2          | 0.62023795     |
|    mean_motor3          | 0.46318626     |
|    mean_motor4          | 0.46620312     |
|    mean_motor5          | 0.6584928      |
|    mean_motor6          | 0.48534402     |
|    mean_motor7          | 0.62570536     |
| train/                  |                |
|    approx_kl            | 0.045006838    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.078          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.708          |
|    cost_value_loss      | 0.000366       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.75          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0151         |
|    mean_cost_advantages | -5.9958802e-05 |
|    mean_reward_advan... | -0.00849657    |
|    n_updates            | 4280           |
|    nu                   | 8.37           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00248       |
|    reward_explained_... | 0.939          |
|    reward_value_loss    | 0.0292         |
|    std                  | 0.388          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 404           |
|    mean_reward          | 2.31e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.08          |
|    forward_reward       | 0.318         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.29         |
|    reward_forward       | 0.318         |
|    reward_survive       | 1             |
|    x_position           | 1.73          |
|    x_velocity           | 0.318         |
|    y_position           | 6.47          |
|    y_velocity           | 0.362         |
| rollout/                |               |
|    adjusted_reward      | 6.99          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 3.51e+03      |
| time/                   |               |
|    fps                  | 813           |
|    iterations           | 216           |
|    time_elapsed         | 2719          |
|    total_timesteps      | 2211840       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10172         |
|    mean_motor0          | 0.49435717    |
|    mean_motor1          | 0.4039402     |
|    mean_motor2          | 0.59579855    |
|    mean_motor3          | 0.46155006    |
|    mean_motor4          | 0.4696855     |
|    mean_motor5          | 0.6696903     |
|    mean_motor6          | 0.47261858    |
|    mean_motor7          | 0.6158427     |
| train/                  |               |
|    approx_kl            | 0.042536497   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0796        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.869         |
|    cost_value_loss      | 0.00118       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.72         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.025         |
|    mean_cost_advantages | -0.0021398147 |
|    mean_reward_advan... | -0.0067686467 |
|    n_updates            | 4300          |
|    nu                   | 8.43          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0026       |
|    reward_explained_... | 0.92          |
|    reward_value_loss    | 0.0316        |
|    std                  | 0.387         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.87e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.2           |
|    forward_reward       | 0.283          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.39          |
|    reward_forward       | 0.283          |
|    reward_survive       | 1              |
|    x_position           | 2.52           |
|    x_velocity           | 0.283          |
|    y_position           | 9.81           |
|    y_velocity           | 0.354          |
| rollout/                |                |
|    adjusted_reward      | 6.91           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.36e+03       |
| time/                   |                |
|    fps                  | 812            |
|    iterations           | 217            |
|    time_elapsed         | 2733           |
|    total_timesteps      | 2222080        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10174          |
|    mean_motor0          | 0.4804716      |
|    mean_motor1          | 0.38094938     |
|    mean_motor2          | 0.5967995      |
|    mean_motor3          | 0.48224384     |
|    mean_motor4          | 0.47443515     |
|    mean_motor5          | 0.6515811      |
|    mean_motor6          | 0.48447394     |
|    mean_motor7          | 0.63926685     |
| train/                  |                |
|    approx_kl            | 0.04066827     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0768         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.839          |
|    cost_value_loss      | 0.0001         |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.7           |
|    learning_rate        | 3e-05          |
|    loss                 | -4.53e-05      |
|    mean_cost_advantages | -0.00062384683 |
|    mean_reward_advan... | -0.013041156   |
|    n_updates            | 4320           |
|    nu                   | 8.48           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00211       |
|    reward_explained_... | 0.928          |
|    reward_value_loss    | 0.0283         |
|    std                  | 0.386          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.42e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.59           |
|    forward_reward       | 0.379          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.26          |
|    reward_forward       | 0.379          |
|    reward_survive       | 1              |
|    x_position           | 2.58           |
|    x_velocity           | 0.379          |
|    y_position           | 7.01           |
|    y_velocity           | 0.599          |
| rollout/                |                |
|    adjusted_reward      | 7.67           |
|    ep_len_mean          | 487            |
|    ep_rew_mean          | 3.44e+03       |
| time/                   |                |
|    fps                  | 812            |
|    iterations           | 218            |
|    time_elapsed         | 2746           |
|    total_timesteps      | 2232320        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10165          |
|    mean_motor0          | 0.45326447     |
|    mean_motor1          | 0.40141988     |
|    mean_motor2          | 0.6324072      |
|    mean_motor3          | 0.47657663     |
|    mean_motor4          | 0.467135       |
|    mean_motor5          | 0.64332366     |
|    mean_motor6          | 0.4720767      |
|    mean_motor7          | 0.6256888      |
| train/                  |                |
|    approx_kl            | 0.0344508      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.064          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.922          |
|    cost_value_loss      | 1.88e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.68          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0157         |
|    mean_cost_advantages | -0.00016775078 |
|    mean_reward_advan... | -0.008440891   |
|    n_updates            | 4340           |
|    nu                   | 8.53           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00199       |
|    reward_explained_... | 0.931          |
|    reward_value_loss    | 0.0274         |
|    std                  | 0.385          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.75e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.2          |
|    forward_reward       | 0.331        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.48        |
|    reward_forward       | 0.331        |
|    reward_survive       | 1            |
|    x_position           | 1.19         |
|    x_velocity           | 0.331        |
|    y_position           | 3.82         |
|    y_velocity           | 0.581        |
| rollout/                |              |
|    adjusted_reward      | 7.01         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 3.49e+03     |
| time/                   |              |
|    fps                  | 812          |
|    iterations           | 219          |
|    time_elapsed         | 2759         |
|    total_timesteps      | 2242560      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10165        |
|    mean_motor0          | 0.44734415   |
|    mean_motor1          | 0.38779017   |
|    mean_motor2          | 0.63654196   |
|    mean_motor3          | 0.49514604   |
|    mean_motor4          | 0.46962413   |
|    mean_motor5          | 0.6255775    |
|    mean_motor6          | 0.4893065    |
|    mean_motor7          | 0.609565     |
| train/                  |              |
|    approx_kl            | 0.033281583  |
|    average_cost         | 0.0026367188 |
|    clip_fraction        | 0.0791       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.875        |
|    cost_value_loss      | 0.00457      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.66        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.000792     |
|    mean_cost_advantages | 0.0012018628 |
|    mean_reward_advan... | -0.010158621 |
|    n_updates            | 4360         |
|    nu                   | 8.58         |
|    nu_loss              | -0.0225      |
|    policy_gradient_loss | -0.0028      |
|    reward_explained_... | 0.906        |
|    reward_value_loss    | 0.0408       |
|    std                  | 0.384        |
|    total_cost           | 27.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.23e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.44           |
|    forward_reward       | 0.463          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.36          |
|    reward_forward       | 0.463          |
|    reward_survive       | 1              |
|    x_position           | 1.47           |
|    x_velocity           | 0.463          |
|    y_position           | 6.22           |
|    y_velocity           | 0.682          |
| rollout/                |                |
|    adjusted_reward      | 7.37           |
|    ep_len_mean          | 488            |
|    ep_rew_mean          | 3.48e+03       |
| time/                   |                |
|    fps                  | 812            |
|    iterations           | 220            |
|    time_elapsed         | 2773           |
|    total_timesteps      | 2252800        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10167          |
|    mean_motor0          | 0.45928797     |
|    mean_motor1          | 0.40962762     |
|    mean_motor2          | 0.6253788      |
|    mean_motor3          | 0.48540273     |
|    mean_motor4          | 0.4678832      |
|    mean_motor5          | 0.6275026      |
|    mean_motor6          | 0.4646747      |
|    mean_motor7          | 0.633438       |
| train/                  |                |
|    approx_kl            | 0.037198562    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.074          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.832          |
|    cost_value_loss      | 0.000101       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.64          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0188         |
|    mean_cost_advantages | -0.00034572184 |
|    mean_reward_advan... | -0.007393387   |
|    n_updates            | 4380           |
|    nu                   | 8.62           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0023        |
|    reward_explained_... | 0.941          |
|    reward_value_loss    | 0.0299         |
|    std                  | 0.383          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.82e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.76          |
|    forward_reward       | 0.3           |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.3           |
|    reward_survive       | 1             |
|    x_position           | 1.88          |
|    x_velocity           | 0.3           |
|    y_position           | 6.42          |
|    y_velocity           | 0.507         |
| rollout/                |               |
|    adjusted_reward      | 7.62          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 3.58e+03      |
| time/                   |               |
|    fps                  | 812           |
|    iterations           | 221           |
|    time_elapsed         | 2786          |
|    total_timesteps      | 2263040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10182         |
|    mean_motor0          | 0.45655355    |
|    mean_motor1          | 0.39774844    |
|    mean_motor2          | 0.6220857     |
|    mean_motor3          | 0.49442214    |
|    mean_motor4          | 0.45530963    |
|    mean_motor5          | 0.6340503     |
|    mean_motor6          | 0.49382263    |
|    mean_motor7          | 0.6459849     |
| train/                  |               |
|    approx_kl            | 0.030004824   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0868        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.854         |
|    cost_value_loss      | 0.0002        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.61         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00375       |
|    mean_cost_advantages | -0.0014388819 |
|    mean_reward_advan... | -0.0003445739 |
|    n_updates            | 4400          |
|    nu                   | 8.65          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00242      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.032         |
|    std                  | 0.382         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.09e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.26          |
|    forward_reward       | 0.304         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.26         |
|    reward_forward       | 0.304         |
|    reward_survive       | 1             |
|    x_position           | 1.79          |
|    x_velocity           | 0.304         |
|    y_position           | 7.99          |
|    y_velocity           | 0.375         |
| rollout/                |               |
|    adjusted_reward      | 7.77          |
|    ep_len_mean          | 485           |
|    ep_rew_mean          | 3.64e+03      |
| time/                   |               |
|    fps                  | 811           |
|    iterations           | 222           |
|    time_elapsed         | 2799          |
|    total_timesteps      | 2273280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10173         |
|    mean_motor0          | 0.4463355     |
|    mean_motor1          | 0.41410345    |
|    mean_motor2          | 0.62808764    |
|    mean_motor3          | 0.4873499     |
|    mean_motor4          | 0.44997564    |
|    mean_motor5          | 0.6387156     |
|    mean_motor6          | 0.47310868    |
|    mean_motor7          | 0.6638646     |
| train/                  |               |
|    approx_kl            | 0.031317316   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0825        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.896         |
|    cost_value_loss      | 1.55e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.58         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00635       |
|    mean_cost_advantages | -0.0004771715 |
|    mean_reward_advan... | 0.004970363   |
|    n_updates            | 4420          |
|    nu                   | 8.68          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00247      |
|    reward_explained_... | 0.94          |
|    reward_value_loss    | 0.0273        |
|    std                  | 0.381         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.36e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.2           |
|    forward_reward       | 0.23           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.41          |
|    reward_forward       | 0.23           |
|    reward_survive       | 1              |
|    x_position           | 0.882          |
|    x_velocity           | 0.23           |
|    y_position           | 10             |
|    y_velocity           | 0.326          |
| rollout/                |                |
|    adjusted_reward      | 7.39           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 3.64e+03       |
| time/                   |                |
|    fps                  | 811            |
|    iterations           | 223            |
|    time_elapsed         | 2813           |
|    total_timesteps      | 2283520        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10169          |
|    mean_motor0          | 0.43898755     |
|    mean_motor1          | 0.43888968     |
|    mean_motor2          | 0.60912305     |
|    mean_motor3          | 0.46890864     |
|    mean_motor4          | 0.48553562     |
|    mean_motor5          | 0.6126046      |
|    mean_motor6          | 0.4530643      |
|    mean_motor7          | 0.6365634      |
| train/                  |                |
|    approx_kl            | 0.033023484    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0712         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.903          |
|    cost_value_loss      | 7.93e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.55          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0238         |
|    mean_cost_advantages | -0.00012515805 |
|    mean_reward_advan... | -0.008524512   |
|    n_updates            | 4440           |
|    nu                   | 8.71           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00213       |
|    reward_explained_... | 0.877          |
|    reward_value_loss    | 0.0356         |
|    std                  | 0.378          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 409           |
|    mean_reward          | 3.04e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.33          |
|    forward_reward       | 0.276         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.33         |
|    reward_forward       | 0.276         |
|    reward_survive       | 1             |
|    x_position           | 0.721         |
|    x_velocity           | 0.276         |
|    y_position           | 7.16          |
|    y_velocity           | 0.668         |
| rollout/                |               |
|    adjusted_reward      | 8.47          |
|    ep_len_mean          | 484           |
|    ep_rew_mean          | 3.74e+03      |
| time/                   |               |
|    fps                  | 811           |
|    iterations           | 224           |
|    time_elapsed         | 2826          |
|    total_timesteps      | 2293760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10185         |
|    mean_motor0          | 0.44903       |
|    mean_motor1          | 0.45257014    |
|    mean_motor2          | 0.6417441     |
|    mean_motor3          | 0.49471492    |
|    mean_motor4          | 0.44240394    |
|    mean_motor5          | 0.6447922     |
|    mean_motor6          | 0.4703544     |
|    mean_motor7          | 0.646593      |
| train/                  |               |
|    approx_kl            | 0.03484857    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0685        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.886         |
|    cost_value_loss      | 2.62e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.52         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0294        |
|    mean_cost_advantages | -0.0010191371 |
|    mean_reward_advan... | -0.0025104408 |
|    n_updates            | 4460          |
|    nu                   | 8.74          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00225      |
|    reward_explained_... | 0.952         |
|    reward_value_loss    | 0.0283        |
|    std                  | 0.377         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.6e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.02         |
|    forward_reward       | 0.265        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.41        |
|    reward_forward       | 0.265        |
|    reward_survive       | 1            |
|    x_position           | 1.65         |
|    x_velocity           | 0.265        |
|    y_position           | 6.8          |
|    y_velocity           | 0.749        |
| rollout/                |              |
|    adjusted_reward      | 7.72         |
|    ep_len_mean          | 488          |
|    ep_rew_mean          | 3.81e+03     |
| time/                   |              |
|    fps                  | 811          |
|    iterations           | 225          |
|    time_elapsed         | 2839         |
|    total_timesteps      | 2304000      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10163        |
|    mean_motor0          | 0.44468123   |
|    mean_motor1          | 0.46232685   |
|    mean_motor2          | 0.63201535   |
|    mean_motor3          | 0.47012323   |
|    mean_motor4          | 0.45478502   |
|    mean_motor5          | 0.6086771    |
|    mean_motor6          | 0.45242795   |
|    mean_motor7          | 0.61988235   |
| train/                  |              |
|    approx_kl            | 0.06281023   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.924        |
|    cost_value_loss      | 0.00303      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.48        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00163      |
|    mean_cost_advantages | -0.007241334 |
|    mean_reward_advan... | -0.004266236 |
|    n_updates            | 4480         |
|    nu                   | 8.76         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00588     |
|    reward_explained_... | 0.914        |
|    reward_value_loss    | 0.0348       |
|    std                  | 0.375        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.32e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.28          |
|    forward_reward       | 0.314         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.314         |
|    reward_survive       | 1             |
|    x_position           | 1.45          |
|    x_velocity           | 0.314         |
|    y_position           | 7.11          |
|    y_velocity           | 0.706         |
| rollout/                |               |
|    adjusted_reward      | 7.62          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 3.86e+03      |
| time/                   |               |
|    fps                  | 811           |
|    iterations           | 226           |
|    time_elapsed         | 2852          |
|    total_timesteps      | 2314240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10182         |
|    mean_motor0          | 0.4315106     |
|    mean_motor1          | 0.45158124    |
|    mean_motor2          | 0.6378952     |
|    mean_motor3          | 0.4972183     |
|    mean_motor4          | 0.45112762    |
|    mean_motor5          | 0.6219592     |
|    mean_motor6          | 0.44412485    |
|    mean_motor7          | 0.64833844    |
| train/                  |               |
|    approx_kl            | 0.041359257   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0838        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.894         |
|    cost_value_loss      | 0.000498      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.43         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00127      |
|    mean_cost_advantages | -0.0043251906 |
|    mean_reward_advan... | -0.01729646   |
|    n_updates            | 4500          |
|    nu                   | 8.79          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00243      |
|    reward_explained_... | 0.922         |
|    reward_value_loss    | 0.0397        |
|    std                  | 0.373         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.31e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.03           |
|    forward_reward       | 0.49           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.35          |
|    reward_forward       | 0.49           |
|    reward_survive       | 1              |
|    x_position           | 1.73           |
|    x_velocity           | 0.49           |
|    y_position           | 7.81           |
|    y_velocity           | 0.619          |
| rollout/                |                |
|    adjusted_reward      | 8.16           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 3.9e+03        |
| time/                   |                |
|    fps                  | 811            |
|    iterations           | 227            |
|    time_elapsed         | 2866           |
|    total_timesteps      | 2324480        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10184          |
|    mean_motor0          | 0.44010115     |
|    mean_motor1          | 0.4585955      |
|    mean_motor2          | 0.6309513      |
|    mean_motor3          | 0.50326955     |
|    mean_motor4          | 0.44303098     |
|    mean_motor5          | 0.61049193     |
|    mean_motor6          | 0.44601935     |
|    mean_motor7          | 0.65866685     |
| train/                  |                |
|    approx_kl            | 0.02845962     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0644         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.76           |
|    cost_value_loss      | 1.04e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.41          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00709        |
|    mean_cost_advantages | -0.00051310344 |
|    mean_reward_advan... | -0.031621523   |
|    n_updates            | 4520           |
|    nu                   | 8.81           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00149       |
|    reward_explained_... | 0.829          |
|    reward_value_loss    | 0.0441         |
|    std                  | 0.372          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.4e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.4            |
|    forward_reward       | 0.349          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.41          |
|    reward_forward       | 0.349          |
|    reward_survive       | 1              |
|    x_position           | 0.949          |
|    x_velocity           | 0.349          |
|    y_position           | 9.12           |
|    y_velocity           | 0.509          |
| rollout/                |                |
|    adjusted_reward      | 8.8            |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 4.03e+03       |
| time/                   |                |
|    fps                  | 810            |
|    iterations           | 228            |
|    time_elapsed         | 2879           |
|    total_timesteps      | 2334720        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10176          |
|    mean_motor0          | 0.4674458      |
|    mean_motor1          | 0.5091204      |
|    mean_motor2          | 0.64229804     |
|    mean_motor3          | 0.4724949      |
|    mean_motor4          | 0.44734496     |
|    mean_motor5          | 0.6089193      |
|    mean_motor6          | 0.44590592     |
|    mean_motor7          | 0.6427468      |
| train/                  |                |
|    approx_kl            | 0.036931448    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0756         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.923          |
|    cost_value_loss      | 3.07e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.39          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00888        |
|    mean_cost_advantages | -0.00096533226 |
|    mean_reward_advan... | -0.006705458   |
|    n_updates            | 4540           |
|    nu                   | 8.82           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0021        |
|    reward_explained_... | 0.909          |
|    reward_value_loss    | 0.0292         |
|    std                  | 0.371          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 4.45e+03    |
|    mean_ep_length       | 500         |
|    mean_reward          | 3.33e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 8.13        |
|    forward_reward       | 0.264       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.27       |
|    reward_forward       | 0.264       |
|    reward_survive       | 1           |
|    x_position           | 0.13        |
|    x_velocity           | 0.264       |
|    y_position           | 8.08        |
|    y_velocity           | 0.599       |
| rollout/                |             |
|    adjusted_reward      | 7.47        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 3.98e+03    |
| time/                   |             |
|    fps                  | 810         |
|    iterations           | 229         |
|    time_elapsed         | 2892        |
|    total_timesteps      | 2344960     |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10189       |
|    mean_motor0          | 0.4665234   |
|    mean_motor1          | 0.50756353  |
|    mean_motor2          | 0.62398815  |
|    mean_motor3          | 0.49672174  |
|    mean_motor4          | 0.46226     |
|    mean_motor5          | 0.5975644   |
|    mean_motor6          | 0.4258376   |
|    mean_motor7          | 0.6433068   |
| train/                  |             |
|    approx_kl            | 0.12895139  |
|    average_cost         | 0.02216797  |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.868       |
|    cost_value_loss      | 0.0117      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -3.37       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.00468    |
|    mean_cost_advantages | 0.02294955  |
|    mean_reward_advan... | 0.010078895 |
|    n_updates            | 4560        |
|    nu                   | 8.85        |
|    nu_loss              | -0.196      |
|    policy_gradient_loss | -0.00538    |
|    reward_explained_... | 0.961       |
|    reward_value_loss    | 0.0255      |
|    std                  | 0.371       |
|    total_cost           | 227.0       |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.45e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.49e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.4           |
|    forward_reward       | 0.182          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.32          |
|    reward_forward       | 0.182          |
|    reward_survive       | 1              |
|    x_position           | 0.291          |
|    x_velocity           | 0.182          |
|    y_position           | 10.4           |
|    y_velocity           | 0.602          |
| rollout/                |                |
|    adjusted_reward      | 8.39           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 4.04e+03       |
| time/                   |                |
|    fps                  | 810            |
|    iterations           | 230            |
|    time_elapsed         | 2906           |
|    total_timesteps      | 2355200        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10190          |
|    mean_motor0          | 0.44152433     |
|    mean_motor1          | 0.5248412      |
|    mean_motor2          | 0.6347741      |
|    mean_motor3          | 0.4966732      |
|    mean_motor4          | 0.4479285      |
|    mean_motor5          | 0.5992232      |
|    mean_motor6          | 0.43312868     |
|    mean_motor7          | 0.6492876      |
| train/                  |                |
|    approx_kl            | 0.03294767     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0605         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.832          |
|    cost_value_loss      | 4.47e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.35          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0161         |
|    mean_cost_advantages | -0.00095062563 |
|    mean_reward_advan... | -0.03701233    |
|    n_updates            | 4580           |
|    nu                   | 8.88           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00124       |
|    reward_explained_... | 0.905          |
|    reward_value_loss    | 0.0422         |
|    std                  | 0.37           |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.21e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.57          |
|    forward_reward       | 0.224         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.224         |
|    reward_survive       | 1             |
|    x_position           | 0.162         |
|    x_velocity           | 0.224         |
|    y_position           | 8.54          |
|    y_velocity           | 0.815         |
| rollout/                |               |
|    adjusted_reward      | 8.79          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.13e+03      |
| time/                   |               |
|    fps                  | 810           |
|    iterations           | 231           |
|    time_elapsed         | 2919          |
|    total_timesteps      | 2365440       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10186         |
|    mean_motor0          | 0.44522467    |
|    mean_motor1          | 0.5103669     |
|    mean_motor2          | 0.6340121     |
|    mean_motor3          | 0.47974116    |
|    mean_motor4          | 0.4484795     |
|    mean_motor5          | 0.60855925    |
|    mean_motor6          | 0.42653227    |
|    mean_motor7          | 0.6362627     |
| train/                  |               |
|    approx_kl            | 0.03222233    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0721        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.823         |
|    cost_value_loss      | 1.81e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.35         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.013         |
|    mean_cost_advantages | -0.0009994906 |
|    mean_reward_advan... | -0.028852856  |
|    n_updates            | 4600          |
|    nu                   | 8.9           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00155      |
|    reward_explained_... | 0.882         |
|    reward_value_loss    | 0.0467        |
|    std                  | 0.37          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.45e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.76e+03     |
| infos/                  |              |
|    cost                 | 0.016        |
|    distance_from_origin | 9.45         |
|    forward_reward       | 0.293        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.293        |
|    reward_survive       | 1            |
|    x_position           | 0.277        |
|    x_velocity           | 0.293        |
|    y_position           | 9.31         |
|    y_velocity           | 0.612        |
| rollout/                |              |
|    adjusted_reward      | 8.19         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.14e+03     |
| time/                   |              |
|    fps                  | 810          |
|    iterations           | 232          |
|    time_elapsed         | 2932         |
|    total_timesteps      | 2375680      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10166        |
|    mean_motor0          | 0.46761847   |
|    mean_motor1          | 0.45858145   |
|    mean_motor2          | 0.62961996   |
|    mean_motor3          | 0.48012066   |
|    mean_motor4          | 0.45716143   |
|    mean_motor5          | 0.6138078    |
|    mean_motor6          | 0.44162545   |
|    mean_motor7          | 0.60912555   |
| train/                  |              |
|    approx_kl            | 0.04944376   |
|    average_cost         | 0.007421875  |
|    clip_fraction        | 0.112        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.63         |
|    cost_value_loss      | 0.00566      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.33        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0285       |
|    mean_cost_advantages | 0.006335824  |
|    mean_reward_advan... | -0.017772488 |
|    n_updates            | 4620         |
|    nu                   | 8.93         |
|    nu_loss              | -0.0661      |
|    policy_gradient_loss | -0.00332     |
|    reward_explained_... | 0.91         |
|    reward_value_loss    | 0.0394       |
|    std                  | 0.369        |
|    total_cost           | 76.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.45e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.2e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.1          |
|    forward_reward       | 0.283         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.283         |
|    reward_survive       | 1             |
|    x_position           | 0.818         |
|    x_velocity           | 0.283         |
|    y_position           | 10.1          |
|    y_velocity           | 0.472         |
| rollout/                |               |
|    adjusted_reward      | 8.55          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.1e+03       |
| time/                   |               |
|    fps                  | 809           |
|    iterations           | 233           |
|    time_elapsed         | 2946          |
|    total_timesteps      | 2385920       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10165         |
|    mean_motor0          | 0.46336517    |
|    mean_motor1          | 0.4779026     |
|    mean_motor2          | 0.64371264    |
|    mean_motor3          | 0.49220222    |
|    mean_motor4          | 0.45834643    |
|    mean_motor5          | 0.60564095    |
|    mean_motor6          | 0.4278017     |
|    mean_motor7          | 0.6115165     |
| train/                  |               |
|    approx_kl            | 0.036015607   |
|    average_cost         | 0.0022460937  |
|    clip_fraction        | 0.094         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.845         |
|    cost_value_loss      | 0.00122       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.31         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0195        |
|    mean_cost_advantages | 0.00048018666 |
|    mean_reward_advan... | -0.011541994  |
|    n_updates            | 4640          |
|    nu                   | 8.95          |
|    nu_loss              | -0.0201       |
|    policy_gradient_loss | -0.00201      |
|    reward_explained_... | 0.946         |
|    reward_value_loss    | 0.0314        |
|    std                  | 0.368         |
|    total_cost           | 23.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.53e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.53e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.78          |
|    forward_reward       | 0.202         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.37         |
|    reward_forward       | 0.202         |
|    reward_survive       | 1             |
|    x_position           | 0.893         |
|    x_velocity           | 0.202         |
|    y_position           | 8.66          |
|    y_velocity           | 0.355         |
| rollout/                |               |
|    adjusted_reward      | 8.23          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 4.17e+03      |
| time/                   |               |
|    fps                  | 809           |
|    iterations           | 234           |
|    time_elapsed         | 2959          |
|    total_timesteps      | 2396160       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10177         |
|    mean_motor0          | 0.47319013    |
|    mean_motor1          | 0.4697526     |
|    mean_motor2          | 0.64471126    |
|    mean_motor3          | 0.5096127     |
|    mean_motor4          | 0.46402726    |
|    mean_motor5          | 0.62595874    |
|    mean_motor6          | 0.44350523    |
|    mean_motor7          | 0.5718397     |
| train/                  |               |
|    approx_kl            | 0.04378905    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0723        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.829         |
|    cost_value_loss      | 8.92e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.3          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00948       |
|    mean_cost_advantages | -0.0012485093 |
|    mean_reward_advan... | -0.012872225  |
|    n_updates            | 4660          |
|    nu                   | 8.97          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00195      |
|    reward_explained_... | 0.909         |
|    reward_value_loss    | 0.036         |
|    std                  | 0.368         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.53e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.31e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.94         |
|    forward_reward       | 0.178        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.178        |
|    reward_survive       | 1            |
|    x_position           | 0.119        |
|    x_velocity           | 0.178        |
|    y_position           | 9.92         |
|    y_velocity           | 0.493        |
| rollout/                |              |
|    adjusted_reward      | 8.44         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 4.12e+03     |
| time/                   |              |
|    fps                  | 809          |
|    iterations           | 235          |
|    time_elapsed         | 2972         |
|    total_timesteps      | 2406400      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10170        |
|    mean_motor0          | 0.46730956   |
|    mean_motor1          | 0.40527934   |
|    mean_motor2          | 0.67329276   |
|    mean_motor3          | 0.5640428    |
|    mean_motor4          | 0.42589593   |
|    mean_motor5          | 0.63430184   |
|    mean_motor6          | 0.45283723   |
|    mean_motor7          | 0.5712246    |
| train/                  |              |
|    approx_kl            | 0.16287415   |
|    average_cost         | 0.041796874  |
|    clip_fraction        | 0.242        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.738        |
|    cost_value_loss      | 0.0179       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.31        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0272       |
|    mean_cost_advantages | 0.047554582  |
|    mean_reward_advan... | -0.018737013 |
|    n_updates            | 4680         |
|    nu                   | 9.02         |
|    nu_loss              | -0.375       |
|    policy_gradient_loss | -0.00735     |
|    reward_explained_... | 0.918        |
|    reward_value_loss    | 0.041        |
|    std                  | 0.368        |
|    total_cost           | 428.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.54e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.54e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.13         |
|    forward_reward       | 0.159        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.35        |
|    reward_forward       | 0.159        |
|    reward_survive       | 1            |
|    x_position           | 0.734        |
|    x_velocity           | 0.159        |
|    y_position           | 9.03         |
|    y_velocity           | 0.439        |
| rollout/                |              |
|    adjusted_reward      | 8.16         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 4.03e+03     |
| time/                   |              |
|    fps                  | 809          |
|    iterations           | 236          |
|    time_elapsed         | 2986         |
|    total_timesteps      | 2416640      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10168        |
|    mean_motor0          | 0.4730743    |
|    mean_motor1          | 0.39134753   |
|    mean_motor2          | 0.65031445   |
|    mean_motor3          | 0.5540124    |
|    mean_motor4          | 0.44132987   |
|    mean_motor5          | 0.6292456    |
|    mean_motor6          | 0.47213826   |
|    mean_motor7          | 0.57090306   |
| train/                  |              |
|    approx_kl            | 0.035876773  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.873        |
|    cost_value_loss      | 8.55e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.3         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00704      |
|    mean_cost_advantages | -0.001103607 |
|    mean_reward_advan... | -0.015160417 |
|    n_updates            | 4700         |
|    nu                   | 9.06         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00185     |
|    reward_explained_... | 0.911        |
|    reward_value_loss    | 0.0377       |
|    std                  | 0.368        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.54e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.36e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.52          |
|    forward_reward       | 0.22          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.35         |
|    reward_forward       | 0.22          |
|    reward_survive       | 1             |
|    x_position           | 0.263         |
|    x_velocity           | 0.22          |
|    y_position           | 8.51          |
|    y_velocity           | 0.654         |
| rollout/                |               |
|    adjusted_reward      | 8.21          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 4.02e+03      |
| time/                   |               |
|    fps                  | 809           |
|    iterations           | 237           |
|    time_elapsed         | 2999          |
|    total_timesteps      | 2426880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10183         |
|    mean_motor0          | 0.4754331     |
|    mean_motor1          | 0.39435366    |
|    mean_motor2          | 0.65594167    |
|    mean_motor3          | 0.59621716    |
|    mean_motor4          | 0.4475631     |
|    mean_motor5          | 0.6229403     |
|    mean_motor6          | 0.4630103     |
|    mean_motor7          | 0.5758057     |
| train/                  |               |
|    approx_kl            | 0.035555165   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0762        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.927         |
|    cost_value_loss      | 5.46e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.29         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0537        |
|    mean_cost_advantages | -0.0010838176 |
|    mean_reward_advan... | -0.014338598  |
|    n_updates            | 4720          |
|    nu                   | 9.1           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00182      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.0396        |
|    std                  | 0.367         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.54e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.27e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.89          |
|    forward_reward       | 0.434         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.4          |
|    reward_forward       | 0.434         |
|    reward_survive       | 1             |
|    x_position           | 0.579         |
|    x_velocity           | 0.434         |
|    y_position           | 7.81          |
|    y_velocity           | 0.763         |
| rollout/                |               |
|    adjusted_reward      | 8.99          |
|    ep_len_mean          | 479           |
|    ep_rew_mean          | 4.02e+03      |
| time/                   |               |
|    fps                  | 808           |
|    iterations           | 238           |
|    time_elapsed         | 3012          |
|    total_timesteps      | 2437120       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10195         |
|    mean_motor0          | 0.4681713     |
|    mean_motor1          | 0.4374786     |
|    mean_motor2          | 0.69221354    |
|    mean_motor3          | 0.61696565    |
|    mean_motor4          | 0.41841826    |
|    mean_motor5          | 0.64849156    |
|    mean_motor6          | 0.44430408    |
|    mean_motor7          | 0.5889156     |
| train/                  |               |
|    approx_kl            | 0.028932706   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0766        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.948         |
|    cost_value_loss      | 4.95e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.29         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000829      |
|    mean_cost_advantages | -0.0016266357 |
|    mean_reward_advan... | -0.015384632  |
|    n_updates            | 4740          |
|    nu                   | 9.13          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00212      |
|    reward_explained_... | 0.933         |
|    reward_value_loss    | 0.0358        |
|    std                  | 0.367         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.58e+03     |
|    mean_ep_length       | 479          |
|    mean_reward          | 4.58e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 10.3         |
|    forward_reward       | 0.36         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.41        |
|    reward_forward       | 0.36         |
|    reward_survive       | 1            |
|    x_position           | -0.132       |
|    x_velocity           | 0.36         |
|    y_position           | 10.2         |
|    y_velocity           | 0.771        |
| rollout/                |              |
|    adjusted_reward      | 9.61         |
|    ep_len_mean          | 479          |
|    ep_rew_mean          | 4.16e+03     |
| time/                   |              |
|    fps                  | 808          |
|    iterations           | 239          |
|    time_elapsed         | 3026         |
|    total_timesteps      | 2447360      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10194        |
|    mean_motor0          | 0.46052903   |
|    mean_motor1          | 0.45012861   |
|    mean_motor2          | 0.6952796    |
|    mean_motor3          | 0.6153612    |
|    mean_motor4          | 0.4195641    |
|    mean_motor5          | 0.6335952    |
|    mean_motor6          | 0.43341413   |
|    mean_motor7          | 0.5838125    |
| train/                  |              |
|    approx_kl            | 0.06170864   |
|    average_cost         | 0.009179687  |
|    clip_fraction        | 0.121        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.896        |
|    cost_value_loss      | 0.00698      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.28        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0031       |
|    mean_cost_advantages | 0.0074746273 |
|    mean_reward_advan... | -0.006935794 |
|    n_updates            | 4760         |
|    nu                   | 9.16         |
|    nu_loss              | -0.0838      |
|    policy_gradient_loss | -0.00283     |
|    reward_explained_... | 0.92         |
|    reward_value_loss    | 0.0353       |
|    std                  | 0.367        |
|    total_cost           | 94.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.58e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.71e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.69         |
|    forward_reward       | 0.351        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.46        |
|    reward_forward       | 0.351        |
|    reward_survive       | 1            |
|    x_position           | 1.4          |
|    x_velocity           | 0.351        |
|    y_position           | 8.5          |
|    y_velocity           | 0.953        |
| rollout/                |              |
|    adjusted_reward      | 8.05         |
|    ep_len_mean          | 483          |
|    ep_rew_mean          | 4.14e+03     |
| time/                   |              |
|    fps                  | 808          |
|    iterations           | 240          |
|    time_elapsed         | 3039         |
|    total_timesteps      | 2457600      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10187        |
|    mean_motor0          | 0.46507874   |
|    mean_motor1          | 0.41042596   |
|    mean_motor2          | 0.67517316   |
|    mean_motor3          | 0.60749876   |
|    mean_motor4          | 0.45633253   |
|    mean_motor5          | 0.6423281    |
|    mean_motor6          | 0.43930244   |
|    mean_motor7          | 0.65560853   |
| train/                  |              |
|    approx_kl            | 0.15075125   |
|    average_cost         | 0.049707033  |
|    clip_fraction        | 0.242        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.923        |
|    cost_value_loss      | 0.052        |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.26        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0326       |
|    mean_cost_advantages | 0.046925813  |
|    mean_reward_advan... | 0.0056924657 |
|    n_updates            | 4780         |
|    nu                   | 9.22         |
|    nu_loss              | -0.455       |
|    policy_gradient_loss | -0.00622     |
|    reward_explained_... | 0.924        |
|    reward_value_loss    | 0.038        |
|    std                  | 0.366        |
|    total_cost           | 509.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.58e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.57e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.82         |
|    forward_reward       | 0.393        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.48        |
|    reward_forward       | 0.393        |
|    reward_survive       | 1            |
|    x_position           | 0.899        |
|    x_velocity           | 0.393        |
|    y_position           | 4.62         |
|    y_velocity           | 0.81         |
| rollout/                |              |
|    adjusted_reward      | 8.25         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 4.21e+03     |
| time/                   |              |
|    fps                  | 808          |
|    iterations           | 241          |
|    time_elapsed         | 3052         |
|    total_timesteps      | 2467840      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10198        |
|    mean_motor0          | 0.46277055   |
|    mean_motor1          | 0.3818224    |
|    mean_motor2          | 0.65880406   |
|    mean_motor3          | 0.6540642    |
|    mean_motor4          | 0.44683856   |
|    mean_motor5          | 0.69498724   |
|    mean_motor6          | 0.4579959    |
|    mean_motor7          | 0.6211058    |
| train/                  |              |
|    approx_kl            | 0.14457497   |
|    average_cost         | 0.030566406  |
|    clip_fraction        | 0.234        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.973        |
|    cost_value_loss      | 0.0257       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.25        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00137     |
|    mean_cost_advantages | 0.023119783  |
|    mean_reward_advan... | -0.009794965 |
|    n_updates            | 4800         |
|    nu                   | 9.3          |
|    nu_loss              | -0.282       |
|    policy_gradient_loss | -0.00596     |
|    reward_explained_... | 0.935        |
|    reward_value_loss    | 0.0406       |
|    std                  | 0.366        |
|    total_cost           | 313.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.74e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.99          |
|    forward_reward       | 0.506         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.506         |
|    reward_survive       | 1             |
|    x_position           | 1.11          |
|    x_velocity           | 0.506         |
|    y_position           | 6.87          |
|    y_velocity           | 1.02          |
| rollout/                |               |
|    adjusted_reward      | 8.26          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 4.22e+03      |
| time/                   |               |
|    fps                  | 808           |
|    iterations           | 242           |
|    time_elapsed         | 3066          |
|    total_timesteps      | 2478080       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10196         |
|    mean_motor0          | 0.44784275    |
|    mean_motor1          | 0.40737933    |
|    mean_motor2          | 0.6508278     |
|    mean_motor3          | 0.6158616     |
|    mean_motor4          | 0.44816867    |
|    mean_motor5          | 0.6532807     |
|    mean_motor6          | 0.45995384    |
|    mean_motor7          | 0.6375346     |
| train/                  |               |
|    approx_kl            | 0.040499132   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0896        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.697         |
|    cost_value_loss      | 9.95e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.25         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0134        |
|    mean_cost_advantages | 0.00064711424 |
|    mean_reward_advan... | -0.0026949295 |
|    n_updates            | 4820          |
|    nu                   | 9.36          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00228      |
|    reward_explained_... | 0.875         |
|    reward_value_loss    | 0.031         |
|    std                  | 0.365         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.25e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.9            |
|    forward_reward       | 0.215          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.45          |
|    reward_forward       | 0.215          |
|    reward_survive       | 1              |
|    x_position           | 1.47           |
|    x_velocity           | 0.215          |
|    y_position           | 7.76           |
|    y_velocity           | 0.535          |
| rollout/                |                |
|    adjusted_reward      | 8.52           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 4.25e+03       |
| time/                   |                |
|    fps                  | 808            |
|    iterations           | 243            |
|    time_elapsed         | 3079           |
|    total_timesteps      | 2488320        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10199          |
|    mean_motor0          | 0.45889372     |
|    mean_motor1          | 0.4076775      |
|    mean_motor2          | 0.6454012      |
|    mean_motor3          | 0.63272107     |
|    mean_motor4          | 0.41455525     |
|    mean_motor5          | 0.6599889      |
|    mean_motor6          | 0.4490997      |
|    mean_motor7          | 0.663518       |
| train/                  |                |
|    approx_kl            | 0.1288912      |
|    average_cost         | 0.020117188    |
|    clip_fraction        | 0.197          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.928          |
|    cost_value_loss      | 0.00627        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.23          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00784        |
|    mean_cost_advantages | 0.019645259    |
|    mean_reward_advan... | -1.0282546e-05 |
|    n_updates            | 4840           |
|    nu                   | 9.43           |
|    nu_loss              | -0.188         |
|    policy_gradient_loss | -0.00509       |
|    reward_explained_... | 0.918          |
|    reward_value_loss    | 0.031          |
|    std                  | 0.365          |
|    total_cost           | 206.0          |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.49e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.89          |
|    forward_reward       | 0.28          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.41         |
|    reward_forward       | 0.28          |
|    reward_survive       | 1             |
|    x_position           | 2.08          |
|    x_velocity           | 0.28          |
|    y_position           | 8.58          |
|    y_velocity           | 0.476         |
| rollout/                |               |
|    adjusted_reward      | 8.4           |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.13e+03      |
| time/                   |               |
|    fps                  | 807           |
|    iterations           | 244           |
|    time_elapsed         | 3093          |
|    total_timesteps      | 2498560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10188         |
|    mean_motor0          | 0.46054864    |
|    mean_motor1          | 0.41037712    |
|    mean_motor2          | 0.6503859     |
|    mean_motor3          | 0.58201       |
|    mean_motor4          | 0.41474462    |
|    mean_motor5          | 0.68272704    |
|    mean_motor6          | 0.4480923     |
|    mean_motor7          | 0.6640941     |
| train/                  |               |
|    approx_kl            | 0.031263776   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0685        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.78          |
|    cost_value_loss      | 7.79e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.22         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0026        |
|    mean_cost_advantages | 0.00045437872 |
|    mean_reward_advan... | -0.012287447  |
|    n_updates            | 4860          |
|    nu                   | 9.49          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00188      |
|    reward_explained_... | 0.878         |
|    reward_value_loss    | 0.0381        |
|    std                  | 0.364         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.58e+03     |
|    mean_ep_length       | 404          |
|    mean_reward          | 3.19e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 11.8         |
|    forward_reward       | 0.16         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.16         |
|    reward_survive       | 1            |
|    x_position           | 0.89         |
|    x_velocity           | 0.16         |
|    y_position           | 11.8         |
|    y_velocity           | 0.389        |
| rollout/                |              |
|    adjusted_reward      | 8            |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 4.1e+03      |
| time/                   |              |
|    fps                  | 807          |
|    iterations           | 245          |
|    time_elapsed         | 3105         |
|    total_timesteps      | 2508800      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10187        |
|    mean_motor0          | 0.45289406   |
|    mean_motor1          | 0.39928263   |
|    mean_motor2          | 0.6289066    |
|    mean_motor3          | 0.55076075   |
|    mean_motor4          | 0.42007184   |
|    mean_motor5          | 0.6756774    |
|    mean_motor6          | 0.45762873   |
|    mean_motor7          | 0.66065574   |
| train/                  |              |
|    approx_kl            | 0.032910943  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0677       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.799        |
|    cost_value_loss      | 2.59e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.2         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0161       |
|    mean_cost_advantages | 0.0005986672 |
|    mean_reward_advan... | -0.011950396 |
|    n_updates            | 4880         |
|    nu                   | 9.55         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00164     |
|    reward_explained_... | 0.846        |
|    reward_value_loss    | 0.035        |
|    std                  | 0.363        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.49e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10            |
|    forward_reward       | 0.2           |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.31         |
|    reward_forward       | 0.2           |
|    reward_survive       | 1             |
|    x_position           | 1.15          |
|    x_velocity           | 0.2           |
|    y_position           | 9.92          |
|    y_velocity           | 0.64          |
| rollout/                |               |
|    adjusted_reward      | 8.58          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.15e+03      |
| time/                   |               |
|    fps                  | 807           |
|    iterations           | 246           |
|    time_elapsed         | 3119          |
|    total_timesteps      | 2519040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10188         |
|    mean_motor0          | 0.45290223    |
|    mean_motor1          | 0.42065948    |
|    mean_motor2          | 0.6438601     |
|    mean_motor3          | 0.55887485    |
|    mean_motor4          | 0.40835792    |
|    mean_motor5          | 0.6821174     |
|    mean_motor6          | 0.44851723    |
|    mean_motor7          | 0.67921126    |
| train/                  |               |
|    approx_kl            | 0.036592238   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0827        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.937         |
|    cost_value_loss      | 5.76e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.17         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0287        |
|    mean_cost_advantages | 0.0018365982  |
|    mean_reward_advan... | -0.0016888153 |
|    n_updates            | 4900          |
|    nu                   | 9.6           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00206      |
|    reward_explained_... | 0.921         |
|    reward_value_loss    | 0.0312        |
|    std                  | 0.362         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.14e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 12.2          |
|    forward_reward       | 0.24          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.34         |
|    reward_forward       | 0.24          |
|    reward_survive       | 1             |
|    x_position           | 1.25          |
|    x_velocity           | 0.24          |
|    y_position           | 12.1          |
|    y_velocity           | 0.414         |
| rollout/                |               |
|    adjusted_reward      | 8.7           |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.16e+03      |
| time/                   |               |
|    fps                  | 807           |
|    iterations           | 247           |
|    time_elapsed         | 3132          |
|    total_timesteps      | 2529280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10179         |
|    mean_motor0          | 0.45556408    |
|    mean_motor1          | 0.42295337    |
|    mean_motor2          | 0.6494131     |
|    mean_motor3          | 0.5509644     |
|    mean_motor4          | 0.3991255     |
|    mean_motor5          | 0.6663776     |
|    mean_motor6          | 0.4346938     |
|    mean_motor7          | 0.69725335    |
| train/                  |               |
|    approx_kl            | 0.030773098   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0639        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.776         |
|    cost_value_loss      | 2.87e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.15         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0101        |
|    mean_cost_advantages | 0.00018006431 |
|    mean_reward_advan... | 0.0045282794  |
|    n_updates            | 4920          |
|    nu                   | 9.65          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00173      |
|    reward_explained_... | 0.924         |
|    reward_value_loss    | 0.0295        |
|    std                  | 0.361         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 405           |
|    mean_reward          | 3.74e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.7          |
|    forward_reward       | 0.238         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.36         |
|    reward_forward       | 0.238         |
|    reward_survive       | 1             |
|    x_position           | 0.477         |
|    x_velocity           | 0.238         |
|    y_position           | 10.7          |
|    y_velocity           | 0.643         |
| rollout/                |               |
|    adjusted_reward      | 8.96          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.2e+03       |
| time/                   |               |
|    fps                  | 807           |
|    iterations           | 248           |
|    time_elapsed         | 3145          |
|    total_timesteps      | 2539520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10172         |
|    mean_motor0          | 0.4476494     |
|    mean_motor1          | 0.44448608    |
|    mean_motor2          | 0.6448251     |
|    mean_motor3          | 0.5503968     |
|    mean_motor4          | 0.38980216    |
|    mean_motor5          | 0.6788822     |
|    mean_motor6          | 0.42267808    |
|    mean_motor7          | 0.6893573     |
| train/                  |               |
|    approx_kl            | 0.038710214   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0741        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.902         |
|    cost_value_loss      | 1.73e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.14         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0243        |
|    mean_cost_advantages | 0.00049564696 |
|    mean_reward_advan... | -0.0035075203 |
|    n_updates            | 4940          |
|    nu                   | 9.69          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00184      |
|    reward_explained_... | 0.909         |
|    reward_value_loss    | 0.0306        |
|    std                  | 0.361         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.13e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.71           |
|    forward_reward       | 0.44           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.47          |
|    reward_forward       | 0.44           |
|    reward_survive       | 1              |
|    x_position           | 0.154          |
|    x_velocity           | 0.44           |
|    y_position           | 6.65           |
|    y_velocity           | 0.748          |
| rollout/                |                |
|    adjusted_reward      | 8.63           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.26e+03       |
| time/                   |                |
|    fps                  | 807            |
|    iterations           | 249            |
|    time_elapsed         | 3158           |
|    total_timesteps      | 2549760        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10179          |
|    mean_motor0          | 0.4435864      |
|    mean_motor1          | 0.43610573     |
|    mean_motor2          | 0.6255961      |
|    mean_motor3          | 0.56182814     |
|    mean_motor4          | 0.39817116     |
|    mean_motor5          | 0.663755       |
|    mean_motor6          | 0.42446858     |
|    mean_motor7          | 0.6878251      |
| train/                  |                |
|    approx_kl            | 0.053772874    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0994         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.821          |
|    cost_value_loss      | 0.000792       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.11          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0137         |
|    mean_cost_advantages | -0.00065753463 |
|    mean_reward_advan... | -0.0049296087  |
|    n_updates            | 4960           |
|    nu                   | 9.73           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00228       |
|    reward_explained_... | 0.909          |
|    reward_value_loss    | 0.0319         |
|    std                  | 0.359          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.58e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.42e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.4          |
|    forward_reward       | 0.309        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.41        |
|    reward_forward       | 0.309        |
|    reward_survive       | 1            |
|    x_position           | 0.811        |
|    x_velocity           | 0.309        |
|    y_position           | 7.34         |
|    y_velocity           | 0.484        |
| rollout/                |              |
|    adjusted_reward      | 8.42         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.27e+03     |
| time/                   |              |
|    fps                  | 807          |
|    iterations           | 250          |
|    time_elapsed         | 3172         |
|    total_timesteps      | 2560000      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10172        |
|    mean_motor0          | 0.45804048   |
|    mean_motor1          | 0.4146134    |
|    mean_motor2          | 0.62594163   |
|    mean_motor3          | 0.5596689    |
|    mean_motor4          | 0.38690445   |
|    mean_motor5          | 0.66936064   |
|    mean_motor6          | 0.4316214    |
|    mean_motor7          | 0.66544133   |
| train/                  |              |
|    approx_kl            | 0.031358022  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0798       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.806        |
|    cost_value_loss      | 2.49e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -3.09        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0446       |
|    mean_cost_advantages | 0.0005493913 |
|    mean_reward_advan... | -0.011577348 |
|    n_updates            | 4980         |
|    nu                   | 9.76         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00147     |
|    reward_explained_... | 0.899        |
|    reward_value_loss    | 0.0369       |
|    std                  | 0.359        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.56e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.13          |
|    forward_reward       | 0.316         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.316         |
|    reward_survive       | 1             |
|    x_position           | 0.453         |
|    x_velocity           | 0.316         |
|    y_position           | 9.06          |
|    y_velocity           | 0.812         |
| rollout/                |               |
|    adjusted_reward      | 8.38          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.27e+03      |
| time/                   |               |
|    fps                  | 806           |
|    iterations           | 251           |
|    time_elapsed         | 3185          |
|    total_timesteps      | 2570240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10176         |
|    mean_motor0          | 0.4542094     |
|    mean_motor1          | 0.42544073    |
|    mean_motor2          | 0.63407874    |
|    mean_motor3          | 0.5955471     |
|    mean_motor4          | 0.40837556    |
|    mean_motor5          | 0.63649255    |
|    mean_motor6          | 0.42860684    |
|    mean_motor7          | 0.6540057     |
| train/                  |               |
|    approx_kl            | 0.044620723   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0826        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.859         |
|    cost_value_loss      | 0.00037       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.07         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00112      |
|    mean_cost_advantages | -0.0015753448 |
|    mean_reward_advan... | -0.014330378  |
|    n_updates            | 5000          |
|    nu                   | 9.8           |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00217      |
|    reward_explained_... | 0.881         |
|    reward_value_loss    | 0.0401        |
|    std                  | 0.357         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.43e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.92           |
|    forward_reward       | 0.35           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.35          |
|    reward_forward       | 0.35           |
|    reward_survive       | 1              |
|    x_position           | 0.0755         |
|    x_velocity           | 0.35           |
|    y_position           | 7.89           |
|    y_velocity           | 0.87           |
| rollout/                |                |
|    adjusted_reward      | 9.27           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.33e+03       |
| time/                   |                |
|    fps                  | 806            |
|    iterations           | 252            |
|    time_elapsed         | 3198           |
|    total_timesteps      | 2580480        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10180          |
|    mean_motor0          | 0.45586053     |
|    mean_motor1          | 0.42567044     |
|    mean_motor2          | 0.64625996     |
|    mean_motor3          | 0.6087086      |
|    mean_motor4          | 0.38446766     |
|    mean_motor5          | 0.66222095     |
|    mean_motor6          | 0.41227236     |
|    mean_motor7          | 0.6418308      |
| train/                  |                |
|    approx_kl            | 0.038769208    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0823         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.844          |
|    cost_value_loss      | 8.29e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.05          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0242         |
|    mean_cost_advantages | -0.00029315712 |
|    mean_reward_advan... | -0.019229876   |
|    n_updates            | 5020           |
|    nu                   | 9.82           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00195       |
|    reward_explained_... | 0.872          |
|    reward_value_loss    | 0.0461         |
|    std                  | 0.357          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.47e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.71          |
|    forward_reward       | 0.437         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.35         |
|    reward_forward       | 0.437         |
|    reward_survive       | 1             |
|    x_position           | 0.414         |
|    x_velocity           | 0.437         |
|    y_position           | 8.6           |
|    y_velocity           | 0.689         |
| rollout/                |               |
|    adjusted_reward      | 8.31          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.27e+03      |
| time/                   |               |
|    fps                  | 806           |
|    iterations           | 253           |
|    time_elapsed         | 3211          |
|    total_timesteps      | 2590720       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10166         |
|    mean_motor0          | 0.45284334    |
|    mean_motor1          | 0.4124725     |
|    mean_motor2          | 0.5979426     |
|    mean_motor3          | 0.54988533    |
|    mean_motor4          | 0.39864853    |
|    mean_motor5          | 0.64023894    |
|    mean_motor6          | 0.42239064    |
|    mean_motor7          | 0.6434761     |
| train/                  |               |
|    approx_kl            | 0.031397305   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0684        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.663         |
|    cost_value_loss      | 4.97e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.04         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0278        |
|    mean_cost_advantages | -7.397833e-05 |
|    mean_reward_advan... | -0.0031442598 |
|    n_updates            | 5040          |
|    nu                   | 9.85          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00148      |
|    reward_explained_... | 0.869         |
|    reward_value_loss    | 0.0408        |
|    std                  | 0.357         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.26e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.84          |
|    forward_reward       | 0.265         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.265         |
|    reward_survive       | 1             |
|    x_position           | 0.742         |
|    x_velocity           | 0.265         |
|    y_position           | 8.76          |
|    y_velocity           | 0.575         |
| rollout/                |               |
|    adjusted_reward      | 8.23          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.24e+03      |
| time/                   |               |
|    fps                  | 806           |
|    iterations           | 254           |
|    time_elapsed         | 3225          |
|    total_timesteps      | 2600960       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10189         |
|    mean_motor0          | 0.46646       |
|    mean_motor1          | 0.41036296    |
|    mean_motor2          | 0.60815626    |
|    mean_motor3          | 0.608146      |
|    mean_motor4          | 0.40307608    |
|    mean_motor5          | 0.661584      |
|    mean_motor6          | 0.42866945    |
|    mean_motor7          | 0.6354336     |
| train/                  |               |
|    approx_kl            | 0.031795263   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0725        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.94          |
|    cost_value_loss      | 3.98e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -3.04         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00843       |
|    mean_cost_advantages | -0.0009933674 |
|    mean_reward_advan... | -0.008881981  |
|    n_updates            | 5060          |
|    nu                   | 9.87          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00199      |
|    reward_explained_... | 0.918         |
|    reward_value_loss    | 0.0391        |
|    std                  | 0.357         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.22e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.68           |
|    forward_reward       | 0.254          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.29          |
|    reward_forward       | 0.254          |
|    reward_survive       | 1              |
|    x_position           | 1.05           |
|    x_velocity           | 0.254          |
|    y_position           | 9.6            |
|    y_velocity           | 0.596          |
| rollout/                |                |
|    adjusted_reward      | 8.92           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.28e+03       |
| time/                   |                |
|    fps                  | 806            |
|    iterations           | 255            |
|    time_elapsed         | 3238           |
|    total_timesteps      | 2611200        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10195          |
|    mean_motor0          | 0.44643778     |
|    mean_motor1          | 0.41647848     |
|    mean_motor2          | 0.62039566     |
|    mean_motor3          | 0.619268       |
|    mean_motor4          | 0.39774045     |
|    mean_motor5          | 0.671325       |
|    mean_motor6          | 0.43388373     |
|    mean_motor7          | 0.6352247      |
| train/                  |                |
|    approx_kl            | 0.03415925     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0748         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.728          |
|    cost_value_loss      | 7.21e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -3.01          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0299         |
|    mean_cost_advantages | -0.00015560832 |
|    mean_reward_advan... | -0.029206503   |
|    n_updates            | 5080           |
|    nu                   | 9.89           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00183       |
|    reward_explained_... | 0.809          |
|    reward_value_loss    | 0.0498         |
|    std                  | 0.355          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.47e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.47           |
|    forward_reward       | 0.208          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.31          |
|    reward_forward       | 0.208          |
|    reward_survive       | 1              |
|    x_position           | -0.0133        |
|    x_velocity           | 0.208          |
|    y_position           | 8.43           |
|    y_velocity           | 0.484          |
| rollout/                |                |
|    adjusted_reward      | 8.23           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 4.23e+03       |
| time/                   |                |
|    fps                  | 806            |
|    iterations           | 256            |
|    time_elapsed         | 3252           |
|    total_timesteps      | 2621440        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10179          |
|    mean_motor0          | 0.482032       |
|    mean_motor1          | 0.4182096      |
|    mean_motor2          | 0.61844784     |
|    mean_motor3          | 0.5714269      |
|    mean_motor4          | 0.3793276      |
|    mean_motor5          | 0.6798982      |
|    mean_motor6          | 0.41701674     |
|    mean_motor7          | 0.6474621      |
| train/                  |                |
|    approx_kl            | 0.032992147    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0725         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.91           |
|    cost_value_loss      | 7.92e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.97          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0272         |
|    mean_cost_advantages | -0.00015801225 |
|    mean_reward_advan... | -0.008672057   |
|    n_updates            | 5100           |
|    nu                   | 9.91           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00179       |
|    reward_explained_... | 0.878          |
|    reward_value_loss    | 0.04           |
|    std                  | 0.353          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.26e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.6           |
|    forward_reward       | 0.242          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.36          |
|    reward_forward       | 0.242          |
|    reward_survive       | 1              |
|    x_position           | -0.412         |
|    x_velocity           | 0.242          |
|    y_position           | 10.5           |
|    y_velocity           | 0.689          |
| rollout/                |                |
|    adjusted_reward      | 8.39           |
|    ep_len_mean          | 483            |
|    ep_rew_mean          | 4.06e+03       |
| time/                   |                |
|    fps                  | 805            |
|    iterations           | 257            |
|    time_elapsed         | 3265           |
|    total_timesteps      | 2631680        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10169          |
|    mean_motor0          | 0.44291645     |
|    mean_motor1          | 0.4270056      |
|    mean_motor2          | 0.60781443     |
|    mean_motor3          | 0.56627274     |
|    mean_motor4          | 0.38104573     |
|    mean_motor5          | 0.6642012      |
|    mean_motor6          | 0.41022605     |
|    mean_motor7          | 0.6523901      |
| train/                  |                |
|    approx_kl            | 0.035314213    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0704         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.758          |
|    cost_value_loss      | 6.14e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.93          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0297         |
|    mean_cost_advantages | -0.00017947148 |
|    mean_reward_advan... | -0.023847833   |
|    n_updates            | 5120           |
|    nu                   | 9.93           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00192       |
|    reward_explained_... | 0.857          |
|    reward_value_loss    | 0.0454         |
|    std                  | 0.352          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.58e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.56e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 11.8          |
|    forward_reward       | 0.165         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.34         |
|    reward_forward       | 0.165         |
|    reward_survive       | 1             |
|    x_position           | -0.0977       |
|    x_velocity           | 0.165         |
|    y_position           | 11.8          |
|    y_velocity           | 0.429         |
| rollout/                |               |
|    adjusted_reward      | 8.75          |
|    ep_len_mean          | 483           |
|    ep_rew_mean          | 4.1e+03       |
| time/                   |               |
|    fps                  | 805           |
|    iterations           | 258           |
|    time_elapsed         | 3279          |
|    total_timesteps      | 2641920       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10171         |
|    mean_motor0          | 0.42439502    |
|    mean_motor1          | 0.44575486    |
|    mean_motor2          | 0.6097683     |
|    mean_motor3          | 0.56063837    |
|    mean_motor4          | 0.39188975    |
|    mean_motor5          | 0.68526524    |
|    mean_motor6          | 0.41857243    |
|    mean_motor7          | 0.6303141     |
| train/                  |               |
|    approx_kl            | 0.031828538   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0796        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.693         |
|    cost_value_loss      | 2.7e-06       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.91         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00253       |
|    mean_cost_advantages | -0.0002150574 |
|    mean_reward_advan... | -0.030072462  |
|    n_updates            | 5140          |
|    nu                   | 9.94          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00156      |
|    reward_explained_... | 0.85          |
|    reward_value_loss    | 0.0507        |
|    std                  | 0.35          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.58e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.71e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.5           |
|    forward_reward       | 0.159          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.23          |
|    reward_forward       | 0.159          |
|    reward_survive       | 1              |
|    x_position           | 0.392          |
|    x_velocity           | 0.159          |
|    y_position           | 10.4           |
|    y_velocity           | 0.402          |
| rollout/                |                |
|    adjusted_reward      | 9.18           |
|    ep_len_mean          | 483            |
|    ep_rew_mean          | 4.19e+03       |
| time/                   |                |
|    fps                  | 805            |
|    iterations           | 259            |
|    time_elapsed         | 3292           |
|    total_timesteps      | 2652160        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10173          |
|    mean_motor0          | 0.4262773      |
|    mean_motor1          | 0.4453177      |
|    mean_motor2          | 0.6188308      |
|    mean_motor3          | 0.5557593      |
|    mean_motor4          | 0.3934267      |
|    mean_motor5          | 0.6838647      |
|    mean_motor6          | 0.41587466     |
|    mean_motor7          | 0.6173779      |
| train/                  |                |
|    approx_kl            | 0.03252696     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0752         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.944          |
|    cost_value_loss      | 7.71e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.89          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0218         |
|    mean_cost_advantages | -0.00028726627 |
|    mean_reward_advan... | -0.017267518   |
|    n_updates            | 5160           |
|    nu                   | 9.95           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00145       |
|    reward_explained_... | 0.888          |
|    reward_value_loss    | 0.0434         |
|    std                  | 0.35           |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.71e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.71e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.67         |
|    forward_reward       | 0.268        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.36        |
|    reward_forward       | 0.268        |
|    reward_survive       | 1            |
|    x_position           | -0.245       |
|    x_velocity           | 0.268        |
|    y_position           | 8.64         |
|    y_velocity           | 0.48         |
| rollout/                |              |
|    adjusted_reward      | 8.39         |
|    ep_len_mean          | 483          |
|    ep_rew_mean          | 4.15e+03     |
| time/                   |              |
|    fps                  | 805          |
|    iterations           | 260          |
|    time_elapsed         | 3305         |
|    total_timesteps      | 2662400      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10170        |
|    mean_motor0          | 0.47251973   |
|    mean_motor1          | 0.43528867   |
|    mean_motor2          | 0.61633956   |
|    mean_motor3          | 0.5308484    |
|    mean_motor4          | 0.4087805    |
|    mean_motor5          | 0.68593144   |
|    mean_motor6          | 0.43020827   |
|    mean_motor7          | 0.6283649    |
| train/                  |              |
|    approx_kl            | 0.030955726  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.827        |
|    cost_value_loss      | 2.34e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.88        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0189       |
|    mean_cost_advantages | 3.521062e-05 |
|    mean_reward_advan... | -0.013208011 |
|    n_updates            | 5180         |
|    nu                   | 9.97         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | 0.848        |
|    reward_value_loss    | 0.0428       |
|    std                  | 0.35         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.71e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.67e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.17          |
|    forward_reward       | 0.213         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.38         |
|    reward_forward       | 0.213         |
|    reward_survive       | 1             |
|    x_position           | -0.254        |
|    x_velocity           | 0.213         |
|    y_position           | 9.13          |
|    y_velocity           | 0.563         |
| rollout/                |               |
|    adjusted_reward      | 9.02          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 4.25e+03      |
| time/                   |               |
|    fps                  | 805           |
|    iterations           | 261           |
|    time_elapsed         | 3319          |
|    total_timesteps      | 2672640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10175         |
|    mean_motor0          | 0.44150954    |
|    mean_motor1          | 0.4432099     |
|    mean_motor2          | 0.62403595    |
|    mean_motor3          | 0.52772474    |
|    mean_motor4          | 0.42040005    |
|    mean_motor5          | 0.6815933     |
|    mean_motor6          | 0.40633598    |
|    mean_motor7          | 0.6342605     |
| train/                  |               |
|    approx_kl            | 0.040998198   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0809        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.932         |
|    cost_value_loss      | 1.3e-05       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.87         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0221        |
|    mean_cost_advantages | -0.0003509003 |
|    mean_reward_advan... | -0.013806462  |
|    n_updates            | 5200          |
|    nu                   | 9.98          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00183      |
|    reward_explained_... | 0.916         |
|    reward_value_loss    | 0.0394        |
|    std                  | 0.349         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.71e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.66e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.14           |
|    forward_reward       | 0.34           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.35          |
|    reward_forward       | 0.34           |
|    reward_survive       | 1              |
|    x_position           | -0.246         |
|    x_velocity           | 0.34           |
|    y_position           | 8.11           |
|    y_velocity           | 0.798          |
| rollout/                |                |
|    adjusted_reward      | 8.91           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.37e+03       |
| time/                   |                |
|    fps                  | 805            |
|    iterations           | 262            |
|    time_elapsed         | 3332           |
|    total_timesteps      | 2682880        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10171          |
|    mean_motor0          | 0.4340821      |
|    mean_motor1          | 0.43993688     |
|    mean_motor2          | 0.6337264      |
|    mean_motor3          | 0.5332649      |
|    mean_motor4          | 0.42102545     |
|    mean_motor5          | 0.6928007      |
|    mean_motor6          | 0.41717052     |
|    mean_motor7          | 0.62812865     |
| train/                  |                |
|    approx_kl            | 0.030973945    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0631         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.863          |
|    cost_value_loss      | 5.93e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.86          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0126         |
|    mean_cost_advantages | -8.5303654e-05 |
|    mean_reward_advan... | 0.0005980611   |
|    n_updates            | 5220           |
|    nu                   | 9.99           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00201       |
|    reward_explained_... | 0.921          |
|    reward_value_loss    | 0.0365         |
|    std                  | 0.349          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.71e+03       |
|    mean_ep_length       | 431            |
|    mean_reward          | 4.08e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.87           |
|    forward_reward       | 0.354          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.43          |
|    reward_forward       | 0.354          |
|    reward_survive       | 1              |
|    x_position           | -0.621         |
|    x_velocity           | 0.354          |
|    y_position           | 8.79           |
|    y_velocity           | 0.758          |
| rollout/                |                |
|    adjusted_reward      | 9.55           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.46e+03       |
| time/                   |                |
|    fps                  | 804            |
|    iterations           | 263            |
|    time_elapsed         | 3345           |
|    total_timesteps      | 2693120        |
| torque/                 |                |
|    greater_than_0.25    | 10238          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10178          |
|    mean_motor0          | 0.43866634     |
|    mean_motor1          | 0.45053536     |
|    mean_motor2          | 0.648265       |
|    mean_motor3          | 0.536799       |
|    mean_motor4          | 0.39885658     |
|    mean_motor5          | 0.707772       |
|    mean_motor6          | 0.40292677     |
|    mean_motor7          | 0.6408         |
| train/                  |                |
|    approx_kl            | 0.026386702    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0736         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.881          |
|    cost_value_loss      | 3.3e-06        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.84          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0348         |
|    mean_cost_advantages | -1.1460312e-05 |
|    mean_reward_advan... | -0.009042898   |
|    n_updates            | 5240           |
|    nu                   | 10             |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00181       |
|    reward_explained_... | 0.892          |
|    reward_value_loss    | 0.0408         |
|    std                  | 0.347          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.71e+03      |
|    mean_ep_length       | 498           |
|    mean_reward          | 4.39e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.75          |
|    forward_reward       | 0.286         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.34         |
|    reward_forward       | 0.286         |
|    reward_survive       | 1             |
|    x_position           | 0.0843        |
|    x_velocity           | 0.286         |
|    y_position           | 8.7           |
|    y_velocity           | 0.632         |
| rollout/                |               |
|    adjusted_reward      | 8.56          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.43e+03      |
| time/                   |               |
|    fps                  | 804           |
|    iterations           | 264           |
|    time_elapsed         | 3358          |
|    total_timesteps      | 2703360       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10163         |
|    mean_motor0          | 0.4455112     |
|    mean_motor1          | 0.4438266     |
|    mean_motor2          | 0.6203939     |
|    mean_motor3          | 0.52216434    |
|    mean_motor4          | 0.41700754    |
|    mean_motor5          | 0.64351827    |
|    mean_motor6          | 0.42391855    |
|    mean_motor7          | 0.6751339     |
| train/                  |               |
|    approx_kl            | 0.039668165   |
|    average_cost         | 0.00390625    |
|    clip_fraction        | 0.0808        |
|    clip_range           | 0.4           |
|    cost_explained_va... | -165          |
|    cost_value_loss      | 0.00258       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.8          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0432        |
|    mean_cost_advantages | 0.005965279   |
|    mean_reward_advan... | -0.0017412618 |
|    n_updates            | 5260          |
|    nu                   | 10            |
|    nu_loss              | -0.039        |
|    policy_gradient_loss | -0.00172      |
|    reward_explained_... | 0.861         |
|    reward_value_loss    | 0.0396        |
|    std                  | 0.346         |
|    total_cost           | 40.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.76e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.76e+03     |
| infos/                  |              |
|    cost                 | 0.033        |
|    distance_from_origin | 6.94         |
|    forward_reward       | 0.337        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.48        |
|    reward_forward       | 0.337        |
|    reward_survive       | 1            |
|    x_position           | -1.1         |
|    x_velocity           | 0.337        |
|    y_position           | 6.79         |
|    y_velocity           | 0.986        |
| rollout/                |              |
|    adjusted_reward      | 8.8          |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.45e+03     |
| time/                   |              |
|    fps                  | 804          |
|    iterations           | 265          |
|    time_elapsed         | 3372         |
|    total_timesteps      | 2713600      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10173        |
|    mean_motor0          | 0.4355772    |
|    mean_motor1          | 0.51619804   |
|    mean_motor2          | 0.6320592    |
|    mean_motor3          | 0.48674092   |
|    mean_motor4          | 0.43771157   |
|    mean_motor5          | 0.6629862    |
|    mean_motor6          | 0.413971     |
|    mean_motor7          | 0.6599645    |
| train/                  |              |
|    approx_kl            | 0.09888003   |
|    average_cost         | 0.009765625  |
|    clip_fraction        | 0.182        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.678        |
|    cost_value_loss      | 0.00459      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.79        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.019        |
|    mean_cost_advantages | 0.012334492  |
|    mean_reward_advan... | -0.005286991 |
|    n_updates            | 5280         |
|    nu                   | 10           |
|    nu_loss              | -0.0977      |
|    policy_gradient_loss | -0.005       |
|    reward_explained_... | 0.917        |
|    reward_value_loss    | 0.0391       |
|    std                  | 0.346        |
|    total_cost           | 100.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.76e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.98e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.78          |
|    forward_reward       | 0.259         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.259         |
|    reward_survive       | 1             |
|    x_position           | 0.13          |
|    x_velocity           | 0.259         |
|    y_position           | 6.76          |
|    y_velocity           | 0.648         |
| rollout/                |               |
|    adjusted_reward      | 8.74          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.47e+03      |
| time/                   |               |
|    fps                  | 804           |
|    iterations           | 266           |
|    time_elapsed         | 3385          |
|    total_timesteps      | 2723840       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10171         |
|    mean_motor0          | 0.42239112    |
|    mean_motor1          | 0.48090178    |
|    mean_motor2          | 0.6311086     |
|    mean_motor3          | 0.48211947    |
|    mean_motor4          | 0.42714024    |
|    mean_motor5          | 0.68786573    |
|    mean_motor6          | 0.43586245    |
|    mean_motor7          | 0.6044999     |
| train/                  |               |
|    approx_kl            | 0.07992391    |
|    average_cost         | 0.009570313   |
|    clip_fraction        | 0.174         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.625         |
|    cost_value_loss      | 0.00299       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.79         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00471       |
|    mean_cost_advantages | 0.013767473   |
|    mean_reward_advan... | -0.0040851063 |
|    n_updates            | 5300          |
|    nu                   | 10            |
|    nu_loss              | -0.0959       |
|    policy_gradient_loss | -0.00406      |
|    reward_explained_... | 0.931         |
|    reward_value_loss    | 0.0348        |
|    std                  | 0.346         |
|    total_cost           | 98.0          |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.76e+03       |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.34e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.96           |
|    forward_reward       | 0.239          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.33          |
|    reward_forward       | 0.239          |
|    reward_survive       | 1              |
|    x_position           | -0.0248        |
|    x_velocity           | 0.239          |
|    y_position           | 9.93           |
|    y_velocity           | 0.564          |
| rollout/                |                |
|    adjusted_reward      | 8.69           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 4.38e+03       |
| time/                   |                |
|    fps                  | 804            |
|    iterations           | 267            |
|    time_elapsed         | 3399           |
|    total_timesteps      | 2734080        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10168          |
|    mean_motor0          | 0.42089525     |
|    mean_motor1          | 0.47594637     |
|    mean_motor2          | 0.625867       |
|    mean_motor3          | 0.48184377     |
|    mean_motor4          | 0.4466373      |
|    mean_motor5          | 0.7041936      |
|    mean_motor6          | 0.44025397     |
|    mean_motor7          | 0.6126829      |
| train/                  |                |
|    approx_kl            | 0.034706693    |
|    average_cost         | 0.0026367188   |
|    clip_fraction        | 0.094          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.853          |
|    cost_value_loss      | 0.0014         |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.78          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0529         |
|    mean_cost_advantages | -8.9809284e-05 |
|    mean_reward_advan... | -0.00846129    |
|    n_updates            | 5320           |
|    nu                   | 10.1           |
|    nu_loss              | -0.0265        |
|    policy_gradient_loss | -0.00201       |
|    reward_explained_... | 0.914          |
|    reward_value_loss    | 0.0333         |
|    std                  | 0.345          |
|    total_cost           | 27.0           |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.76e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.94e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.2          |
|    forward_reward       | 0.252         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.33         |
|    reward_forward       | 0.252         |
|    reward_survive       | 1             |
|    x_position           | 0.0622        |
|    x_velocity           | 0.252         |
|    y_position           | 10.2          |
|    y_velocity           | 0.753         |
| rollout/                |               |
|    adjusted_reward      | 8.32          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.26e+03      |
| time/                   |               |
|    fps                  | 804           |
|    iterations           | 268           |
|    time_elapsed         | 3412          |
|    total_timesteps      | 2744320       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10179         |
|    mean_motor0          | 0.4611372     |
|    mean_motor1          | 0.44446746    |
|    mean_motor2          | 0.62954974    |
|    mean_motor3          | 0.48913616    |
|    mean_motor4          | 0.43569627    |
|    mean_motor5          | 0.7303099     |
|    mean_motor6          | 0.47509828    |
|    mean_motor7          | 0.6004741     |
| train/                  |               |
|    approx_kl            | 0.025707912   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0578        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.889         |
|    cost_value_loss      | 1.91e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.76         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00538       |
|    mean_cost_advantages | -0.0005478339 |
|    mean_reward_advan... | -0.00830925   |
|    n_updates            | 5340          |
|    nu                   | 10.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00141      |
|    reward_explained_... | 0.905         |
|    reward_value_loss    | 0.0357        |
|    std                  | 0.344         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 4.76e+03      |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.6e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 12.6          |
|    forward_reward       | 0.192         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.51         |
|    reward_forward       | 0.192         |
|    reward_survive       | 1             |
|    x_position           | -0.909        |
|    x_velocity           | 0.192         |
|    y_position           | 12.5          |
|    y_velocity           | 0.374         |
| rollout/                |               |
|    adjusted_reward      | 9.2           |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.32e+03      |
| time/                   |               |
|    fps                  | 804           |
|    iterations           | 269           |
|    time_elapsed         | 3425          |
|    total_timesteps      | 2754560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10169         |
|    mean_motor0          | 0.4346047     |
|    mean_motor1          | 0.4751607     |
|    mean_motor2          | 0.6323543     |
|    mean_motor3          | 0.4902823     |
|    mean_motor4          | 0.4389659     |
|    mean_motor5          | 0.701442      |
|    mean_motor6          | 0.45222718    |
|    mean_motor7          | 0.6043301     |
| train/                  |               |
|    approx_kl            | 0.034906905   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0817        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.938         |
|    cost_value_loss      | 1.99e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.72         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00904       |
|    mean_cost_advantages | -0.0005809645 |
|    mean_reward_advan... | -0.014186812  |
|    n_updates            | 5360          |
|    nu                   | 10.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00184      |
|    reward_explained_... | 0.894         |
|    reward_value_loss    | 0.0403        |
|    std                  | 0.342         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 4.76e+03       |
|    mean_ep_length       | 417            |
|    mean_reward          | 3.18e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.85           |
|    forward_reward       | 0.303          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.38          |
|    reward_forward       | 0.303          |
|    reward_survive       | 1              |
|    x_position           | -0.373         |
|    x_velocity           | 0.303          |
|    y_position           | 6.7            |
|    y_velocity           | 0.38           |
| rollout/                |                |
|    adjusted_reward      | 8.75           |
|    ep_len_mean          | 490            |
|    ep_rew_mean          | 4.3e+03        |
| time/                   |                |
|    fps                  | 803            |
|    iterations           | 270            |
|    time_elapsed         | 3438           |
|    total_timesteps      | 2764800        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10160          |
|    mean_motor0          | 0.45752844     |
|    mean_motor1          | 0.4681964      |
|    mean_motor2          | 0.6215842      |
|    mean_motor3          | 0.47449836     |
|    mean_motor4          | 0.44808784     |
|    mean_motor5          | 0.7184036      |
|    mean_motor6          | 0.45082527     |
|    mean_motor7          | 0.61025447     |
| train/                  |                |
|    approx_kl            | 0.044081826    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0887         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.914          |
|    cost_value_loss      | 0.000618       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.69          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00567        |
|    mean_cost_advantages | -0.00082651817 |
|    mean_reward_advan... | -0.0071443305  |
|    n_updates            | 5380           |
|    nu                   | 10.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00214       |
|    reward_explained_... | 0.884          |
|    reward_value_loss    | 0.042          |
|    std                  | 0.341          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 4.76e+03     |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.66e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.82         |
|    forward_reward       | 0.195        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.33        |
|    reward_forward       | 0.195        |
|    reward_survive       | 1            |
|    x_position           | 0.038        |
|    x_velocity           | 0.195        |
|    y_position           | 7.76         |
|    y_velocity           | 0.459        |
| rollout/                |              |
|    adjusted_reward      | 8.9          |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 4.31e+03     |
| time/                   |              |
|    fps                  | 803          |
|    iterations           | 271          |
|    time_elapsed         | 3452         |
|    total_timesteps      | 2775040      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10175        |
|    mean_motor0          | 0.41348472   |
|    mean_motor1          | 0.45265618   |
|    mean_motor2          | 0.6151737    |
|    mean_motor3          | 0.49259752   |
|    mean_motor4          | 0.4481644    |
|    mean_motor5          | 0.6854575    |
|    mean_motor6          | 0.4676784    |
|    mean_motor7          | 0.62709725   |
| train/                  |              |
|    approx_kl            | 0.19137275   |
|    average_cost         | 0.02861328   |
|    clip_fraction        | 0.216        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.881        |
|    cost_value_loss      | 0.0158       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.66        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00787     |
|    mean_cost_advantages | 0.029500019  |
|    mean_reward_advan... | -0.011464389 |
|    n_updates            | 5400         |
|    nu                   | 10.1         |
|    nu_loss              | -0.289       |
|    policy_gradient_loss | -0.00796     |
|    reward_explained_... | 0.925        |
|    reward_value_loss    | 0.0424       |
|    std                  | 0.34         |
|    total_cost           | 293.0        |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 5e+03       |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 6.13        |
|    forward_reward       | 0.288       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.4        |
|    reward_forward       | 0.288       |
|    reward_survive       | 1           |
|    x_position           | -0.0711     |
|    x_velocity           | 0.288       |
|    y_position           | 6.11        |
|    y_velocity           | 0.629       |
| rollout/                |             |
|    adjusted_reward      | 8.54        |
|    ep_len_mean          | 495         |
|    ep_rew_mean          | 4.34e+03    |
| time/                   |             |
|    fps                  | 803         |
|    iterations           | 272         |
|    time_elapsed         | 3465        |
|    total_timesteps      | 2785280     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10149       |
|    mean_motor0          | 0.42359295  |
|    mean_motor1          | 0.42382663  |
|    mean_motor2          | 0.6374348   |
|    mean_motor3          | 0.4035937   |
|    mean_motor4          | 0.47900945  |
|    mean_motor5          | 0.63363886  |
|    mean_motor6          | 0.52195215  |
|    mean_motor7          | 0.6339383   |
| train/                  |             |
|    approx_kl            | 0.7245633   |
|    average_cost         | 0.033203125 |
|    clip_fraction        | 0.418       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.911       |
|    cost_value_loss      | 0.0123      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -2.65       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0108      |
|    mean_cost_advantages | 0.03575643  |
|    mean_reward_advan... | -0.02014662 |
|    n_updates            | 5420        |
|    nu                   | 10.2        |
|    nu_loss              | -0.337      |
|    policy_gradient_loss | -0.0071     |
|    reward_explained_... | 0.822       |
|    reward_value_loss    | 0.0485      |
|    std                  | 0.34        |
|    total_cost           | 340.0       |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.32e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.55           |
|    forward_reward       | 0.365          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.42          |
|    reward_forward       | 0.365          |
|    reward_survive       | 1              |
|    x_position           | 0.312          |
|    x_velocity           | 0.365          |
|    y_position           | 8.47           |
|    y_velocity           | 0.869          |
| rollout/                |                |
|    adjusted_reward      | 8.03           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 4.32e+03       |
| time/                   |                |
|    fps                  | 803            |
|    iterations           | 273            |
|    time_elapsed         | 3479           |
|    total_timesteps      | 2795520        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10154          |
|    mean_motor0          | 0.44222203     |
|    mean_motor1          | 0.44731134     |
|    mean_motor2          | 0.64006877     |
|    mean_motor3          | 0.40276927     |
|    mean_motor4          | 0.4815155      |
|    mean_motor5          | 0.61310697     |
|    mean_motor6          | 0.51694965     |
|    mean_motor7          | 0.6649177      |
| train/                  |                |
|    approx_kl            | 0.040541325    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0664         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.928          |
|    cost_value_loss      | 0.000151       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.66          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0114         |
|    mean_cost_advantages | -0.00074561656 |
|    mean_reward_advan... | 6.577545e-05   |
|    n_updates            | 5440           |
|    nu                   | 10.2           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00159       |
|    reward_explained_... | 0.935          |
|    reward_value_loss    | 0.0309         |
|    std                  | 0.34           |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.08e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.83          |
|    forward_reward       | 0.353         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.353         |
|    reward_survive       | 1             |
|    x_position           | 1.85          |
|    x_velocity           | 0.353         |
|    y_position           | 8.56          |
|    y_velocity           | 0.331         |
| rollout/                |               |
|    adjusted_reward      | 8.39          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.24e+03      |
| time/                   |               |
|    fps                  | 803           |
|    iterations           | 274           |
|    time_elapsed         | 3492          |
|    total_timesteps      | 2805760       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10175         |
|    mean_motor0          | 0.42244858    |
|    mean_motor1          | 0.43150765    |
|    mean_motor2          | 0.661291      |
|    mean_motor3          | 0.41198054    |
|    mean_motor4          | 0.5063226     |
|    mean_motor5          | 0.6212675     |
|    mean_motor6          | 0.5757005     |
|    mean_motor7          | 0.6287654     |
| train/                  |               |
|    approx_kl            | 0.56617296    |
|    average_cost         | 0.036230467   |
|    clip_fraction        | 0.284         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.946         |
|    cost_value_loss      | 0.025         |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.68         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.0356       |
|    mean_cost_advantages | 0.035067663   |
|    mean_reward_advan... | 0.00011071526 |
|    n_updates            | 5460          |
|    nu                   | 10.3          |
|    nu_loss              | -0.371        |
|    policy_gradient_loss | -0.0144       |
|    reward_explained_... | 0.93          |
|    reward_value_loss    | 0.0291        |
|    std                  | 0.341         |
|    total_cost           | 371.0         |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.69e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.45          |
|    forward_reward       | 0.245         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.36         |
|    reward_forward       | 0.245         |
|    reward_survive       | 1             |
|    x_position           | 0.785         |
|    x_velocity           | 0.245         |
|    y_position           | 9.36          |
|    y_velocity           | 0.734         |
| rollout/                |               |
|    adjusted_reward      | 7.11          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.06e+03      |
| time/                   |               |
|    fps                  | 803           |
|    iterations           | 275           |
|    time_elapsed         | 3505          |
|    total_timesteps      | 2816000       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10160         |
|    mean_motor0          | 0.44063163    |
|    mean_motor1          | 0.4371706     |
|    mean_motor2          | 0.6353088     |
|    mean_motor3          | 0.3995906     |
|    mean_motor4          | 0.50875825    |
|    mean_motor5          | 0.5988661     |
|    mean_motor6          | 0.5393098     |
|    mean_motor7          | 0.6609484     |
| train/                  |               |
|    approx_kl            | 0.03204908    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.061         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.758         |
|    cost_value_loss      | 9.08e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.65         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00556       |
|    mean_cost_advantages | 0.00095891784 |
|    mean_reward_advan... | -0.0026545217 |
|    n_updates            | 5480          |
|    nu                   | 10.3          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00168      |
|    reward_explained_... | 0.915         |
|    reward_value_loss    | 0.0331        |
|    std                  | 0.339         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.37e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.55          |
|    forward_reward       | 0.25          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.38         |
|    reward_forward       | 0.25          |
|    reward_survive       | 1             |
|    x_position           | 0.252         |
|    x_velocity           | 0.25          |
|    y_position           | 8.48          |
|    y_velocity           | 0.715         |
| rollout/                |               |
|    adjusted_reward      | 8.17          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 3.97e+03      |
| time/                   |               |
|    fps                  | 803           |
|    iterations           | 276           |
|    time_elapsed         | 3519          |
|    total_timesteps      | 2826240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10160         |
|    mean_motor0          | 0.4170509     |
|    mean_motor1          | 0.40869626    |
|    mean_motor2          | 0.65225303    |
|    mean_motor3          | 0.40303364    |
|    mean_motor4          | 0.4860328     |
|    mean_motor5          | 0.5942745     |
|    mean_motor6          | 0.59636       |
|    mean_motor7          | 0.6038802     |
| train/                  |               |
|    approx_kl            | 0.04246753    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0936        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.894         |
|    cost_value_loss      | 2.75e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.62         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0108        |
|    mean_cost_advantages | 0.0012002189  |
|    mean_reward_advan... | -0.0074519566 |
|    n_updates            | 5500          |
|    nu                   | 10.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00218      |
|    reward_explained_... | 0.957         |
|    reward_value_loss    | 0.0295        |
|    std                  | 0.338         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.93e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.9          |
|    forward_reward       | 0.223        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.223        |
|    reward_survive       | 1            |
|    x_position           | 0.666        |
|    x_velocity           | 0.223        |
|    y_position           | 7.85         |
|    y_velocity           | 0.751        |
| rollout/                |              |
|    adjusted_reward      | 7.63         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 3.93e+03     |
| time/                   |              |
|    fps                  | 802          |
|    iterations           | 277          |
|    time_elapsed         | 3532         |
|    total_timesteps      | 2836480      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10156        |
|    mean_motor0          | 0.49150673   |
|    mean_motor1          | 0.41659722   |
|    mean_motor2          | 0.6541165    |
|    mean_motor3          | 0.4108355    |
|    mean_motor4          | 0.48719716   |
|    mean_motor5          | 0.600164     |
|    mean_motor6          | 0.60266805   |
|    mean_motor7          | 0.6159576    |
| train/                  |              |
|    approx_kl            | 0.034520697  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0869       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.851        |
|    cost_value_loss      | 2.43e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.6         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0113       |
|    mean_cost_advantages | 0.0010965855 |
|    mean_reward_advan... | -0.004661426 |
|    n_updates            | 5520         |
|    nu                   | 10.4         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00207     |
|    reward_explained_... | 0.922        |
|    reward_value_loss    | 0.0319       |
|    std                  | 0.338        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.01e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.58          |
|    forward_reward       | 0.286         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.286         |
|    reward_survive       | 1             |
|    x_position           | 0.673         |
|    x_velocity           | 0.286         |
|    y_position           | 9.53          |
|    y_velocity           | 0.66          |
| rollout/                |               |
|    adjusted_reward      | 7.57          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 3.84e+03      |
| time/                   |               |
|    fps                  | 802           |
|    iterations           | 278           |
|    time_elapsed         | 3545          |
|    total_timesteps      | 2846720       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10153         |
|    mean_motor0          | 0.5022307     |
|    mean_motor1          | 0.42912072    |
|    mean_motor2          | 0.65558785    |
|    mean_motor3          | 0.4093977     |
|    mean_motor4          | 0.4908215     |
|    mean_motor5          | 0.61358887    |
|    mean_motor6          | 0.6125225     |
|    mean_motor7          | 0.61860806    |
| train/                  |               |
|    approx_kl            | 0.035574056   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0905        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.921         |
|    cost_value_loss      | 1.41e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.58         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00727       |
|    mean_cost_advantages | 0.00031140534 |
|    mean_reward_advan... | -0.020712039  |
|    n_updates            | 5540          |
|    nu                   | 10.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00187      |
|    reward_explained_... | 0.887         |
|    reward_value_loss    | 0.0417        |
|    std                  | 0.336         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.84e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.91          |
|    forward_reward       | 0.15          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.35         |
|    reward_forward       | 0.15          |
|    reward_survive       | 1             |
|    x_position           | 0.615         |
|    x_velocity           | 0.15          |
|    y_position           | 9.85          |
|    y_velocity           | 0.273         |
| rollout/                |               |
|    adjusted_reward      | 9.34          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 3.93e+03      |
| time/                   |               |
|    fps                  | 802           |
|    iterations           | 279           |
|    time_elapsed         | 3559          |
|    total_timesteps      | 2856960       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10150         |
|    mean_motor0          | 0.4258512     |
|    mean_motor1          | 0.42762166    |
|    mean_motor2          | 0.6655632     |
|    mean_motor3          | 0.38226533    |
|    mean_motor4          | 0.49642473    |
|    mean_motor5          | 0.57642275    |
|    mean_motor6          | 0.58104074    |
|    mean_motor7          | 0.60176766    |
| train/                  |               |
|    approx_kl            | 0.04324007    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0941        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.922         |
|    cost_value_loss      | 1.04e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.55         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0221        |
|    mean_cost_advantages | 0.00058388215 |
|    mean_reward_advan... | -0.0147423595 |
|    n_updates            | 5560          |
|    nu                   | 10.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00197      |
|    reward_explained_... | 0.898         |
|    reward_value_loss    | 0.0344        |
|    std                  | 0.336         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.88e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 11            |
|    forward_reward       | 0.152         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.19         |
|    reward_forward       | 0.152         |
|    reward_survive       | 1             |
|    x_position           | 0.611         |
|    x_velocity           | 0.152         |
|    y_position           | 10.9          |
|    y_velocity           | 0.251         |
| rollout/                |               |
|    adjusted_reward      | 8.46          |
|    ep_len_mean          | 490           |
|    ep_rew_mean          | 4.01e+03      |
| time/                   |               |
|    fps                  | 802           |
|    iterations           | 280           |
|    time_elapsed         | 3572          |
|    total_timesteps      | 2867200       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10153         |
|    mean_motor0          | 0.43597397    |
|    mean_motor1          | 0.4312581     |
|    mean_motor2          | 0.6701466     |
|    mean_motor3          | 0.39108366    |
|    mean_motor4          | 0.49300194    |
|    mean_motor5          | 0.6005854     |
|    mean_motor6          | 0.61152613    |
|    mean_motor7          | 0.59561116    |
| train/                  |               |
|    approx_kl            | 0.02782385    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0651        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.888         |
|    cost_value_loss      | 8.79e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.55         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00591       |
|    mean_cost_advantages | 0.00032518228 |
|    mean_reward_advan... | 0.0056167403  |
|    n_updates            | 5580          |
|    nu                   | 10.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00147      |
|    reward_explained_... | 0.838         |
|    reward_value_loss    | 0.0364        |
|    std                  | 0.336         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.09e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.88         |
|    forward_reward       | 0.172        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.35        |
|    reward_forward       | 0.172        |
|    reward_survive       | 1            |
|    x_position           | 0.0844       |
|    x_velocity           | 0.172        |
|    y_position           | 8.81         |
|    y_velocity           | 0.257        |
| rollout/                |              |
|    adjusted_reward      | 8.61         |
|    ep_len_mean          | 486          |
|    ep_rew_mean          | 4.01e+03     |
| time/                   |              |
|    fps                  | 802          |
|    iterations           | 281          |
|    time_elapsed         | 3585         |
|    total_timesteps      | 2877440      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10148        |
|    mean_motor0          | 0.4146912    |
|    mean_motor1          | 0.4465924    |
|    mean_motor2          | 0.64454764   |
|    mean_motor3          | 0.37962255   |
|    mean_motor4          | 0.51047546   |
|    mean_motor5          | 0.57564      |
|    mean_motor6          | 0.56911063   |
|    mean_motor7          | 0.5892657    |
| train/                  |              |
|    approx_kl            | 0.03693291   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0923       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.892        |
|    cost_value_loss      | 5.54e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.55        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.000914     |
|    mean_cost_advantages | 0.0004220092 |
|    mean_reward_advan... | -0.008845019 |
|    n_updates            | 5600         |
|    nu                   | 10.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00187     |
|    reward_explained_... | 0.876        |
|    reward_value_loss    | 0.0342       |
|    std                  | 0.336        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.43e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.2          |
|    forward_reward       | 0.198         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.198         |
|    reward_survive       | 1             |
|    x_position           | 0.507         |
|    x_velocity           | 0.198         |
|    y_position           | 10.2          |
|    y_velocity           | 0.362         |
| rollout/                |               |
|    adjusted_reward      | 7.96          |
|    ep_len_mean          | 477           |
|    ep_rew_mean          | 3.98e+03      |
| time/                   |               |
|    fps                  | 802           |
|    iterations           | 282           |
|    time_elapsed         | 3599          |
|    total_timesteps      | 2887680       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10162         |
|    mean_motor0          | 0.43981618    |
|    mean_motor1          | 0.47080582    |
|    mean_motor2          | 0.64835864    |
|    mean_motor3          | 0.3929977     |
|    mean_motor4          | 0.4926764     |
|    mean_motor5          | 0.60335106    |
|    mean_motor6          | 0.5387058     |
|    mean_motor7          | 0.60395455    |
| train/                  |               |
|    approx_kl            | 0.03555516    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0844        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.831         |
|    cost_value_loss      | 7.52e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.54         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0238        |
|    mean_cost_advantages | 0.0004870713  |
|    mean_reward_advan... | -0.0040754667 |
|    n_updates            | 5620          |
|    nu                   | 10.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0015       |
|    reward_explained_... | 0.916         |
|    reward_value_loss    | 0.0366        |
|    std                  | 0.335         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.86e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.23         |
|    forward_reward       | 0.281        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.44        |
|    reward_forward       | 0.281        |
|    reward_survive       | 1            |
|    x_position           | 0.581        |
|    x_velocity           | 0.281        |
|    y_position           | 6.13         |
|    y_velocity           | 0.545        |
| rollout/                |              |
|    adjusted_reward      | 7.93         |
|    ep_len_mean          | 472          |
|    ep_rew_mean          | 3.93e+03     |
| time/                   |              |
|    fps                  | 802          |
|    iterations           | 283          |
|    time_elapsed         | 3612         |
|    total_timesteps      | 2897920      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10155        |
|    mean_motor0          | 0.48029786   |
|    mean_motor1          | 0.467134     |
|    mean_motor2          | 0.6286848    |
|    mean_motor3          | 0.39493233   |
|    mean_motor4          | 0.5170137    |
|    mean_motor5          | 0.5644297    |
|    mean_motor6          | 0.56315035   |
|    mean_motor7          | 0.5750879    |
| train/                  |              |
|    approx_kl            | 0.042183064  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.824        |
|    cost_value_loss      | 7.45e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.52        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.000828     |
|    mean_cost_advantages | 0.0004505076 |
|    mean_reward_advan... | -0.004981309 |
|    n_updates            | 5640         |
|    nu                   | 10.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00223     |
|    reward_explained_... | 0.933        |
|    reward_value_loss    | 0.0353       |
|    std                  | 0.334        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.7e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.17          |
|    forward_reward       | 0.368         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.368         |
|    reward_survive       | 1             |
|    x_position           | 0.645         |
|    x_velocity           | 0.368         |
|    y_position           | 8.13          |
|    y_velocity           | 0.612         |
| rollout/                |               |
|    adjusted_reward      | 7.9           |
|    ep_len_mean          | 474           |
|    ep_rew_mean          | 3.89e+03      |
| time/                   |               |
|    fps                  | 802           |
|    iterations           | 284           |
|    time_elapsed         | 3625          |
|    total_timesteps      | 2908160       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10137         |
|    mean_motor0          | 0.42633003    |
|    mean_motor1          | 0.4684829     |
|    mean_motor2          | 0.6338085     |
|    mean_motor3          | 0.4060822     |
|    mean_motor4          | 0.531222      |
|    mean_motor5          | 0.5609046     |
|    mean_motor6          | 0.52447134    |
|    mean_motor7          | 0.55525243    |
| train/                  |               |
|    approx_kl            | 0.048759084   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0937        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.918         |
|    cost_value_loss      | 6.55e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.5          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0124        |
|    mean_cost_advantages | 0.00014419068 |
|    mean_reward_advan... | -0.02087957   |
|    n_updates            | 5660          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00172      |
|    reward_explained_... | 0.897         |
|    reward_value_loss    | 0.0434        |
|    std                  | 0.333         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.43e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 10.2         |
|    forward_reward       | 0.339        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.339        |
|    reward_survive       | 1            |
|    x_position           | 0.73         |
|    x_velocity           | 0.339        |
|    y_position           | 10.1         |
|    y_velocity           | 0.805        |
| rollout/                |              |
|    adjusted_reward      | 8.71         |
|    ep_len_mean          | 483          |
|    ep_rew_mean          | 3.98e+03     |
| time/                   |              |
|    fps                  | 801          |
|    iterations           | 285          |
|    time_elapsed         | 3639         |
|    total_timesteps      | 2918400      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 10164        |
|    mean_motor0          | 0.4067356    |
|    mean_motor1          | 0.44983092   |
|    mean_motor2          | 0.63182294   |
|    mean_motor3          | 0.4000166    |
|    mean_motor4          | 0.53790426   |
|    mean_motor5          | 0.58670247   |
|    mean_motor6          | 0.5099866    |
|    mean_motor7          | 0.56108207   |
| train/                  |              |
|    approx_kl            | 0.038828332  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0998       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.693        |
|    cost_value_loss      | 3.05e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.47        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00419      |
|    mean_cost_advantages | 0.0002821015 |
|    mean_reward_advan... | -0.024685567 |
|    n_updates            | 5680         |
|    nu                   | 10.7         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00187     |
|    reward_explained_... | 0.901        |
|    reward_value_loss    | 0.0453       |
|    std                  | 0.332        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.55e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.04          |
|    forward_reward       | 0.357         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.357         |
|    reward_survive       | 1             |
|    x_position           | 0.668         |
|    x_velocity           | 0.357         |
|    y_position           | 7.98          |
|    y_velocity           | 0.92          |
| rollout/                |               |
|    adjusted_reward      | 7.67          |
|    ep_len_mean          | 488           |
|    ep_rew_mean          | 3.94e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 286           |
|    time_elapsed         | 3652          |
|    total_timesteps      | 2928640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10151         |
|    mean_motor0          | 0.42883396    |
|    mean_motor1          | 0.45472065    |
|    mean_motor2          | 0.6254771     |
|    mean_motor3          | 0.39228117    |
|    mean_motor4          | 0.5211119     |
|    mean_motor5          | 0.5700635     |
|    mean_motor6          | 0.50400615    |
|    mean_motor7          | 0.59115815    |
| train/                  |               |
|    approx_kl            | 0.031085078   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.066         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.726         |
|    cost_value_loss      | 1.9e-06       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.45         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0464        |
|    mean_cost_advantages | 0.00022544482 |
|    mean_reward_advan... | -0.0202971    |
|    n_updates            | 5700          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00145      |
|    reward_explained_... | 0.871         |
|    reward_value_loss    | 0.0483        |
|    std                  | 0.332         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.45e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.89          |
|    forward_reward       | 0.405         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.58         |
|    reward_forward       | 0.405         |
|    reward_survive       | 1             |
|    x_position           | 0.405         |
|    x_velocity           | 0.405         |
|    y_position           | 4.84          |
|    y_velocity           | 0.95          |
| rollout/                |               |
|    adjusted_reward      | 8.36          |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 3.99e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 287           |
|    time_elapsed         | 3666          |
|    total_timesteps      | 2938880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10158         |
|    mean_motor0          | 0.41879225    |
|    mean_motor1          | 0.48473057    |
|    mean_motor2          | 0.63364357    |
|    mean_motor3          | 0.39290076    |
|    mean_motor4          | 0.53015155    |
|    mean_motor5          | 0.5914295     |
|    mean_motor6          | 0.47658792    |
|    mean_motor7          | 0.6230093     |
| train/                  |               |
|    approx_kl            | 0.034767937   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0876        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.825         |
|    cost_value_loss      | 2.55e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.44         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0161        |
|    mean_cost_advantages | 0.00021233011 |
|    mean_reward_advan... | -0.028148616  |
|    n_updates            | 5720          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00183      |
|    reward_explained_... | 0.913         |
|    reward_value_loss    | 0.0449        |
|    std                  | 0.331         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.66e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.61          |
|    forward_reward       | 0.313         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.313         |
|    reward_survive       | 1             |
|    x_position           | 0.0659        |
|    x_velocity           | 0.313         |
|    y_position           | 7.59          |
|    y_velocity           | 0.712         |
| rollout/                |               |
|    adjusted_reward      | 8.87          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.17e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 288           |
|    time_elapsed         | 3679          |
|    total_timesteps      | 2949120       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10149         |
|    mean_motor0          | 0.40839273    |
|    mean_motor1          | 0.50779253    |
|    mean_motor2          | 0.64695996    |
|    mean_motor3          | 0.394043      |
|    mean_motor4          | 0.5397762     |
|    mean_motor5          | 0.5924657     |
|    mean_motor6          | 0.46126255    |
|    mean_motor7          | 0.5931088     |
| train/                  |               |
|    approx_kl            | 0.030428672   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0762        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.688         |
|    cost_value_loss      | 1.87e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.41         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0564        |
|    mean_cost_advantages | 0.00013992615 |
|    mean_reward_advan... | -0.014821088  |
|    n_updates            | 5740          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00147      |
|    reward_explained_... | 0.897         |
|    reward_value_loss    | 0.0432        |
|    std                  | 0.33          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.93e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.44          |
|    forward_reward       | 0.359         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.34         |
|    reward_forward       | 0.359         |
|    reward_survive       | 1             |
|    x_position           | 0.638         |
|    x_velocity           | 0.359         |
|    y_position           | 9.37          |
|    y_velocity           | 1.01          |
| rollout/                |               |
|    adjusted_reward      | 9.08          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.28e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 289           |
|    time_elapsed         | 3692          |
|    total_timesteps      | 2959360       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10152         |
|    mean_motor0          | 0.42971054    |
|    mean_motor1          | 0.5006328     |
|    mean_motor2          | 0.64724886    |
|    mean_motor3          | 0.37133393    |
|    mean_motor4          | 0.51802504    |
|    mean_motor5          | 0.6084249     |
|    mean_motor6          | 0.43128172    |
|    mean_motor7          | 0.62269276    |
| train/                  |               |
|    approx_kl            | 0.03832852    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0883        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.772         |
|    cost_value_loss      | 1.16e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.38         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00475       |
|    mean_cost_advantages | 0.00012768937 |
|    mean_reward_advan... | -0.020873353  |
|    n_updates            | 5760          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00155      |
|    reward_explained_... | 0.807         |
|    reward_value_loss    | 0.0454        |
|    std                  | 0.329         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 405           |
|    mean_reward          | 3.13e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.52          |
|    forward_reward       | 0.325         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.38         |
|    reward_forward       | 0.325         |
|    reward_survive       | 1             |
|    x_position           | -0.389        |
|    x_velocity           | 0.325         |
|    y_position           | 9.5           |
|    y_velocity           | 0.858         |
| rollout/                |               |
|    adjusted_reward      | 8.54          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.23e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 290           |
|    time_elapsed         | 3705          |
|    total_timesteps      | 2969600       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10150         |
|    mean_motor0          | 0.43301448    |
|    mean_motor1          | 0.48655254    |
|    mean_motor2          | 0.65178484    |
|    mean_motor3          | 0.3793479     |
|    mean_motor4          | 0.50308913    |
|    mean_motor5          | 0.62249374    |
|    mean_motor6          | 0.441887      |
|    mean_motor7          | 0.60938996    |
| train/                  |               |
|    approx_kl            | 0.03338402    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0873        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.786         |
|    cost_value_loss      | 4.2e-05       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.36         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00167       |
|    mean_cost_advantages | 3.7119957e-05 |
|    mean_reward_advan... | 0.0057516517  |
|    n_updates            | 5780          |
|    nu                   | 10.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0019       |
|    reward_explained_... | 0.899         |
|    reward_value_loss    | 0.0368        |
|    std                  | 0.328         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.55e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.06          |
|    forward_reward       | 0.235         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.22         |
|    reward_forward       | 0.235         |
|    reward_survive       | 1             |
|    x_position           | -0.453        |
|    x_velocity           | 0.235         |
|    y_position           | 9.03          |
|    y_velocity           | 0.446         |
| rollout/                |               |
|    adjusted_reward      | 8.52          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.33e+03      |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 291           |
|    time_elapsed         | 3718          |
|    total_timesteps      | 2979840       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10139         |
|    mean_motor0          | 0.41674381    |
|    mean_motor1          | 0.4632123     |
|    mean_motor2          | 0.6334275     |
|    mean_motor3          | 0.38660052    |
|    mean_motor4          | 0.5145878     |
|    mean_motor5          | 0.6091399     |
|    mean_motor6          | 0.48211747    |
|    mean_motor7          | 0.5864559     |
| train/                  |               |
|    approx_kl            | 0.03527658    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0774        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.64          |
|    cost_value_loss      | 2.1e-06       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.34         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0193        |
|    mean_cost_advantages | 0.00014353315 |
|    mean_reward_advan... | -0.015700711  |
|    n_updates            | 5800          |
|    nu                   | 10.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0018       |
|    reward_explained_... | 0.893         |
|    reward_value_loss    | 0.0436        |
|    std                  | 0.327         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.38e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 11.1         |
|    forward_reward       | 0.253        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.253        |
|    reward_survive       | 1            |
|    x_position           | 0.559        |
|    x_velocity           | 0.253        |
|    y_position           | 11.1         |
|    y_velocity           | 0.467        |
| rollout/                |              |
|    adjusted_reward      | 8.49         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.32e+03     |
| time/                   |              |
|    fps                  | 801          |
|    iterations           | 292          |
|    time_elapsed         | 3732         |
|    total_timesteps      | 2990080      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10134        |
|    mean_motor0          | 0.4194387    |
|    mean_motor1          | 0.46848258   |
|    mean_motor2          | 0.6451106    |
|    mean_motor3          | 0.39202264   |
|    mean_motor4          | 0.52248764   |
|    mean_motor5          | 0.60172766   |
|    mean_motor6          | 0.44914192   |
|    mean_motor7          | 0.56259197   |
| train/                  |              |
|    approx_kl            | 0.03716635   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0789       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.622        |
|    cost_value_loss      | 2.67e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.31        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0276       |
|    mean_cost_advantages | 7.217727e-05 |
|    mean_reward_advan... | -0.01775637  |
|    n_updates            | 5820         |
|    nu                   | 10.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00172     |
|    reward_explained_... | 0.917        |
|    reward_value_loss    | 0.0416       |
|    std                  | 0.326        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.21e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.79          |
|    forward_reward       | 0.178         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.25         |
|    reward_forward       | 0.178         |
|    reward_survive       | 1             |
|    x_position           | 0.217         |
|    x_velocity           | 0.178         |
|    y_position           | 9.73          |
|    y_velocity           | 0.349         |
| rollout/                |               |
|    adjusted_reward      | 8.98          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.3e+03       |
| time/                   |               |
|    fps                  | 801           |
|    iterations           | 293           |
|    time_elapsed         | 3745          |
|    total_timesteps      | 3000320       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10132         |
|    mean_motor0          | 0.4013668     |
|    mean_motor1          | 0.4492558     |
|    mean_motor2          | 0.6560648     |
|    mean_motor3          | 0.3848028     |
|    mean_motor4          | 0.5151026     |
|    mean_motor5          | 0.60133576    |
|    mean_motor6          | 0.4556519     |
|    mean_motor7          | 0.5628721     |
| train/                  |               |
|    approx_kl            | 0.036601704   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0758        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.738         |
|    cost_value_loss      | 2.41e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.28         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0189        |
|    mean_cost_advantages | 0.00012262492 |
|    mean_reward_advan... | -0.020175496  |
|    n_updates            | 5840          |
|    nu                   | 10.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00166      |
|    reward_explained_... | 0.874         |
|    reward_value_loss    | 0.0471        |
|    std                  | 0.325         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.45e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.22          |
|    forward_reward       | 0.298         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.25         |
|    reward_forward       | 0.298         |
|    reward_survive       | 1             |
|    x_position           | 0.136         |
|    x_velocity           | 0.298         |
|    y_position           | 7.18          |
|    y_velocity           | 0.697         |
| rollout/                |               |
|    adjusted_reward      | 9             |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.3e+03       |
| time/                   |               |
|    fps                  | 800           |
|    iterations           | 294           |
|    time_elapsed         | 3758          |
|    total_timesteps      | 3010560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10121         |
|    mean_motor0          | 0.39915913    |
|    mean_motor1          | 0.46336025    |
|    mean_motor2          | 0.6772835     |
|    mean_motor3          | 0.38621545    |
|    mean_motor4          | 0.5095967     |
|    mean_motor5          | 0.60134405    |
|    mean_motor6          | 0.43932256    |
|    mean_motor7          | 0.55336004    |
| train/                  |               |
|    approx_kl            | 0.037617844   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0787        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.737         |
|    cost_value_loss      | 1.16e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.26         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0144        |
|    mean_cost_advantages | 0.00011081448 |
|    mean_reward_advan... | -0.014282541  |
|    n_updates            | 5860          |
|    nu                   | 10.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00164      |
|    reward_explained_... | 0.862         |
|    reward_value_loss    | 0.043         |
|    std                  | 0.324         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.98e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.17         |
|    forward_reward       | 0.196        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.196        |
|    reward_survive       | 1            |
|    x_position           | -0.0887      |
|    x_velocity           | 0.196        |
|    y_position           | 7.17         |
|    y_velocity           | 0.602        |
| rollout/                |              |
|    adjusted_reward      | 8.8          |
|    ep_len_mean          | 499          |
|    ep_rew_mean          | 4.39e+03     |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 295          |
|    time_elapsed         | 3772         |
|    total_timesteps      | 3020800      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10120        |
|    mean_motor0          | 0.41968203   |
|    mean_motor1          | 0.46653205   |
|    mean_motor2          | 0.67769563   |
|    mean_motor3          | 0.36996925   |
|    mean_motor4          | 0.48032132   |
|    mean_motor5          | 0.59407896   |
|    mean_motor6          | 0.42582583   |
|    mean_motor7          | 0.569837     |
| train/                  |              |
|    approx_kl            | 0.035435252  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0783       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.447        |
|    cost_value_loss      | 9.28e-07     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.24        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.022        |
|    mean_cost_advantages | 6.763561e-05 |
|    mean_reward_advan... | -0.013619898 |
|    n_updates            | 5880         |
|    nu                   | 10.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00165     |
|    reward_explained_... | 0.819        |
|    reward_value_loss    | 0.0378       |
|    std                  | 0.323        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.27e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 8.57           |
|    forward_reward       | 0.215          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.32          |
|    reward_forward       | 0.215          |
|    reward_survive       | 1              |
|    x_position           | -0.719         |
|    x_velocity           | 0.215          |
|    y_position           | 8.53           |
|    y_velocity           | 0.696          |
| rollout/                |                |
|    adjusted_reward      | 8.8            |
|    ep_len_mean          | 499            |
|    ep_rew_mean          | 4.39e+03       |
| time/                   |                |
|    fps                  | 800            |
|    iterations           | 296            |
|    time_elapsed         | 3785           |
|    total_timesteps      | 3031040        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10137          |
|    mean_motor0          | 0.41552004     |
|    mean_motor1          | 0.47011018     |
|    mean_motor2          | 0.69911236     |
|    mean_motor3          | 0.38224927     |
|    mean_motor4          | 0.49827975     |
|    mean_motor5          | 0.6073765      |
|    mean_motor6          | 0.4581408      |
|    mean_motor7          | 0.5700264      |
| train/                  |                |
|    approx_kl            | 0.027909392    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.074          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.832          |
|    cost_value_loss      | 1.19e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.24          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0102         |
|    mean_cost_advantages | -4.4901997e-05 |
|    mean_reward_advan... | -0.007834302   |
|    n_updates            | 5900           |
|    nu                   | 10.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00165       |
|    reward_explained_... | 0.907          |
|    reward_value_loss    | 0.0406         |
|    std                  | 0.324          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.34e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.32         |
|    forward_reward       | 0.167        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.17        |
|    reward_forward       | 0.167        |
|    reward_survive       | 1            |
|    x_position           | -0.794       |
|    x_velocity           | 0.167        |
|    y_position           | 9.26         |
|    y_velocity           | 0.312        |
| rollout/                |              |
|    adjusted_reward      | 8.49         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 4.36e+03     |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 297          |
|    time_elapsed         | 3798         |
|    total_timesteps      | 3041280      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10132        |
|    mean_motor0          | 0.40920815   |
|    mean_motor1          | 0.453166     |
|    mean_motor2          | 0.68654454   |
|    mean_motor3          | 0.37775496   |
|    mean_motor4          | 0.5124365    |
|    mean_motor5          | 0.5778686    |
|    mean_motor6          | 0.47930393   |
|    mean_motor7          | 0.56124395   |
| train/                  |              |
|    approx_kl            | 0.038274538  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0907       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.483        |
|    cost_value_loss      | 1.42e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.23        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0113       |
|    mean_cost_advantages | 0.0001405415 |
|    mean_reward_advan... | -0.014410475 |
|    n_updates            | 5920         |
|    nu                   | 10.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00185     |
|    reward_explained_... | 0.841        |
|    reward_value_loss    | 0.0427       |
|    std                  | 0.323        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.52e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 11.1         |
|    forward_reward       | 0.281        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.21        |
|    reward_forward       | 0.281        |
|    reward_survive       | 1            |
|    x_position           | -0.86        |
|    x_velocity           | 0.281        |
|    y_position           | 11           |
|    y_velocity           | 0.35         |
| rollout/                |              |
|    adjusted_reward      | 8.32         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.3e+03      |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 298          |
|    time_elapsed         | 3812         |
|    total_timesteps      | 3051520      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10120        |
|    mean_motor0          | 0.42109045   |
|    mean_motor1          | 0.43766546   |
|    mean_motor2          | 0.7067603    |
|    mean_motor3          | 0.37526628   |
|    mean_motor4          | 0.48063332   |
|    mean_motor5          | 0.5743295    |
|    mean_motor6          | 0.46497473   |
|    mean_motor7          | 0.56275815   |
| train/                  |              |
|    approx_kl            | 0.042528003  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0898       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.582        |
|    cost_value_loss      | 1.81e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.21        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00473      |
|    mean_cost_advantages | 0.0001267202 |
|    mean_reward_advan... | -0.03130719  |
|    n_updates            | 5940         |
|    nu                   | 10.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00165     |
|    reward_explained_... | 0.787        |
|    reward_value_loss    | 0.0492       |
|    std                  | 0.322        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 402          |
|    mean_reward          | 3.64e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.22         |
|    forward_reward       | 0.324        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.29        |
|    reward_forward       | 0.324        |
|    reward_survive       | 1            |
|    x_position           | -0.344       |
|    x_velocity           | 0.324        |
|    y_position           | 7.15         |
|    y_velocity           | 0.918        |
| rollout/                |              |
|    adjusted_reward      | 8.36         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.24e+03     |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 299          |
|    time_elapsed         | 3824         |
|    total_timesteps      | 3061760      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10115        |
|    mean_motor0          | 0.4034981    |
|    mean_motor1          | 0.44396234   |
|    mean_motor2          | 0.73044837   |
|    mean_motor3          | 0.39273757   |
|    mean_motor4          | 0.4620418    |
|    mean_motor5          | 0.58944297   |
|    mean_motor6          | 0.45373374   |
|    mean_motor7          | 0.5870441    |
| train/                  |              |
|    approx_kl            | 0.049642276  |
|    average_cost         | 0.0037109375 |
|    clip_fraction        | 0.142        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -3.8         |
|    cost_value_loss      | 0.00358      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.21        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0243       |
|    mean_cost_advantages | 0.0053593377 |
|    mean_reward_advan... | -0.017921206 |
|    n_updates            | 5960         |
|    nu                   | 10.8         |
|    nu_loss              | -0.0401      |
|    policy_gradient_loss | -0.00273     |
|    reward_explained_... | 0.894        |
|    reward_value_loss    | 0.044        |
|    std                  | 0.322        |
|    total_cost           | 38.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 413           |
|    mean_reward          | 2.52e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.16          |
|    forward_reward       | 0.31          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.27         |
|    reward_forward       | 0.31          |
|    reward_survive       | 1             |
|    x_position           | 0.17          |
|    x_velocity           | 0.31          |
|    y_position           | 7.16          |
|    y_velocity           | 0.87          |
| rollout/                |               |
|    adjusted_reward      | 8.41          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.21e+03      |
| time/                   |               |
|    fps                  | 800           |
|    iterations           | 300           |
|    time_elapsed         | 3838          |
|    total_timesteps      | 3072000       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10125         |
|    mean_motor0          | 0.40365314    |
|    mean_motor1          | 0.44257155    |
|    mean_motor2          | 0.73982495    |
|    mean_motor3          | 0.39817515    |
|    mean_motor4          | 0.45692435    |
|    mean_motor5          | 0.56322664    |
|    mean_motor6          | 0.46496305    |
|    mean_motor7          | 0.5808346     |
| train/                  |               |
|    approx_kl            | 0.04163312    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.102         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.63          |
|    cost_value_loss      | 4.41e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.2          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0021        |
|    mean_cost_advantages | 0.00036790041 |
|    mean_reward_advan... | -0.033161134  |
|    n_updates            | 5980          |
|    nu                   | 10.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00181      |
|    reward_explained_... | 0.81          |
|    reward_value_loss    | 0.0508        |
|    std                  | 0.322         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 437           |
|    mean_reward          | 2.95e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.33          |
|    forward_reward       | 0.187         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.16         |
|    reward_forward       | 0.187         |
|    reward_survive       | 1             |
|    x_position           | -0.814        |
|    x_velocity           | 0.187         |
|    y_position           | 8.26          |
|    y_velocity           | 0.535         |
| rollout/                |               |
|    adjusted_reward      | 8.34          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.21e+03      |
| time/                   |               |
|    fps                  | 800           |
|    iterations           | 301           |
|    time_elapsed         | 3851          |
|    total_timesteps      | 3082240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10111         |
|    mean_motor0          | 0.41472912    |
|    mean_motor1          | 0.4393355     |
|    mean_motor2          | 0.7246735     |
|    mean_motor3          | 0.39441654    |
|    mean_motor4          | 0.47596002    |
|    mean_motor5          | 0.57073057    |
|    mean_motor6          | 0.45961088    |
|    mean_motor7          | 0.56860864    |
| train/                  |               |
|    approx_kl            | 0.03907196    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0959        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.694         |
|    cost_value_loss      | 3.97e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.2          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00825       |
|    mean_cost_advantages | 0.00037978304 |
|    mean_reward_advan... | -0.027753303  |
|    n_updates            | 6000          |
|    nu                   | 10.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00182      |
|    reward_explained_... | 0.777         |
|    reward_value_loss    | 0.0479        |
|    std                  | 0.322         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.49e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.88         |
|    forward_reward       | 0.287        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.17        |
|    reward_forward       | 0.287        |
|    reward_survive       | 1            |
|    x_position           | -0.277       |
|    x_velocity           | 0.287        |
|    y_position           | 9.86         |
|    y_velocity           | 0.58         |
| rollout/                |              |
|    adjusted_reward      | 8.05         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 4.15e+03     |
| time/                   |              |
|    fps                  | 800          |
|    iterations           | 302          |
|    time_elapsed         | 3864         |
|    total_timesteps      | 3092480      |
| torque/                 |              |
|    greater_than_0.25    | 10236        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 10110        |
|    mean_motor0          | 0.43444976   |
|    mean_motor1          | 0.46369085   |
|    mean_motor2          | 0.727338     |
|    mean_motor3          | 0.38627028   |
|    mean_motor4          | 0.48749033   |
|    mean_motor5          | 0.56525165   |
|    mean_motor6          | 0.44636083   |
|    mean_motor7          | 0.5898619    |
| train/                  |              |
|    approx_kl            | 0.52866095   |
|    average_cost         | 0.028027344  |
|    clip_fraction        | 0.211        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -0.164       |
|    cost_value_loss      | 0.00408      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.2         |
|    learning_rate        | 3e-05        |
|    loss                 | -0.032       |
|    mean_cost_advantages | 0.040800594  |
|    mean_reward_advan... | -0.026260236 |
|    n_updates            | 6020         |
|    nu                   | 10.8         |
|    nu_loss              | -0.303       |
|    policy_gradient_loss | -0.0279      |
|    reward_explained_... | 0.823        |
|    reward_value_loss    | 0.0432       |
|    std                  | 0.322        |
|    total_cost           | 287.0        |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.45e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 11.8           |
|    forward_reward       | 0.276          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.26          |
|    reward_forward       | 0.276          |
|    reward_survive       | 1              |
|    x_position           | -0.689         |
|    x_velocity           | 0.276          |
|    y_position           | 11.8           |
|    y_velocity           | 0.397          |
| rollout/                |                |
|    adjusted_reward      | 8.24           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 4.09e+03       |
| time/                   |                |
|    fps                  | 800            |
|    iterations           | 303            |
|    time_elapsed         | 3877           |
|    total_timesteps      | 3102720        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10112          |
|    mean_motor0          | 0.43343148     |
|    mean_motor1          | 0.43659145     |
|    mean_motor2          | 0.7155511      |
|    mean_motor3          | 0.3670792      |
|    mean_motor4          | 0.49494156     |
|    mean_motor5          | 0.58754605     |
|    mean_motor6          | 0.49768814     |
|    mean_motor7          | 0.56925285     |
| train/                  |                |
|    approx_kl            | 0.036511056    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.104          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.734          |
|    cost_value_loss      | 0.000115       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -2.19          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0345         |
|    mean_cost_advantages | -0.00018404727 |
|    mean_reward_advan... | -0.027489752   |
|    n_updates            | 6040           |
|    nu                   | 10.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00154       |
|    reward_explained_... | 0.871          |
|    reward_value_loss    | 0.0441         |
|    std                  | 0.322          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.56e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.82         |
|    forward_reward       | 0.241        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.241        |
|    reward_survive       | 1            |
|    x_position           | -0.542       |
|    x_velocity           | 0.241        |
|    y_position           | 9.79         |
|    y_velocity           | 0.725        |
| rollout/                |              |
|    adjusted_reward      | 8.33         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.09e+03     |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 304          |
|    time_elapsed         | 3891         |
|    total_timesteps      | 3112960      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 10129        |
|    mean_motor0          | 0.40801802   |
|    mean_motor1          | 0.45948896   |
|    mean_motor2          | 0.71831113   |
|    mean_motor3          | 0.3723746    |
|    mean_motor4          | 0.51048803   |
|    mean_motor5          | 0.58495605   |
|    mean_motor6          | 0.4556218    |
|    mean_motor7          | 0.5679617    |
| train/                  |              |
|    approx_kl            | 0.04400106   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.097        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.638        |
|    cost_value_loss      | 9.62e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.19        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0356       |
|    mean_cost_advantages | 0.0002514047 |
|    mean_reward_advan... | -0.020882491 |
|    n_updates            | 6060         |
|    nu                   | 10.9         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00169     |
|    reward_explained_... | 0.879        |
|    reward_value_loss    | 0.0386       |
|    std                  | 0.322        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.27e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 10.8         |
|    forward_reward       | 0.224        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.21        |
|    reward_forward       | 0.224        |
|    reward_survive       | 1            |
|    x_position           | -0.85        |
|    x_velocity           | 0.224        |
|    y_position           | 10.8         |
|    y_velocity           | 0.343        |
| rollout/                |              |
|    adjusted_reward      | 8.52         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.1e+03      |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 305          |
|    time_elapsed         | 3904         |
|    total_timesteps      | 3123200      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10118        |
|    mean_motor0          | 0.41131014   |
|    mean_motor1          | 0.43627733   |
|    mean_motor2          | 0.7332323    |
|    mean_motor3          | 0.3806503    |
|    mean_motor4          | 0.48877692   |
|    mean_motor5          | 0.59449583   |
|    mean_motor6          | 0.44999298   |
|    mean_motor7          | 0.56022245   |
| train/                  |              |
|    approx_kl            | 0.03320786   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0771       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.76         |
|    cost_value_loss      | 3.44e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.2         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0349       |
|    mean_cost_advantages | 0.0001797148 |
|    mean_reward_advan... | -0.019175233 |
|    n_updates            | 6080         |
|    nu                   | 10.9         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00146     |
|    reward_explained_... | 0.874        |
|    reward_value_loss    | 0.0395       |
|    std                  | 0.322        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.56e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.22         |
|    forward_reward       | 0.414        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.33        |
|    reward_forward       | 0.414        |
|    reward_survive       | 1            |
|    x_position           | -0.619       |
|    x_velocity           | 0.414        |
|    y_position           | 9.14         |
|    y_velocity           | 0.903        |
| rollout/                |              |
|    adjusted_reward      | 8.15         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.11e+03     |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 306          |
|    time_elapsed         | 3917         |
|    total_timesteps      | 3133440      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10138        |
|    mean_motor0          | 0.40993577   |
|    mean_motor1          | 0.4553306    |
|    mean_motor2          | 0.7332563    |
|    mean_motor3          | 0.38472787   |
|    mean_motor4          | 0.5128561    |
|    mean_motor5          | 0.56948507   |
|    mean_motor6          | 0.4393297    |
|    mean_motor7          | 0.5649704    |
| train/                  |              |
|    approx_kl            | 0.03893256   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0866       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.786        |
|    cost_value_loss      | 2.05e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.2         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00366      |
|    mean_cost_advantages | 0.0001942978 |
|    mean_reward_advan... | -0.014158863 |
|    n_updates            | 6100         |
|    nu                   | 10.9         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00171     |
|    reward_explained_... | 0.877        |
|    reward_value_loss    | 0.041        |
|    std                  | 0.322        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.43e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.3           |
|    forward_reward       | 0.426         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.42         |
|    reward_forward       | 0.426         |
|    reward_survive       | 1             |
|    x_position           | -0.464        |
|    x_velocity           | 0.426         |
|    y_position           | 8.27          |
|    y_velocity           | 0.883         |
| rollout/                |               |
|    adjusted_reward      | 7.47          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.02e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 307           |
|    time_elapsed         | 3931          |
|    total_timesteps      | 3143680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10130         |
|    mean_motor0          | 0.47341537    |
|    mean_motor1          | 0.47309643    |
|    mean_motor2          | 0.69304055    |
|    mean_motor3          | 0.38815844    |
|    mean_motor4          | 0.47855157    |
|    mean_motor5          | 0.5680938     |
|    mean_motor6          | 0.41137663    |
|    mean_motor7          | 0.5915815     |
| train/                  |               |
|    approx_kl            | 0.031501632   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0738        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.332         |
|    cost_value_loss      | 1.04e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.18         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.000216     |
|    mean_cost_advantages | 0.00013489931 |
|    mean_reward_advan... | -0.020636953  |
|    n_updates            | 6120          |
|    nu                   | 10.9          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00163      |
|    reward_explained_... | 0.853         |
|    reward_value_loss    | 0.0408        |
|    std                  | 0.322         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.38e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.98          |
|    forward_reward       | 0.225         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.36         |
|    reward_forward       | 0.225         |
|    reward_survive       | 1             |
|    x_position           | -0.995        |
|    x_velocity           | 0.225         |
|    y_position           | 8.88          |
|    y_velocity           | 0.449         |
| rollout/                |               |
|    adjusted_reward      | 8.48          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.09e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 308           |
|    time_elapsed         | 3944          |
|    total_timesteps      | 3153920       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10128         |
|    mean_motor0          | 0.4328733     |
|    mean_motor1          | 0.44538575    |
|    mean_motor2          | 0.72443855    |
|    mean_motor3          | 0.3880113     |
|    mean_motor4          | 0.51075035    |
|    mean_motor5          | 0.55205524    |
|    mean_motor6          | 0.43036595    |
|    mean_motor7          | 0.5604437     |
| train/                  |               |
|    approx_kl            | 0.038224764   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0928        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.626         |
|    cost_value_loss      | 2.76e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.16         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0178        |
|    mean_cost_advantages | 0.00017797905 |
|    mean_reward_advan... | -0.017001841  |
|    n_updates            | 6140          |
|    nu                   | 10.9          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00194      |
|    reward_explained_... | 0.912         |
|    reward_value_loss    | 0.0372        |
|    std                  | 0.321         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.44e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.49         |
|    forward_reward       | 0.336        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.36        |
|    reward_forward       | 0.336        |
|    reward_survive       | 1            |
|    x_position           | -0.429       |
|    x_velocity           | 0.336        |
|    y_position           | 6.38         |
|    y_velocity           | 0.821        |
| rollout/                |              |
|    adjusted_reward      | 8.1          |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 4.06e+03     |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 309          |
|    time_elapsed         | 3957         |
|    total_timesteps      | 3164160      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10133        |
|    mean_motor0          | 0.52918357   |
|    mean_motor1          | 0.42662063   |
|    mean_motor2          | 0.72231835   |
|    mean_motor3          | 0.3854913    |
|    mean_motor4          | 0.49271616   |
|    mean_motor5          | 0.59650624   |
|    mean_motor6          | 0.46141768   |
|    mean_motor7          | 0.5736323    |
| train/                  |              |
|    approx_kl            | 0.031368338  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0975       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.749        |
|    cost_value_loss      | 8.72e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.14        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0256       |
|    mean_cost_advantages | 0.0001316433 |
|    mean_reward_advan... | -0.017846998 |
|    n_updates            | 6160         |
|    nu                   | 11           |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00176     |
|    reward_explained_... | 0.818        |
|    reward_value_loss    | 0.0417       |
|    std                  | 0.32         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.96e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.09          |
|    forward_reward       | 0.375         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.36         |
|    reward_forward       | 0.375         |
|    reward_survive       | 1             |
|    x_position           | -0.49         |
|    x_velocity           | 0.375         |
|    y_position           | 7.04          |
|    y_velocity           | 0.863         |
| rollout/                |               |
|    adjusted_reward      | 7.62          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 3.98e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 310           |
|    time_elapsed         | 3971          |
|    total_timesteps      | 3174400       |
| torque/                 |               |
|    greater_than_0.25    | 10237         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10105         |
|    mean_motor0          | 0.43006477    |
|    mean_motor1          | 0.4320913     |
|    mean_motor2          | 0.7140354     |
|    mean_motor3          | 0.38382316    |
|    mean_motor4          | 0.46784106    |
|    mean_motor5          | 0.58789504    |
|    mean_motor6          | 0.428192      |
|    mean_motor7          | 0.5918939     |
| train/                  |               |
|    approx_kl            | 0.042077124   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.109         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.81          |
|    cost_value_loss      | 9.47e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.12         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.0014       |
|    mean_cost_advantages | 0.00013350174 |
|    mean_reward_advan... | -0.029326856  |
|    n_updates            | 6180          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00195      |
|    reward_explained_... | 0.8           |
|    reward_value_loss    | 0.0473        |
|    std                  | 0.319         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.13e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.36          |
|    forward_reward       | 0.249         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.26         |
|    reward_forward       | 0.249         |
|    reward_survive       | 1             |
|    x_position           | 0.17          |
|    x_velocity           | 0.249         |
|    y_position           | 7.25          |
|    y_velocity           | 0.902         |
| rollout/                |               |
|    adjusted_reward      | 8.52          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3.94e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 311           |
|    time_elapsed         | 3984          |
|    total_timesteps      | 3184640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10108         |
|    mean_motor0          | 0.45347515    |
|    mean_motor1          | 0.41331035    |
|    mean_motor2          | 0.73551464    |
|    mean_motor3          | 0.38646916    |
|    mean_motor4          | 0.46085268    |
|    mean_motor5          | 0.6106977     |
|    mean_motor6          | 0.42693466    |
|    mean_motor7          | 0.56473345    |
| train/                  |               |
|    approx_kl            | 0.037583083   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0898        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.688         |
|    cost_value_loss      | 3.05e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.09         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00612       |
|    mean_cost_advantages | 0.00010192369 |
|    mean_reward_advan... | -0.019473977  |
|    n_updates            | 6200          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00203      |
|    reward_explained_... | 0.902         |
|    reward_value_loss    | 0.0386        |
|    std                  | 0.318         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 407          |
|    mean_reward          | 3.38e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 9.02         |
|    forward_reward       | 0.277        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.27        |
|    reward_forward       | 0.277        |
|    reward_survive       | 1            |
|    x_position           | -1.06        |
|    x_velocity           | 0.277        |
|    y_position           | 8.92         |
|    y_velocity           | 0.635        |
| rollout/                |              |
|    adjusted_reward      | 8.4          |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 4.03e+03     |
| time/                   |              |
|    fps                  | 799          |
|    iterations           | 312          |
|    time_elapsed         | 3997         |
|    total_timesteps      | 3194880      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10106        |
|    mean_motor0          | 0.40662628   |
|    mean_motor1          | 0.42252097   |
|    mean_motor2          | 0.7479002    |
|    mean_motor3          | 0.3843388    |
|    mean_motor4          | 0.46474686   |
|    mean_motor5          | 0.58519036   |
|    mean_motor6          | 0.4244658    |
|    mean_motor7          | 0.5779462    |
| train/                  |              |
|    approx_kl            | 0.033709656  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0669       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.637        |
|    cost_value_loss      | 4.41e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.07        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0242       |
|    mean_cost_advantages | 6.304802e-05 |
|    mean_reward_advan... | -0.01983947  |
|    n_updates            | 6220         |
|    nu                   | 11           |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00169     |
|    reward_explained_... | 0.651        |
|    reward_value_loss    | 0.0463       |
|    std                  | 0.317        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.84e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.3          |
|    forward_reward       | 0.303         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.23         |
|    reward_forward       | 0.303         |
|    reward_survive       | 1             |
|    x_position           | -0.733        |
|    x_velocity           | 0.303         |
|    y_position           | 10.3          |
|    y_velocity           | 0.54          |
| rollout/                |               |
|    adjusted_reward      | 8.23          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.98e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 313           |
|    time_elapsed         | 4010          |
|    total_timesteps      | 3205120       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10113         |
|    mean_motor0          | 0.41854787    |
|    mean_motor1          | 0.4214635     |
|    mean_motor2          | 0.724897      |
|    mean_motor3          | 0.39127147    |
|    mean_motor4          | 0.4722727     |
|    mean_motor5          | 0.588349      |
|    mean_motor6          | 0.43318862    |
|    mean_motor7          | 0.57677513    |
| train/                  |               |
|    approx_kl            | 0.033419583   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.07          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.589         |
|    cost_value_loss      | 8.28e-07      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.05         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00736       |
|    mean_cost_advantages | 0.00016774332 |
|    mean_reward_advan... | -0.021408241  |
|    n_updates            | 6240          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00148      |
|    reward_explained_... | 0.635         |
|    reward_value_loss    | 0.0438        |
|    std                  | 0.316         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.45e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.03          |
|    forward_reward       | 0.219         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.24         |
|    reward_forward       | 0.219         |
|    reward_survive       | 1             |
|    x_position           | -0.694        |
|    x_velocity           | 0.219         |
|    y_position           | 8.96          |
|    y_velocity           | 0.474         |
| rollout/                |               |
|    adjusted_reward      | 8.85          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 4.04e+03      |
| time/                   |               |
|    fps                  | 799           |
|    iterations           | 314           |
|    time_elapsed         | 4024          |
|    total_timesteps      | 3215360       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10106         |
|    mean_motor0          | 0.4266891     |
|    mean_motor1          | 0.42485276    |
|    mean_motor2          | 0.694386      |
|    mean_motor3          | 0.39437547    |
|    mean_motor4          | 0.49278742    |
|    mean_motor5          | 0.5774204     |
|    mean_motor6          | 0.42930022    |
|    mean_motor7          | 0.56871784    |
| train/                  |               |
|    approx_kl            | 0.0450825     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.112         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.717         |
|    cost_value_loss      | 1.72e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.03         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00707       |
|    mean_cost_advantages | 2.4092285e-05 |
|    mean_reward_advan... | -0.021120623  |
|    n_updates            | 6260          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00195      |
|    reward_explained_... | 0.82          |
|    reward_value_loss    | 0.0392        |
|    std                  | 0.316         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 427           |
|    mean_reward          | 2.97e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.16          |
|    forward_reward       | 0.289         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.289         |
|    reward_survive       | 1             |
|    x_position           | -0.632        |
|    x_velocity           | 0.289         |
|    y_position           | 7.06          |
|    y_velocity           | 0.724         |
| rollout/                |               |
|    adjusted_reward      | 8.91          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 4.24e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 315           |
|    time_elapsed         | 4037          |
|    total_timesteps      | 3225600       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10107         |
|    mean_motor0          | 0.44226742    |
|    mean_motor1          | 0.43354407    |
|    mean_motor2          | 0.69896126    |
|    mean_motor3          | 0.40500993    |
|    mean_motor4          | 0.48110548    |
|    mean_motor5          | 0.58239174    |
|    mean_motor6          | 0.42622775    |
|    mean_motor7          | 0.5527128     |
| train/                  |               |
|    approx_kl            | 0.06388539    |
|    average_cost         | 0.0044921874  |
|    clip_fraction        | 0.143         |
|    clip_range           | 0.4           |
|    cost_explained_va... | -324          |
|    cost_value_loss      | 0.00161       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.01         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0189        |
|    mean_cost_advantages | 0.0071437783  |
|    mean_reward_advan... | -0.0034226333 |
|    n_updates            | 6280          |
|    nu                   | 11            |
|    nu_loss              | -0.0494       |
|    policy_gradient_loss | -0.00246      |
|    reward_explained_... | 0.751         |
|    reward_value_loss    | 0.0395        |
|    std                  | 0.315         |
|    total_cost           | 46.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.36e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 8.25         |
|    forward_reward       | 0.251        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.26        |
|    reward_forward       | 0.251        |
|    reward_survive       | 1            |
|    x_position           | -1.17        |
|    x_velocity           | 0.251        |
|    y_position           | 8.15         |
|    y_velocity           | 0.466        |
| rollout/                |              |
|    adjusted_reward      | 8.5          |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 4.25e+03     |
| time/                   |              |
|    fps                  | 798          |
|    iterations           | 316          |
|    time_elapsed         | 4050         |
|    total_timesteps      | 3235840      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10122        |
|    mean_motor0          | 0.4148416    |
|    mean_motor1          | 0.42386857   |
|    mean_motor2          | 0.71913826   |
|    mean_motor3          | 0.41303092   |
|    mean_motor4          | 0.4790922    |
|    mean_motor5          | 0.57949317   |
|    mean_motor6          | 0.43132743   |
|    mean_motor7          | 0.5647986    |
| train/                  |              |
|    approx_kl            | 0.054589696  |
|    average_cost         | 0.010058594  |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -2.59        |
|    cost_value_loss      | 0.00616      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -2.02        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0231       |
|    mean_cost_advantages | 0.015477118  |
|    mean_reward_advan... | 0.0028602604 |
|    n_updates            | 6300         |
|    nu                   | 11           |
|    nu_loss              | -0.111       |
|    policy_gradient_loss | -0.00236     |
|    reward_explained_... | 0.805        |
|    reward_value_loss    | 0.0357       |
|    std                  | 0.316        |
|    total_cost           | 103.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.39e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.69          |
|    forward_reward       | 0.171         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.2          |
|    reward_forward       | 0.171         |
|    reward_survive       | 1             |
|    x_position           | -1.06         |
|    x_velocity           | 0.171         |
|    y_position           | 7.58          |
|    y_velocity           | 0.395         |
| rollout/                |               |
|    adjusted_reward      | 8.51          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 4.27e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 317           |
|    time_elapsed         | 4063          |
|    total_timesteps      | 3246080       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10234         |
|    greater_than_0.5     | 10107         |
|    mean_motor0          | 0.43109903    |
|    mean_motor1          | 0.42554244    |
|    mean_motor2          | 0.70055026    |
|    mean_motor3          | 0.40001383    |
|    mean_motor4          | 0.4861011     |
|    mean_motor5          | 0.56456435    |
|    mean_motor6          | 0.4091987     |
|    mean_motor7          | 0.5682642     |
| train/                  |               |
|    approx_kl            | 0.03589269    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0896        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.755         |
|    cost_value_loss      | 4.18e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2.02         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00737       |
|    mean_cost_advantages | 0.00021926108 |
|    mean_reward_advan... | -0.018508965  |
|    n_updates            | 6320          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00174      |
|    reward_explained_... | 0.733         |
|    reward_value_loss    | 0.042         |
|    std                  | 0.315         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 368           |
|    mean_reward          | 2.89e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 9.2           |
|    forward_reward       | 0.178         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.24         |
|    reward_forward       | 0.178         |
|    reward_survive       | 1             |
|    x_position           | -0.894        |
|    x_velocity           | 0.178         |
|    y_position           | 9.07          |
|    y_velocity           | 0.333         |
| rollout/                |               |
|    adjusted_reward      | 8.21          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 4.31e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 318           |
|    time_elapsed         | 4076          |
|    total_timesteps      | 3256320       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10128         |
|    mean_motor0          | 0.4496224     |
|    mean_motor1          | 0.4226714     |
|    mean_motor2          | 0.68715537    |
|    mean_motor3          | 0.39854077    |
|    mean_motor4          | 0.47332144    |
|    mean_motor5          | 0.562851      |
|    mean_motor6          | 0.47281522    |
|    mean_motor7          | 0.57452786    |
| train/                  |               |
|    approx_kl            | 0.036780573   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.101         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.811         |
|    cost_value_loss      | 4.8e-05       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -2            |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0228        |
|    mean_cost_advantages | 0.00018301103 |
|    mean_reward_advan... | -0.014016055  |
|    n_updates            | 6340          |
|    nu                   | 11            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00172      |
|    reward_explained_... | 0.799         |
|    reward_value_loss    | 0.0408        |
|    std                  | 0.314         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.37e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 10.2          |
|    forward_reward       | 0.158         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.13         |
|    reward_forward       | 0.158         |
|    reward_survive       | 1             |
|    x_position           | -1.22         |
|    x_velocity           | 0.158         |
|    y_position           | 10.1          |
|    y_velocity           | 0.487         |
| rollout/                |               |
|    adjusted_reward      | 7.82          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 4.15e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 319           |
|    time_elapsed         | 4089          |
|    total_timesteps      | 3266560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10118         |
|    mean_motor0          | 0.43548256    |
|    mean_motor1          | 0.44010526    |
|    mean_motor2          | 0.65676427    |
|    mean_motor3          | 0.39745885    |
|    mean_motor4          | 0.48032314    |
|    mean_motor5          | 0.5598694     |
|    mean_motor6          | 0.43902332    |
|    mean_motor7          | 0.57443005    |
| train/                  |               |
|    approx_kl            | 0.039804023   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0897        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.912         |
|    cost_value_loss      | 0.000245      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.97         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000462      |
|    mean_cost_advantages | -0.0013616056 |
|    mean_reward_advan... | -0.008938154  |
|    n_updates            | 6360          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00165      |
|    reward_explained_... | 0.886         |
|    reward_value_loss    | 0.0365        |
|    std                  | 0.313         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.27e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.36          |
|    forward_reward       | 0.155         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.155         |
|    reward_survive       | 1             |
|    x_position           | -1.19         |
|    x_velocity           | 0.155         |
|    y_position           | 7.23          |
|    y_velocity           | 0.552         |
| rollout/                |               |
|    adjusted_reward      | 7.8           |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 4.03e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 320           |
|    time_elapsed         | 4103          |
|    total_timesteps      | 3276800       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10107         |
|    mean_motor0          | 0.41432732    |
|    mean_motor1          | 0.4565081     |
|    mean_motor2          | 0.6509696     |
|    mean_motor3          | 0.4038723     |
|    mean_motor4          | 0.46007305    |
|    mean_motor5          | 0.5526197     |
|    mean_motor6          | 0.45804754    |
|    mean_motor7          | 0.5842893     |
| train/                  |               |
|    approx_kl            | 0.03326129    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0687        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.805         |
|    cost_value_loss      | 1.52e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.95         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00247       |
|    mean_cost_advantages | 0.00013523258 |
|    mean_reward_advan... | -0.014885264  |
|    n_updates            | 6380          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00143      |
|    reward_explained_... | 0.904         |
|    reward_value_loss    | 0.0349        |
|    std                  | 0.313         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 439           |
|    mean_reward          | 3.75e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.3           |
|    forward_reward       | 0.302         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.28         |
|    reward_forward       | 0.302         |
|    reward_survive       | 1             |
|    x_position           | -0.989        |
|    x_velocity           | 0.302         |
|    y_position           | 7.19          |
|    y_velocity           | 0.694         |
| rollout/                |               |
|    adjusted_reward      | 8.16          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.92e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 321           |
|    time_elapsed         | 4116          |
|    total_timesteps      | 3287040       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10080         |
|    mean_motor0          | 0.4208877     |
|    mean_motor1          | 0.45124444    |
|    mean_motor2          | 0.66557384    |
|    mean_motor3          | 0.4004973     |
|    mean_motor4          | 0.45507926    |
|    mean_motor5          | 0.5810579     |
|    mean_motor6          | 0.39166152    |
|    mean_motor7          | 0.5645391     |
| train/                  |               |
|    approx_kl            | 0.029082458   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0872        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.729         |
|    cost_value_loss      | 1.86e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.94         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00385       |
|    mean_cost_advantages | -7.003688e-05 |
|    mean_reward_advan... | -0.020287788  |
|    n_updates            | 6400          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00173      |
|    reward_explained_... | 0.829         |
|    reward_value_loss    | 0.0381        |
|    std                  | 0.313         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 409           |
|    mean_reward          | 3.53e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.44          |
|    forward_reward       | 0.295         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.41         |
|    reward_forward       | 0.295         |
|    reward_survive       | 1             |
|    x_position           | -0.826        |
|    x_velocity           | 0.295         |
|    y_position           | 6.37          |
|    y_velocity           | 0.862         |
| rollout/                |               |
|    adjusted_reward      | 8.77          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 3.93e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 322           |
|    time_elapsed         | 4129          |
|    total_timesteps      | 3297280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10096         |
|    mean_motor0          | 0.43630475    |
|    mean_motor1          | 0.44987088    |
|    mean_motor2          | 0.66683424    |
|    mean_motor3          | 0.41761318    |
|    mean_motor4          | 0.46249804    |
|    mean_motor5          | 0.54519683    |
|    mean_motor6          | 0.37475947    |
|    mean_motor7          | 0.5528307     |
| train/                  |               |
|    approx_kl            | 0.038147766   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0736        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.808         |
|    cost_value_loss      | 4.75e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.94         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0162        |
|    mean_cost_advantages | 4.6352157e-05 |
|    mean_reward_advan... | -0.008630037  |
|    n_updates            | 6420          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00149      |
|    reward_explained_... | 0.842         |
|    reward_value_loss    | 0.0333        |
|    std                  | 0.313         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 4.22e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.43          |
|    forward_reward       | 0.356         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.34         |
|    reward_forward       | 0.356         |
|    reward_survive       | 1             |
|    x_position           | -0.216        |
|    x_velocity           | 0.356         |
|    y_position           | 5.17          |
|    y_velocity           | 0.988         |
| rollout/                |               |
|    adjusted_reward      | 7.66          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 3.91e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 323           |
|    time_elapsed         | 4142          |
|    total_timesteps      | 3307520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10114         |
|    mean_motor0          | 0.4230486     |
|    mean_motor1          | 0.49139318    |
|    mean_motor2          | 0.6602693     |
|    mean_motor3          | 0.41035882    |
|    mean_motor4          | 0.4659965     |
|    mean_motor5          | 0.532978      |
|    mean_motor6          | 0.391604      |
|    mean_motor7          | 0.55693007    |
| train/                  |               |
|    approx_kl            | 0.034527488   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0792        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.848         |
|    cost_value_loss      | 4.01e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.93         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00314       |
|    mean_cost_advantages | 2.353982e-06  |
|    mean_reward_advan... | -0.0038597626 |
|    n_updates            | 6440          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00139      |
|    reward_explained_... | 0.738         |
|    reward_value_loss    | 0.0391        |
|    std                  | 0.312         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.2e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 10.4           |
|    forward_reward       | 0.118          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.15          |
|    reward_forward       | 0.118          |
|    reward_survive       | 1              |
|    x_position           | -1.97          |
|    x_velocity           | 0.118          |
|    y_position           | 10.2           |
|    y_velocity           | 0.18           |
| rollout/                |                |
|    adjusted_reward      | 8.21           |
|    ep_len_mean          | 483            |
|    ep_rew_mean          | 3.92e+03       |
| time/                   |                |
|    fps                  | 798            |
|    iterations           | 324            |
|    time_elapsed         | 4155           |
|    total_timesteps      | 3317760        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10235          |
|    greater_than_0.5     | 10126          |
|    mean_motor0          | 0.4154738      |
|    mean_motor1          | 0.4566807      |
|    mean_motor2          | 0.68043774     |
|    mean_motor3          | 0.42465702     |
|    mean_motor4          | 0.48789054     |
|    mean_motor5          | 0.5393911      |
|    mean_motor6          | 0.40544415     |
|    mean_motor7          | 0.5499618      |
| train/                  |                |
|    approx_kl            | 0.04309941     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0758         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.942          |
|    cost_value_loss      | 1.63e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.92          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.016          |
|    mean_cost_advantages | -0.00035757237 |
|    mean_reward_advan... | -0.019529346   |
|    n_updates            | 6460           |
|    nu                   | 11.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00154       |
|    reward_explained_... | 0.878          |
|    reward_value_loss    | 0.04           |
|    std                  | 0.312          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 4.05e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.72         |
|    forward_reward       | 0.265        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.26        |
|    reward_forward       | 0.265        |
|    reward_survive       | 1            |
|    x_position           | -0.365       |
|    x_velocity           | 0.265        |
|    y_position           | 5.6          |
|    y_velocity           | 0.775        |
| rollout/                |              |
|    adjusted_reward      | 7.71         |
|    ep_len_mean          | 492          |
|    ep_rew_mean          | 4.01e+03     |
| time/                   |              |
|    fps                  | 798          |
|    iterations           | 325          |
|    time_elapsed         | 4169         |
|    total_timesteps      | 3328000      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10134        |
|    mean_motor0          | 0.45611772   |
|    mean_motor1          | 0.47202796   |
|    mean_motor2          | 0.69324636   |
|    mean_motor3          | 0.43559662   |
|    mean_motor4          | 0.47749424   |
|    mean_motor5          | 0.5370892    |
|    mean_motor6          | 0.43544728   |
|    mean_motor7          | 0.53200257   |
| train/                  |              |
|    approx_kl            | 0.24173912   |
|    average_cost         | 0.016308594  |
|    clip_fraction        | 0.338        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -0.504       |
|    cost_value_loss      | 0.00262      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.92        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00677     |
|    mean_cost_advantages | 0.024415096  |
|    mean_reward_advan... | -0.014144646 |
|    n_updates            | 6480         |
|    nu                   | 11.1         |
|    nu_loss              | -0.181       |
|    policy_gradient_loss | -0.00472     |
|    reward_explained_... | 0.852        |
|    reward_value_loss    | 0.0357       |
|    std                  | 0.312        |
|    total_cost           | 167.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.7e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.78          |
|    forward_reward       | 0.321         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.35         |
|    reward_forward       | 0.321         |
|    reward_survive       | 1             |
|    x_position           | -1.21         |
|    x_velocity           | 0.321         |
|    y_position           | 7.64          |
|    y_velocity           | 0.59          |
| rollout/                |               |
|    adjusted_reward      | 7.38          |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 3.91e+03      |
| time/                   |               |
|    fps                  | 798           |
|    iterations           | 326           |
|    time_elapsed         | 4182          |
|    total_timesteps      | 3338240       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10235         |
|    greater_than_0.5     | 10109         |
|    mean_motor0          | 0.44625276    |
|    mean_motor1          | 0.48810062    |
|    mean_motor2          | 0.6923689     |
|    mean_motor3          | 0.42965525    |
|    mean_motor4          | 0.45599452    |
|    mean_motor5          | 0.53536296    |
|    mean_motor6          | 0.42659774    |
|    mean_motor7          | 0.5354883     |
| train/                  |               |
|    approx_kl            | 0.03632705    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0994        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.8           |
|    cost_value_loss      | 0.000291      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.9          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00442       |
|    mean_cost_advantages | -0.0008165127 |
|    mean_reward_advan... | -0.026225451  |
|    n_updates            | 6500          |
|    nu                   | 11.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00165      |
|    reward_explained_... | 0.819         |
|    reward_value_loss    | 0.0424        |
|    std                  | 0.311         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 4.05e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 9.21           |
|    forward_reward       | 0.269          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.28          |
|    reward_forward       | 0.269          |
|    reward_survive       | 1              |
|    x_position           | -1.27          |
|    x_velocity           | 0.269          |
|    y_position           | 9.12           |
|    y_velocity           | 0.8            |
| rollout/                |                |
|    adjusted_reward      | 7.59           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 3.83e+03       |
| time/                   |                |
|    fps                  | 798            |
|    iterations           | 327            |
|    time_elapsed         | 4195           |
|    total_timesteps      | 3348480        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10145          |
|    mean_motor0          | 0.47221336     |
|    mean_motor1          | 0.50133216     |
|    mean_motor2          | 0.7094107      |
|    mean_motor3          | 0.44227296     |
|    mean_motor4          | 0.47973037     |
|    mean_motor5          | 0.5155929      |
|    mean_motor6          | 0.40352002     |
|    mean_motor7          | 0.5202463      |
| train/                  |                |
|    approx_kl            | 0.040818326    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0963         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.838          |
|    cost_value_loss      | 2.17e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.89          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0153         |
|    mean_cost_advantages | -5.6770317e-05 |
|    mean_reward_advan... | -0.029188398   |
|    n_updates            | 6520           |
|    nu                   | 11.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00178       |
|    reward_explained_... | 0.847          |
|    reward_value_loss    | 0.0384         |
|    std                  | 0.311          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 389         |
|    mean_reward          | 2.76e+03    |
| infos/                  |             |
|    cost                 | 0.026       |
|    distance_from_origin | 9.45        |
|    forward_reward       | 0.247       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.37       |
|    reward_forward       | 0.247       |
|    reward_survive       | 1           |
|    x_position           | -1.81       |
|    x_velocity           | 0.247       |
|    y_position           | 9.25        |
|    y_velocity           | 0.395       |
| rollout/                |             |
|    adjusted_reward      | 7.12        |
|    ep_len_mean          | 491         |
|    ep_rew_mean          | 3.76e+03    |
| time/                   |             |
|    fps                  | 798         |
|    iterations           | 328         |
|    time_elapsed         | 4208        |
|    total_timesteps      | 3358720     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10139       |
|    mean_motor0          | 0.45885438  |
|    mean_motor1          | 0.44241238  |
|    mean_motor2          | 0.7618237   |
|    mean_motor3          | 0.46681175  |
|    mean_motor4          | 0.48172742  |
|    mean_motor5          | 0.5664199   |
|    mean_motor6          | 0.43436128  |
|    mean_motor7          | 0.49020416  |
| train/                  |             |
|    approx_kl            | 1.438255    |
|    average_cost         | 0.024414062 |
|    clip_fraction        | 0.475       |
|    clip_range           | 0.4         |
|    cost_explained_va... | -0.503      |
|    cost_value_loss      | 0.00683     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -1.89       |
|    learning_rate        | 3e-05       |
|    loss                 | 0.548       |
|    mean_cost_advantages | 0.0360054   |
|    mean_reward_advan... | -0.02748799 |
|    n_updates            | 6540        |
|    nu                   | 11.2        |
|    nu_loss              | -0.272      |
|    policy_gradient_loss | -0.00713    |
|    reward_explained_... | 0.839       |
|    reward_value_loss    | 0.041       |
|    std                  | 0.311       |
|    total_cost           | 250.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.43e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.29        |
|    forward_reward       | 0.174       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.29       |
|    reward_forward       | 0.174       |
|    reward_survive       | 1           |
|    x_position           | -0.242      |
|    x_velocity           | 0.174       |
|    y_position           | 5.28        |
|    y_velocity           | 0.396       |
| rollout/                |             |
|    adjusted_reward      | 5.26        |
|    ep_len_mean          | 491         |
|    ep_rew_mean          | 3.47e+03    |
| time/                   |             |
|    fps                  | 797         |
|    iterations           | 329         |
|    time_elapsed         | 4222        |
|    total_timesteps      | 3368960     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10133       |
|    mean_motor0          | 0.4512171   |
|    mean_motor1          | 0.43702787  |
|    mean_motor2          | 0.6952072   |
|    mean_motor3          | 0.42326587  |
|    mean_motor4          | 0.45732704  |
|    mean_motor5          | 0.5282594   |
|    mean_motor6          | 0.50251687  |
|    mean_motor7          | 0.54748935  |
| train/                  |             |
|    approx_kl            | 1.4377998   |
|    average_cost         | 0.05859375  |
|    clip_fraction        | 0.465       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.907       |
|    cost_value_loss      | 0.0805      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -1.89       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0239     |
|    mean_cost_advantages | 0.062037688 |
|    mean_reward_advan... | -0.04741791 |
|    n_updates            | 6560        |
|    nu                   | 11.2        |
|    nu_loss              | -0.655      |
|    policy_gradient_loss | -0.0251     |
|    reward_explained_... | 0.801       |
|    reward_value_loss    | 0.0559      |
|    std                  | 0.311       |
|    total_cost           | 600.0       |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.53e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.78           |
|    forward_reward       | 0.157          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.21          |
|    reward_forward       | 0.157          |
|    reward_survive       | 1              |
|    x_position           | -0.128         |
|    x_velocity           | 0.157          |
|    y_position           | 5.77           |
|    y_velocity           | 0.401          |
| rollout/                |                |
|    adjusted_reward      | 4.98           |
|    ep_len_mean          | 491            |
|    ep_rew_mean          | 3.21e+03       |
| time/                   |                |
|    fps                  | 797            |
|    iterations           | 330            |
|    time_elapsed         | 4235           |
|    total_timesteps      | 3379200        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10079          |
|    mean_motor0          | 0.43375516     |
|    mean_motor1          | 0.43338957     |
|    mean_motor2          | 0.6995226      |
|    mean_motor3          | 0.42485315     |
|    mean_motor4          | 0.45820308     |
|    mean_motor5          | 0.49734458     |
|    mean_motor6          | 0.4805743      |
|    mean_motor7          | 0.54282194     |
| train/                  |                |
|    approx_kl            | 0.042394467    |
|    average_cost         | 0.00068359374  |
|    clip_fraction        | 0.0972         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.918          |
|    cost_value_loss      | 0.000813       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.88          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.022          |
|    mean_cost_advantages | -0.00060299545 |
|    mean_reward_advan... | -0.0693634     |
|    n_updates            | 6580           |
|    nu                   | 11.3           |
|    nu_loss              | -0.00768       |
|    policy_gradient_loss | -0.00163       |
|    reward_explained_... | 0.707          |
|    reward_value_loss    | 0.0369         |
|    std                  | 0.31           |
|    total_cost           | 7.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.65e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.4          |
|    forward_reward       | 0.201        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.3         |
|    reward_forward       | 0.201        |
|    reward_survive       | 1            |
|    x_position           | -0.036       |
|    x_velocity           | 0.201        |
|    y_position           | 6.36         |
|    y_velocity           | 0.509        |
| rollout/                |              |
|    adjusted_reward      | 4.81         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 2.94e+03     |
| time/                   |              |
|    fps                  | 797          |
|    iterations           | 331          |
|    time_elapsed         | 4248         |
|    total_timesteps      | 3389440      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10109        |
|    mean_motor0          | 0.45875078   |
|    mean_motor1          | 0.44198814   |
|    mean_motor2          | 0.68185854   |
|    mean_motor3          | 0.41952658   |
|    mean_motor4          | 0.4565831    |
|    mean_motor5          | 0.4903759    |
|    mean_motor6          | 0.46277398   |
|    mean_motor7          | 0.5343076    |
| train/                  |              |
|    approx_kl            | 0.037479024  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0883       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.903        |
|    cost_value_loss      | 7.96e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.86        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00732      |
|    mean_cost_advantages | 0.0002103976 |
|    mean_reward_advan... | -0.067903146 |
|    n_updates            | 6600         |
|    nu                   | 11.3         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00136     |
|    reward_explained_... | 0.599        |
|    reward_value_loss    | 0.0326       |
|    std                  | 0.31         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.39e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.1           |
|    forward_reward       | 0.188         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.28         |
|    reward_forward       | 0.188         |
|    reward_survive       | 1             |
|    x_position           | -0.113        |
|    x_velocity           | 0.188         |
|    y_position           | 4.06          |
|    y_velocity           | 0.474         |
| rollout/                |               |
|    adjusted_reward      | 5.14          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.69e+03      |
| time/                   |               |
|    fps                  | 797           |
|    iterations           | 332           |
|    time_elapsed         | 4262          |
|    total_timesteps      | 3399680       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10125         |
|    mean_motor0          | 0.44289547    |
|    mean_motor1          | 0.43197274    |
|    mean_motor2          | 0.6913987     |
|    mean_motor3          | 0.4366826     |
|    mean_motor4          | 0.47445393    |
|    mean_motor5          | 0.510786      |
|    mean_motor6          | 0.5177139     |
|    mean_motor7          | 0.5269958     |
| train/                  |               |
|    approx_kl            | 0.036553048   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0981        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.906         |
|    cost_value_loss      | 4.63e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.86         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00313       |
|    mean_cost_advantages | -3.450811e-05 |
|    mean_reward_advan... | -0.063786246  |
|    n_updates            | 6620          |
|    nu                   | 11.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00129      |
|    reward_explained_... | 0.761         |
|    reward_value_loss    | 0.0294        |
|    std                  | 0.309         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.72e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.16           |
|    forward_reward       | 0.197          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.36          |
|    reward_forward       | 0.197          |
|    reward_survive       | 1              |
|    x_position           | -0.376         |
|    x_velocity           | 0.197          |
|    y_position           | 6.13           |
|    y_velocity           | 0.327          |
| rollout/                |                |
|    adjusted_reward      | 5.23           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.53e+03       |
| time/                   |                |
|    fps                  | 797            |
|    iterations           | 333            |
|    time_elapsed         | 4275           |
|    total_timesteps      | 3409920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10138          |
|    mean_motor0          | 0.48535794     |
|    mean_motor1          | 0.43707862     |
|    mean_motor2          | 0.7095858      |
|    mean_motor3          | 0.42231697     |
|    mean_motor4          | 0.46214598     |
|    mean_motor5          | 0.5333381      |
|    mean_motor6          | 0.47824687     |
|    mean_motor7          | 0.5128145      |
| train/                  |                |
|    approx_kl            | 0.04089638     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0977         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.954          |
|    cost_value_loss      | 7.61e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.84          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00626        |
|    mean_cost_advantages | -0.00066063076 |
|    mean_reward_advan... | -0.0526685     |
|    n_updates            | 6640           |
|    nu                   | 11.4           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00146       |
|    reward_explained_... | 0.741          |
|    reward_value_loss    | 0.0286         |
|    std                  | 0.309          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.65e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.93           |
|    forward_reward       | 0.225          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.16          |
|    reward_forward       | 0.225          |
|    reward_survive       | 1              |
|    x_position           | -0.451         |
|    x_velocity           | 0.225          |
|    y_position           | 4.84           |
|    y_velocity           | 0.59           |
| rollout/                |                |
|    adjusted_reward      | 5.06           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.53e+03       |
| time/                   |                |
|    fps                  | 797            |
|    iterations           | 334            |
|    time_elapsed         | 4288           |
|    total_timesteps      | 3420160        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10132          |
|    mean_motor0          | 0.480264       |
|    mean_motor1          | 0.44515473     |
|    mean_motor2          | 0.7033514      |
|    mean_motor3          | 0.40664965     |
|    mean_motor4          | 0.46623093     |
|    mean_motor5          | 0.5025787      |
|    mean_motor6          | 0.56321126     |
|    mean_motor7          | 0.532339       |
| train/                  |                |
|    approx_kl            | 0.044823576    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.106          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.882          |
|    cost_value_loss      | 6.01e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.82          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00955        |
|    mean_cost_advantages | -1.4420596e-05 |
|    mean_reward_advan... | -0.042740468   |
|    n_updates            | 6660           |
|    nu                   | 11.5           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00168       |
|    reward_explained_... | 0.787          |
|    reward_value_loss    | 0.0246         |
|    std                  | 0.308          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.95e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.98          |
|    forward_reward       | 0.42          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.31         |
|    reward_forward       | 0.42          |
|    reward_survive       | 1             |
|    x_position           | -0.349        |
|    x_velocity           | 0.42          |
|    y_position           | 3.89          |
|    y_velocity           | 0.706         |
| rollout/                |               |
|    adjusted_reward      | 5.45          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.57e+03      |
| time/                   |               |
|    fps                  | 797           |
|    iterations           | 335           |
|    time_elapsed         | 4302          |
|    total_timesteps      | 3430400       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10131         |
|    mean_motor0          | 0.5063399     |
|    mean_motor1          | 0.41654152    |
|    mean_motor2          | 0.7158023     |
|    mean_motor3          | 0.42648035    |
|    mean_motor4          | 0.4570438     |
|    mean_motor5          | 0.49505085    |
|    mean_motor6          | 0.5046475     |
|    mean_motor7          | 0.5235778     |
| train/                  |               |
|    approx_kl            | 0.04018612    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.103         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.912         |
|    cost_value_loss      | 3.73e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.8          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0142        |
|    mean_cost_advantages | -4.678111e-05 |
|    mean_reward_advan... | -0.04293359   |
|    n_updates            | 6680          |
|    nu                   | 11.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00161      |
|    reward_explained_... | 0.764         |
|    reward_value_loss    | 0.023         |
|    std                  | 0.307         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.68e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.55          |
|    forward_reward       | 0.213         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.38         |
|    reward_forward       | 0.213         |
|    reward_survive       | 1             |
|    x_position           | -0.948        |
|    x_velocity           | 0.213         |
|    y_position           | 5.45          |
|    y_velocity           | 0.6           |
| rollout/                |               |
|    adjusted_reward      | 5.79          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.64e+03      |
| time/                   |               |
|    fps                  | 797           |
|    iterations           | 336           |
|    time_elapsed         | 4315          |
|    total_timesteps      | 3440640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10161         |
|    mean_motor0          | 0.51158273    |
|    mean_motor1          | 0.4010195     |
|    mean_motor2          | 0.71547693    |
|    mean_motor3          | 0.42468697    |
|    mean_motor4          | 0.47710308    |
|    mean_motor5          | 0.52509123    |
|    mean_motor6          | 0.5709482     |
|    mean_motor7          | 0.5116573     |
| train/                  |               |
|    approx_kl            | 0.039164703   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0968        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.885         |
|    cost_value_loss      | 3.58e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.78         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0124        |
|    mean_cost_advantages | -4.643622e-05 |
|    mean_reward_advan... | -0.038387366  |
|    n_updates            | 6700          |
|    nu                   | 11.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00167      |
|    reward_explained_... | 0.239         |
|    reward_value_loss    | 0.0238        |
|    std                  | 0.306         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.87e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.63           |
|    forward_reward       | 0.236          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.26          |
|    reward_forward       | 0.236          |
|    reward_survive       | 1              |
|    x_position           | -0.624         |
|    x_velocity           | 0.236          |
|    y_position           | 6.59           |
|    y_velocity           | 0.438          |
| rollout/                |                |
|    adjusted_reward      | 5.59           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 2.66e+03       |
| time/                   |                |
|    fps                  | 797            |
|    iterations           | 337            |
|    time_elapsed         | 4329           |
|    total_timesteps      | 3450880        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10147          |
|    mean_motor0          | 0.51600766     |
|    mean_motor1          | 0.40819615     |
|    mean_motor2          | 0.7067379      |
|    mean_motor3          | 0.42903978     |
|    mean_motor4          | 0.48173866     |
|    mean_motor5          | 0.4834475      |
|    mean_motor6          | 0.53838027     |
|    mean_motor7          | 0.5246692      |
| train/                  |                |
|    approx_kl            | 0.039590955    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0749         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.906          |
|    cost_value_loss      | 2.07e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.76          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0144         |
|    mean_cost_advantages | -0.00020761645 |
|    mean_reward_advan... | -0.02159656    |
|    n_updates            | 6720           |
|    nu                   | 11.6           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00146       |
|    reward_explained_... | 0.283          |
|    reward_value_loss    | 0.0194         |
|    std                  | 0.305          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.81e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.2            |
|    forward_reward       | 0.117          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.24          |
|    reward_forward       | 0.117          |
|    reward_survive       | 1              |
|    x_position           | -0.716         |
|    x_velocity           | 0.117          |
|    y_position           | 6.12           |
|    y_velocity           | 0.225          |
| rollout/                |                |
|    adjusted_reward      | 5.58           |
|    ep_len_mean          | 489            |
|    ep_rew_mean          | 2.71e+03       |
| time/                   |                |
|    fps                  | 797            |
|    iterations           | 338            |
|    time_elapsed         | 4342           |
|    total_timesteps      | 3461120        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10145          |
|    mean_motor0          | 0.5058889      |
|    mean_motor1          | 0.41944212     |
|    mean_motor2          | 0.7202569      |
|    mean_motor3          | 0.42567784     |
|    mean_motor4          | 0.47658467     |
|    mean_motor5          | 0.49287185     |
|    mean_motor6          | 0.5821089      |
|    mean_motor7          | 0.5333844      |
| train/                  |                |
|    approx_kl            | 0.037529904    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0896         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.723          |
|    cost_value_loss      | 9.14e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.73          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00509        |
|    mean_cost_advantages | -0.00031962915 |
|    mean_reward_advan... | -0.028690165   |
|    n_updates            | 6740           |
|    nu                   | 11.6           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.0015        |
|    reward_explained_... | 0.415          |
|    reward_value_loss    | 0.0214         |
|    std                  | 0.304          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 416           |
|    mean_reward          | 1.95e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.69          |
|    forward_reward       | 0.18          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.21         |
|    reward_forward       | 0.18          |
|    reward_survive       | 1             |
|    x_position           | -0.964        |
|    x_velocity           | 0.18          |
|    y_position           | 5.51          |
|    y_velocity           | 0.51          |
| rollout/                |               |
|    adjusted_reward      | 5.77          |
|    ep_len_mean          | 489           |
|    ep_rew_mean          | 2.77e+03      |
| time/                   |               |
|    fps                  | 797           |
|    iterations           | 339           |
|    time_elapsed         | 4355          |
|    total_timesteps      | 3471360       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10142         |
|    mean_motor0          | 0.5556445     |
|    mean_motor1          | 0.42250246    |
|    mean_motor2          | 0.699513      |
|    mean_motor3          | 0.43066922    |
|    mean_motor4          | 0.483497      |
|    mean_motor5          | 0.4924756     |
|    mean_motor6          | 0.54953915    |
|    mean_motor7          | 0.54274684    |
| train/                  |               |
|    approx_kl            | 0.04815121    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0963        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.905         |
|    cost_value_loss      | 2.75e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.71         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000725      |
|    mean_cost_advantages | -0.0003557989 |
|    mean_reward_advan... | -0.019212905  |
|    n_updates            | 6760          |
|    nu                   | 11.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00144      |
|    reward_explained_... | 0.797         |
|    reward_value_loss    | 0.0189        |
|    std                  | 0.303         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.01e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.93          |
|    forward_reward       | 0.122         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.39         |
|    reward_forward       | 0.122         |
|    reward_survive       | 1             |
|    x_position           | -0.553        |
|    x_velocity           | 0.122         |
|    y_position           | 4.67          |
|    y_velocity           | 0.198         |
| rollout/                |               |
|    adjusted_reward      | 5.66          |
|    ep_len_mean          | 489           |
|    ep_rew_mean          | 2.78e+03      |
| time/                   |               |
|    fps                  | 796           |
|    iterations           | 340           |
|    time_elapsed         | 4368          |
|    total_timesteps      | 3481600       |
| torque/                 |               |
|    greater_than_0.25    | 10235         |
|    greater_than_0.3     | 10233         |
|    greater_than_0.5     | 10138         |
|    mean_motor0          | 0.5186962     |
|    mean_motor1          | 0.4131011     |
|    mean_motor2          | 0.7057091     |
|    mean_motor3          | 0.44366878    |
|    mean_motor4          | 0.48259154    |
|    mean_motor5          | 0.4843111     |
|    mean_motor6          | 0.525649      |
|    mean_motor7          | 0.54088247    |
| train/                  |               |
|    approx_kl            | 0.037935585   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0874        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.928         |
|    cost_value_loss      | 2.61e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.68         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00397       |
|    mean_cost_advantages | -0.0007739789 |
|    mean_reward_advan... | -0.013107173  |
|    n_updates            | 6780          |
|    nu                   | 11.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00157      |
|    reward_explained_... | 0.764         |
|    reward_value_loss    | 0.0182        |
|    std                  | 0.302         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.3e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.28           |
|    forward_reward       | 0.229          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.32          |
|    reward_forward       | 0.229          |
|    reward_survive       | 1              |
|    x_position           | -1.18          |
|    x_velocity           | 0.229          |
|    y_position           | 6.15           |
|    y_velocity           | 0.306          |
| rollout/                |                |
|    adjusted_reward      | 5.46           |
|    ep_len_mean          | 497            |
|    ep_rew_mean          | 2.79e+03       |
| time/                   |                |
|    fps                  | 796            |
|    iterations           | 341            |
|    time_elapsed         | 4382           |
|    total_timesteps      | 3491840        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10133          |
|    mean_motor0          | 0.5265326      |
|    mean_motor1          | 0.4287044      |
|    mean_motor2          | 0.698758       |
|    mean_motor3          | 0.44253048     |
|    mean_motor4          | 0.4886364      |
|    mean_motor5          | 0.48134294     |
|    mean_motor6          | 0.5241002      |
|    mean_motor7          | 0.5667329      |
| train/                  |                |
|    approx_kl            | 0.037874438    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0743         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.939          |
|    cost_value_loss      | 2.94e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.67          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00812        |
|    mean_cost_advantages | -0.00079057796 |
|    mean_reward_advan... | -0.013812736   |
|    n_updates            | 6800           |
|    nu                   | 11.7           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00144       |
|    reward_explained_... | 0.818          |
|    reward_value_loss    | 0.0181         |
|    std                  | 0.303          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.88e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7             |
|    forward_reward       | 0.192         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.28         |
|    reward_forward       | 0.192         |
|    reward_survive       | 1             |
|    x_position           | -1.38         |
|    x_velocity           | 0.192         |
|    y_position           | 6.86          |
|    y_velocity           | 0.248         |
| rollout/                |               |
|    adjusted_reward      | 5.8           |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.8e+03       |
| time/                   |               |
|    fps                  | 796           |
|    iterations           | 342           |
|    time_elapsed         | 4395          |
|    total_timesteps      | 3502080       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10133         |
|    mean_motor0          | 0.5076834     |
|    mean_motor1          | 0.38611716    |
|    mean_motor2          | 0.70097965    |
|    mean_motor3          | 0.4569668     |
|    mean_motor4          | 0.49809012    |
|    mean_motor5          | 0.4486956     |
|    mean_motor6          | 0.4478692     |
|    mean_motor7          | 0.5521811     |
| train/                  |               |
|    approx_kl            | 0.04752198    |
|    average_cost         | 0.0001953125  |
|    clip_fraction        | 0.0985        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.894         |
|    cost_value_loss      | 0.000206      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.66         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00913       |
|    mean_cost_advantages | -0.0014640063 |
|    mean_reward_advan... | -0.013218532  |
|    n_updates            | 6820          |
|    nu                   | 11.7          |
|    nu_loss              | -0.00228      |
|    policy_gradient_loss | -0.00172      |
|    reward_explained_... | 0.867         |
|    reward_value_loss    | 0.0169        |
|    std                  | 0.302         |
|    total_cost           | 2.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.95e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.11           |
|    forward_reward       | 0.0783         |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.16          |
|    reward_forward       | 0.0783         |
|    reward_survive       | 1              |
|    x_position           | -1.02          |
|    x_velocity           | 0.0783         |
|    y_position           | 7              |
|    y_velocity           | 0.147          |
| rollout/                |                |
|    adjusted_reward      | 5.73           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 2.81e+03       |
| time/                   |                |
|    fps                  | 796            |
|    iterations           | 343            |
|    time_elapsed         | 4409           |
|    total_timesteps      | 3512320        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10138          |
|    mean_motor0          | 0.47473803     |
|    mean_motor1          | 0.39307022     |
|    mean_motor2          | 0.6852615      |
|    mean_motor3          | 0.45833644     |
|    mean_motor4          | 0.4804152      |
|    mean_motor5          | 0.44381452     |
|    mean_motor6          | 0.44703498     |
|    mean_motor7          | 0.5635325      |
| train/                  |                |
|    approx_kl            | 0.03748376     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0861         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.577          |
|    cost_value_loss      | 1.56e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.65          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00157        |
|    mean_cost_advantages | -0.00031727477 |
|    mean_reward_advan... | -0.01621905    |
|    n_updates            | 6840           |
|    nu                   | 11.7           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00142       |
|    reward_explained_... | 0.379          |
|    reward_value_loss    | 0.0176         |
|    std                  | 0.302          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 405           |
|    mean_reward          | 2.43e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.22          |
|    forward_reward       | 0.193         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.27         |
|    reward_forward       | 0.193         |
|    reward_survive       | 1             |
|    x_position           | -0.348        |
|    x_velocity           | 0.193         |
|    y_position           | 4.19          |
|    y_velocity           | 0.509         |
| rollout/                |               |
|    adjusted_reward      | 5.79          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.82e+03      |
| time/                   |               |
|    fps                  | 796           |
|    iterations           | 344           |
|    time_elapsed         | 4421          |
|    total_timesteps      | 3522560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10141         |
|    mean_motor0          | 0.48355103    |
|    mean_motor1          | 0.3988513     |
|    mean_motor2          | 0.69731045    |
|    mean_motor3          | 0.47542644    |
|    mean_motor4          | 0.480163      |
|    mean_motor5          | 0.44375587    |
|    mean_motor6          | 0.47770983    |
|    mean_motor7          | 0.5598775     |
| train/                  |               |
|    approx_kl            | 0.03573386    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0859        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.95          |
|    cost_value_loss      | 8.99e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.62         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00246      |
|    mean_cost_advantages | -0.0006149024 |
|    mean_reward_advan... | -0.013267512  |
|    n_updates            | 6860          |
|    nu                   | 11.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00145      |
|    reward_explained_... | 0.557         |
|    reward_value_loss    | 0.0176        |
|    std                  | 0.3           |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 408           |
|    mean_reward          | 2.3e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.88          |
|    forward_reward       | 0.155         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.3          |
|    reward_forward       | 0.155         |
|    reward_survive       | 1             |
|    x_position           | -0.703        |
|    x_velocity           | 0.155         |
|    y_position           | 4.81          |
|    y_velocity           | 0.481         |
| rollout/                |               |
|    adjusted_reward      | 5.79          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.81e+03      |
| time/                   |               |
|    fps                  | 796           |
|    iterations           | 345           |
|    time_elapsed         | 4434          |
|    total_timesteps      | 3532800       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10121         |
|    mean_motor0          | 0.44885406    |
|    mean_motor1          | 0.3695724     |
|    mean_motor2          | 0.6831137     |
|    mean_motor3          | 0.5058048     |
|    mean_motor4          | 0.46608496    |
|    mean_motor5          | 0.43205673    |
|    mean_motor6          | 0.41292325    |
|    mean_motor7          | 0.564806      |
| train/                  |               |
|    approx_kl            | 0.092117496   |
|    average_cost         | 0.0087890625  |
|    clip_fraction        | 0.206         |
|    clip_range           | 0.4           |
|    cost_explained_va... | -1.83         |
|    cost_value_loss      | 0.00529       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.6          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00881       |
|    mean_cost_advantages | 0.012794155   |
|    mean_reward_advan... | -0.0073951907 |
|    n_updates            | 6880          |
|    nu                   | 11.7          |
|    nu_loss              | -0.103        |
|    policy_gradient_loss | -0.00295      |
|    reward_explained_... | 0.72          |
|    reward_value_loss    | 0.0174        |
|    std                  | 0.3           |
|    total_cost           | 90.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.88e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.12         |
|    forward_reward       | 0.119        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.25        |
|    reward_forward       | 0.119        |
|    reward_survive       | 1            |
|    x_position           | -1.24        |
|    x_velocity           | 0.119        |
|    y_position           | 5.96         |
|    y_velocity           | 0.347        |
| rollout/                |              |
|    adjusted_reward      | 5.78         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 796          |
|    iterations           | 346          |
|    time_elapsed         | 4448         |
|    total_timesteps      | 3543040      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10134        |
|    mean_motor0          | 0.4596857    |
|    mean_motor1          | 0.36870244   |
|    mean_motor2          | 0.68974787   |
|    mean_motor3          | 0.49884757   |
|    mean_motor4          | 0.47806773   |
|    mean_motor5          | 0.42536792   |
|    mean_motor6          | 0.41175383   |
|    mean_motor7          | 0.55598617   |
| train/                  |              |
|    approx_kl            | 0.04069708   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0772       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.929        |
|    cost_value_loss      | 8.49e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.59        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00323      |
|    mean_cost_advantages | 7.372811e-05 |
|    mean_reward_advan... | -0.010538457 |
|    n_updates            | 6900         |
|    nu                   | 11.7         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00103     |
|    reward_explained_... | 0.421        |
|    reward_value_loss    | 0.0155       |
|    std                  | 0.3          |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 470            |
|    mean_reward          | 2.7e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.35           |
|    forward_reward       | 0.23           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.26          |
|    reward_forward       | 0.23           |
|    reward_survive       | 1              |
|    x_position           | -1.29          |
|    x_velocity           | 0.23           |
|    y_position           | 6.2            |
|    y_velocity           | 0.593          |
| rollout/                |                |
|    adjusted_reward      | 5.55           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 2.84e+03       |
| time/                   |                |
|    fps                  | 796            |
|    iterations           | 347            |
|    time_elapsed         | 4461           |
|    total_timesteps      | 3553280        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10122          |
|    mean_motor0          | 0.47234273     |
|    mean_motor1          | 0.38181207     |
|    mean_motor2          | 0.696856       |
|    mean_motor3          | 0.47646904     |
|    mean_motor4          | 0.48992148     |
|    mean_motor5          | 0.43695316     |
|    mean_motor6          | 0.4535307      |
|    mean_motor7          | 0.55450165     |
| train/                  |                |
|    approx_kl            | 0.034427       |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0874         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.83           |
|    cost_value_loss      | 2.31e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.58          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0052         |
|    mean_cost_advantages | -0.00029722575 |
|    mean_reward_advan... | -0.012195615   |
|    n_updates            | 6920           |
|    nu                   | 11.7           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00124       |
|    reward_explained_... | 0.447          |
|    reward_value_loss    | 0.0181         |
|    std                  | 0.299          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.74e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.29         |
|    forward_reward       | 0.165        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.165        |
|    reward_survive       | 1            |
|    x_position           | -1.71        |
|    x_velocity           | 0.165        |
|    y_position           | 6.03         |
|    y_velocity           | 0.544        |
| rollout/                |              |
|    adjusted_reward      | 5.51         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.83e+03     |
| time/                   |              |
|    fps                  | 796          |
|    iterations           | 348          |
|    time_elapsed         | 4475         |
|    total_timesteps      | 3563520      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10134        |
|    mean_motor0          | 0.48252678   |
|    mean_motor1          | 0.4024673    |
|    mean_motor2          | 0.70670855   |
|    mean_motor3          | 0.4709044    |
|    mean_motor4          | 0.5094641    |
|    mean_motor5          | 0.42057633   |
|    mean_motor6          | 0.41425943   |
|    mean_motor7          | 0.56038165   |
| train/                  |              |
|    approx_kl            | 0.033868186  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0829       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.915        |
|    cost_value_loss      | 8.28e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.57        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00585      |
|    mean_cost_advantages | 4.886832e-05 |
|    mean_reward_advan... | -0.012525949 |
|    n_updates            | 6940         |
|    nu                   | 11.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00119     |
|    reward_explained_... | 0.672        |
|    reward_value_loss    | 0.0157       |
|    std                  | 0.299        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3e+03        |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.08         |
|    forward_reward       | 0.15         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.15         |
|    reward_survive       | 1            |
|    x_position           | -1.37        |
|    x_velocity           | 0.15         |
|    y_position           | 5.91         |
|    y_velocity           | 0.382        |
| rollout/                |              |
|    adjusted_reward      | 5.8          |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.82e+03     |
| time/                   |              |
|    fps                  | 796          |
|    iterations           | 349          |
|    time_elapsed         | 4488         |
|    total_timesteps      | 3573760      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10125        |
|    mean_motor0          | 0.4841599    |
|    mean_motor1          | 0.38252538   |
|    mean_motor2          | 0.7506597    |
|    mean_motor3          | 0.47415328   |
|    mean_motor4          | 0.5066873    |
|    mean_motor5          | 0.4258429    |
|    mean_motor6          | 0.42655864   |
|    mean_motor7          | 0.55449325   |
| train/                  |              |
|    approx_kl            | 0.03631215   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0809       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.942        |
|    cost_value_loss      | 8.02e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.55        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00157      |
|    mean_cost_advantages | 0.0005610434 |
|    mean_reward_advan... | -0.010191095 |
|    n_updates            | 6960         |
|    nu                   | 11.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00134     |
|    reward_explained_... | 0.803        |
|    reward_value_loss    | 0.0159       |
|    std                  | 0.298        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.86e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.61           |
|    forward_reward       | 0.272          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.31          |
|    reward_forward       | 0.272          |
|    reward_survive       | 1              |
|    x_position           | -0.881         |
|    x_velocity           | 0.272          |
|    y_position           | 5.5            |
|    y_velocity           | 0.392          |
| rollout/                |                |
|    adjusted_reward      | 5.55           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.83e+03       |
| time/                   |                |
|    fps                  | 796            |
|    iterations           | 350            |
|    time_elapsed         | 4502           |
|    total_timesteps      | 3584000        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10151          |
|    mean_motor0          | 0.4771242      |
|    mean_motor1          | 0.37719175     |
|    mean_motor2          | 0.7563385      |
|    mean_motor3          | 0.48634753     |
|    mean_motor4          | 0.5049423      |
|    mean_motor5          | 0.41718712     |
|    mean_motor6          | 0.41464552     |
|    mean_motor7          | 0.5592278      |
| train/                  |                |
|    approx_kl            | 0.050849408    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0984         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.81           |
|    cost_value_loss      | 8.9e-05        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.52          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00218        |
|    mean_cost_advantages | -0.00022378445 |
|    mean_reward_advan... | -0.008173211   |
|    n_updates            | 6980           |
|    nu                   | 11.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00161       |
|    reward_explained_... | 0.566          |
|    reward_value_loss    | 0.0168         |
|    std                  | 0.296          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.86e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.43          |
|    forward_reward       | 0.12          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.25         |
|    reward_forward       | 0.12          |
|    reward_survive       | 1             |
|    x_position           | -1.17         |
|    x_velocity           | 0.12          |
|    y_position           | 5.3           |
|    y_velocity           | 0.35          |
| rollout/                |               |
|    adjusted_reward      | 5.8           |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.82e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 351           |
|    time_elapsed         | 4515          |
|    total_timesteps      | 3594240       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10121         |
|    mean_motor0          | 0.46961913    |
|    mean_motor1          | 0.37765694    |
|    mean_motor2          | 0.74091953    |
|    mean_motor3          | 0.47784132    |
|    mean_motor4          | 0.49379134    |
|    mean_motor5          | 0.41626096    |
|    mean_motor6          | 0.42806536    |
|    mean_motor7          | 0.5608844     |
| train/                  |               |
|    approx_kl            | 0.030639296   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.086         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.931         |
|    cost_value_loss      | 1.89e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.5          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00203       |
|    mean_cost_advantages | -4.509888e-05 |
|    mean_reward_advan... | -0.014645943  |
|    n_updates            | 7000          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00105      |
|    reward_explained_... | 0.477         |
|    reward_value_loss    | 0.0162        |
|    std                  | 0.296         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.35e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.24          |
|    forward_reward       | 0.156         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.27         |
|    reward_forward       | 0.156         |
|    reward_survive       | 1             |
|    x_position           | -1.08         |
|    x_velocity           | 0.156         |
|    y_position           | 6.11          |
|    y_velocity           | 0.285         |
| rollout/                |               |
|    adjusted_reward      | 5.56          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.82e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 352           |
|    time_elapsed         | 4528          |
|    total_timesteps      | 3604480       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10124         |
|    mean_motor0          | 0.45626992    |
|    mean_motor1          | 0.36677542    |
|    mean_motor2          | 0.7345812     |
|    mean_motor3          | 0.4869991     |
|    mean_motor4          | 0.4832353     |
|    mean_motor5          | 0.42110357    |
|    mean_motor6          | 0.4585        |
|    mean_motor7          | 0.55838364    |
| train/                  |               |
|    approx_kl            | 0.03647869    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.105         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.901         |
|    cost_value_loss      | 6.25e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.49         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00772       |
|    mean_cost_advantages | 0.00030112496 |
|    mean_reward_advan... | -0.009756769  |
|    n_updates            | 7020          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00125      |
|    reward_explained_... | 0.49          |
|    reward_value_loss    | 0.0183        |
|    std                  | 0.296         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.51e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.43           |
|    forward_reward       | 0.134          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.21          |
|    reward_forward       | 0.134          |
|    reward_survive       | 1              |
|    x_position           | -0.954         |
|    x_velocity           | 0.134          |
|    y_position           | 5.29           |
|    y_velocity           | 0.247          |
| rollout/                |                |
|    adjusted_reward      | 5.24           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 2.78e+03       |
| time/                   |                |
|    fps                  | 795            |
|    iterations           | 353            |
|    time_elapsed         | 4542           |
|    total_timesteps      | 3614720        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10159          |
|    mean_motor0          | 0.48769182     |
|    mean_motor1          | 0.39414158     |
|    mean_motor2          | 0.71594596     |
|    mean_motor3          | 0.46130523     |
|    mean_motor4          | 0.48609406     |
|    mean_motor5          | 0.43874225     |
|    mean_motor6          | 0.41608533     |
|    mean_motor7          | 0.5761659      |
| train/                  |                |
|    approx_kl            | 0.045435704    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.113          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.696          |
|    cost_value_loss      | 1.06e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.49          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0166         |
|    mean_cost_advantages | -0.00027220632 |
|    mean_reward_advan... | -0.010191068   |
|    n_updates            | 7040           |
|    nu                   | 11.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00117       |
|    reward_explained_... | 0.486          |
|    reward_value_loss    | 0.0168         |
|    std                  | 0.295          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.86e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.03          |
|    forward_reward       | 0.102         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.43         |
|    reward_forward       | 0.102         |
|    reward_survive       | 1             |
|    x_position           | -1.24         |
|    x_velocity           | 0.102         |
|    y_position           | 4.82          |
|    y_velocity           | 0.252         |
| rollout/                |               |
|    adjusted_reward      | 4.96          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.68e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 354           |
|    time_elapsed         | 4555          |
|    total_timesteps      | 3624960       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10132         |
|    mean_motor0          | 0.5309939     |
|    mean_motor1          | 0.39132413    |
|    mean_motor2          | 0.7018497     |
|    mean_motor3          | 0.45213705    |
|    mean_motor4          | 0.4797845     |
|    mean_motor5          | 0.42376786    |
|    mean_motor6          | 0.43510145    |
|    mean_motor7          | 0.57464397    |
| train/                  |               |
|    approx_kl            | 0.04720064    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0895        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.859         |
|    cost_value_loss      | 2.48e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.48         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00505       |
|    mean_cost_advantages | 6.9754765e-06 |
|    mean_reward_advan... | -0.012877072  |
|    n_updates            | 7060          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00133      |
|    reward_explained_... | 0.809         |
|    reward_value_loss    | 0.0171        |
|    std                  | 0.296         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.7e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.04           |
|    forward_reward       | 0.14           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.24          |
|    reward_forward       | 0.14           |
|    reward_survive       | 1              |
|    x_position           | -1.56          |
|    x_velocity           | 0.14           |
|    y_position           | 4.77           |
|    y_velocity           | 0.431          |
| rollout/                |                |
|    adjusted_reward      | 5.14           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 2.64e+03       |
| time/                   |                |
|    fps                  | 795            |
|    iterations           | 355            |
|    time_elapsed         | 4569           |
|    total_timesteps      | 3635200        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10116          |
|    mean_motor0          | 0.45964465     |
|    mean_motor1          | 0.3957276      |
|    mean_motor2          | 0.7253721      |
|    mean_motor3          | 0.46368608     |
|    mean_motor4          | 0.46862394     |
|    mean_motor5          | 0.43054026     |
|    mean_motor6          | 0.42054868     |
|    mean_motor7          | 0.591535       |
| train/                  |                |
|    approx_kl            | 0.03686715     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0881         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.914          |
|    cost_value_loss      | 8.97e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.46          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00127        |
|    mean_cost_advantages | -0.00069177605 |
|    mean_reward_advan... | -0.017044347   |
|    n_updates            | 7080           |
|    nu                   | 11.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00137       |
|    reward_explained_... | 0.843          |
|    reward_value_loss    | 0.0169         |
|    std                  | 0.294          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.64e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.52          |
|    forward_reward       | 0.147         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.23         |
|    reward_forward       | 0.147         |
|    reward_survive       | 1             |
|    x_position           | -0.573        |
|    x_velocity           | 0.147         |
|    y_position           | 5.45          |
|    y_velocity           | 0.606         |
| rollout/                |               |
|    adjusted_reward      | 5.46          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.61e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 356           |
|    time_elapsed         | 4582          |
|    total_timesteps      | 3645440       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10117         |
|    mean_motor0          | 0.4326504     |
|    mean_motor1          | 0.36894795    |
|    mean_motor2          | 0.7348769     |
|    mean_motor3          | 0.4869052     |
|    mean_motor4          | 0.4744564     |
|    mean_motor5          | 0.4238258     |
|    mean_motor6          | 0.44632405    |
|    mean_motor7          | 0.57878786    |
| train/                  |               |
|    approx_kl            | 0.028868208   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.078         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.951         |
|    cost_value_loss      | 3.42e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.43         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00768       |
|    mean_cost_advantages | -0.0010189336 |
|    mean_reward_advan... | -0.0154887885 |
|    n_updates            | 7100          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000981     |
|    reward_explained_... | 0.737         |
|    reward_value_loss    | 0.0162        |
|    std                  | 0.294         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.73e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.16          |
|    forward_reward       | 0.23          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.21         |
|    reward_forward       | 0.23          |
|    reward_survive       | 1             |
|    x_position           | -0.853        |
|    x_velocity           | 0.23          |
|    y_position           | 5.06          |
|    y_velocity           | 0.445         |
| rollout/                |               |
|    adjusted_reward      | 5.37          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.56e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 357           |
|    time_elapsed         | 4596          |
|    total_timesteps      | 3655680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10160         |
|    mean_motor0          | 0.4419756     |
|    mean_motor1          | 0.3792774     |
|    mean_motor2          | 0.7340015     |
|    mean_motor3          | 0.4917644     |
|    mean_motor4          | 0.47775412    |
|    mean_motor5          | 0.43388686    |
|    mean_motor6          | 0.4444589     |
|    mean_motor7          | 0.5792702     |
| train/                  |               |
|    approx_kl            | 0.03435284    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0789        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.876         |
|    cost_value_loss      | 1.18e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.43         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00903       |
|    mean_cost_advantages | -2.302626e-05 |
|    mean_reward_advan... | -0.012511256  |
|    n_updates            | 7120          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00105      |
|    reward_explained_... | 0.475         |
|    reward_value_loss    | 0.0172        |
|    std                  | 0.294         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.72e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.18          |
|    forward_reward       | 0.0932        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.21         |
|    reward_forward       | 0.0932        |
|    reward_survive       | 1             |
|    x_position           | -0.502        |
|    x_velocity           | 0.0932        |
|    y_position           | 5.07          |
|    y_velocity           | 0.247         |
| rollout/                |               |
|    adjusted_reward      | 5.37          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 2.63e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 358           |
|    time_elapsed         | 4609          |
|    total_timesteps      | 3665920       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10130         |
|    mean_motor0          | 0.42087537    |
|    mean_motor1          | 0.35452637    |
|    mean_motor2          | 0.7419567     |
|    mean_motor3          | 0.5066362     |
|    mean_motor4          | 0.49354172    |
|    mean_motor5          | 0.44325393    |
|    mean_motor6          | 0.4366003     |
|    mean_motor7          | 0.58278286    |
| train/                  |               |
|    approx_kl            | 0.04887732    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.109         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.941         |
|    cost_value_loss      | 1.66e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.42         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0073        |
|    mean_cost_advantages | -0.0004941897 |
|    mean_reward_advan... | -0.010348154  |
|    n_updates            | 7140          |
|    nu                   | 11.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00153      |
|    reward_explained_... | 0.681         |
|    reward_value_loss    | 0.0164        |
|    std                  | 0.293         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 413          |
|    mean_reward          | 2.26e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.74         |
|    forward_reward       | 0.163        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.36        |
|    reward_forward       | 0.163        |
|    reward_survive       | 1            |
|    x_position           | -0.988       |
|    x_velocity           | 0.163        |
|    y_position           | 4.63         |
|    y_velocity           | 0.268        |
| rollout/                |              |
|    adjusted_reward      | 5.29         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 2.64e+03     |
| time/                   |              |
|    fps                  | 795          |
|    iterations           | 359          |
|    time_elapsed         | 4622         |
|    total_timesteps      | 3676160      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10083        |
|    mean_motor0          | 0.47801644   |
|    mean_motor1          | 0.36708516   |
|    mean_motor2          | 0.83924496   |
|    mean_motor3          | 0.42449245   |
|    mean_motor4          | 0.42689896   |
|    mean_motor5          | 0.43577796   |
|    mean_motor6          | 0.512665     |
|    mean_motor7          | 0.5410102    |
| train/                  |              |
|    approx_kl            | 1.7421805    |
|    average_cost         | 0.03955078   |
|    clip_fraction        | 0.723        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -4.64        |
|    cost_value_loss      | 0.00345      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.41        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00827      |
|    mean_cost_advantages | 0.063157335  |
|    mean_reward_advan... | -0.006768676 |
|    n_updates            | 7160         |
|    nu                   | 11.9         |
|    nu_loss              | -0.468       |
|    policy_gradient_loss | -0.0341      |
|    reward_explained_... | 0.547        |
|    reward_value_loss    | 0.0146       |
|    std                  | 0.292        |
|    total_cost           | 405.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 1.9e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.73         |
|    forward_reward       | 0.19         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.21        |
|    reward_forward       | 0.19         |
|    reward_survive       | 1            |
|    x_position           | -0.882       |
|    x_velocity           | 0.19         |
|    y_position           | 4.52         |
|    y_velocity           | 0.301        |
| rollout/                |              |
|    adjusted_reward      | 3.82         |
|    ep_len_mean          | 487          |
|    ep_rew_mean          | 2.5e+03      |
| time/                   |              |
|    fps                  | 795          |
|    iterations           | 360          |
|    time_elapsed         | 4636         |
|    total_timesteps      | 3686400      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10133        |
|    mean_motor0          | 0.54819506   |
|    mean_motor1          | 0.36670157   |
|    mean_motor2          | 0.80448806   |
|    mean_motor3          | 0.46963555   |
|    mean_motor4          | 0.44886145   |
|    mean_motor5          | 0.48059663   |
|    mean_motor6          | 0.4717943    |
|    mean_motor7          | 0.5049813    |
| train/                  |              |
|    approx_kl            | 1.9273437    |
|    average_cost         | 0.07783203   |
|    clip_fraction        | 0.605        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.937        |
|    cost_value_loss      | 0.057        |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.4         |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0487      |
|    mean_cost_advantages | 0.08349458   |
|    mean_reward_advan... | -0.020369962 |
|    n_updates            | 7180         |
|    nu                   | 11.9         |
|    nu_loss              | -0.924       |
|    policy_gradient_loss | -0.03        |
|    reward_explained_... | 0.589        |
|    reward_value_loss    | 0.0231       |
|    std                  | 0.292        |
|    total_cost           | 797.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.22e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.95          |
|    forward_reward       | 0.208         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.36         |
|    reward_forward       | 0.208         |
|    reward_survive       | 1             |
|    x_position           | -0.705        |
|    x_velocity           | 0.208         |
|    y_position           | 4.86          |
|    y_velocity           | 0.37          |
| rollout/                |               |
|    adjusted_reward      | 3.99          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 2.35e+03      |
| time/                   |               |
|    fps                  | 795           |
|    iterations           | 361           |
|    time_elapsed         | 4649          |
|    total_timesteps      | 3696640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10131         |
|    mean_motor0          | 0.5205502     |
|    mean_motor1          | 0.35565418    |
|    mean_motor2          | 0.8169616     |
|    mean_motor3          | 0.4676412     |
|    mean_motor4          | 0.43242115    |
|    mean_motor5          | 0.47730955    |
|    mean_motor6          | 0.48077184    |
|    mean_motor7          | 0.51982605    |
| train/                  |               |
|    approx_kl            | 0.044353478   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0871        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.785         |
|    cost_value_loss      | 3.9e-05       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.38         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00734       |
|    mean_cost_advantages | 0.00030167284 |
|    mean_reward_advan... | -0.03902062   |
|    n_updates            | 7200          |
|    nu                   | 12            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00148      |
|    reward_explained_... | 0.628         |
|    reward_value_loss    | 0.0145        |
|    std                  | 0.292         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.14e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 3.7            |
|    forward_reward       | 0.176          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.41          |
|    reward_forward       | 0.176          |
|    reward_survive       | 1              |
|    x_position           | -1.4           |
|    x_velocity           | 0.176          |
|    y_position           | 3.32           |
|    y_velocity           | 0.45           |
| rollout/                |                |
|    adjusted_reward      | 4.33           |
|    ep_len_mean          | 491            |
|    ep_rew_mean          | 2.25e+03       |
| time/                   |                |
|    fps                  | 794            |
|    iterations           | 362            |
|    time_elapsed         | 4662           |
|    total_timesteps      | 3706880        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10167          |
|    mean_motor0          | 0.576069       |
|    mean_motor1          | 0.3611821      |
|    mean_motor2          | 0.8060516      |
|    mean_motor3          | 0.46326256     |
|    mean_motor4          | 0.4424804      |
|    mean_motor5          | 0.48331857     |
|    mean_motor6          | 0.50381166     |
|    mean_motor7          | 0.5015724      |
| train/                  |                |
|    approx_kl            | 0.058245074    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.128          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.746          |
|    cost_value_loss      | 0.00049        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -1.36          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00894        |
|    mean_cost_advantages | -0.00031265905 |
|    mean_reward_advan... | -0.037947565   |
|    n_updates            | 7220           |
|    nu                   | 12.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00201       |
|    reward_explained_... | 0.0978         |
|    reward_value_loss    | 0.0155         |
|    std                  | 0.291          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.32e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.35         |
|    forward_reward       | 0.277        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.27        |
|    reward_forward       | 0.277        |
|    reward_survive       | 1            |
|    x_position           | -1.11        |
|    x_velocity           | 0.277        |
|    y_position           | 4.2          |
|    y_velocity           | 0.557        |
| rollout/                |              |
|    adjusted_reward      | 4.69         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 2.18e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 363          |
|    time_elapsed         | 4676         |
|    total_timesteps      | 3717120      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10084        |
|    mean_motor0          | 0.46549994   |
|    mean_motor1          | 0.35778624   |
|    mean_motor2          | 0.7758509    |
|    mean_motor3          | 0.44641787   |
|    mean_motor4          | 0.42308277   |
|    mean_motor5          | 0.48020944   |
|    mean_motor6          | 0.4904635    |
|    mean_motor7          | 0.5223758    |
| train/                  |              |
|    approx_kl            | 0.36610484   |
|    average_cost         | 0.015136719  |
|    clip_fraction        | 0.414        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.653        |
|    cost_value_loss      | 0.0099       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.36        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0106       |
|    mean_cost_advantages | 0.019330125  |
|    mean_reward_advan... | -0.027561212 |
|    n_updates            | 7240         |
|    nu                   | 12.1         |
|    nu_loss              | -0.183       |
|    policy_gradient_loss | -0.00597     |
|    reward_explained_... | -0.115       |
|    reward_value_loss    | 0.017        |
|    std                  | 0.291        |
|    total_cost           | 155.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.16e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.63         |
|    forward_reward       | 0.122        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.19        |
|    reward_forward       | 0.122        |
|    reward_survive       | 1            |
|    x_position           | -0.961       |
|    x_velocity           | 0.122        |
|    y_position           | 4.52         |
|    y_velocity           | 0.316        |
| rollout/                |              |
|    adjusted_reward      | 4.56         |
|    ep_len_mean          | 498          |
|    ep_rew_mean          | 2.13e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 364          |
|    time_elapsed         | 4689         |
|    total_timesteps      | 3727360      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10101        |
|    mean_motor0          | 0.4293552    |
|    mean_motor1          | 0.36108813   |
|    mean_motor2          | 0.74486345   |
|    mean_motor3          | 0.42808598   |
|    mean_motor4          | 0.43561786   |
|    mean_motor5          | 0.47599763   |
|    mean_motor6          | 0.49296302   |
|    mean_motor7          | 0.5316436    |
| train/                  |              |
|    approx_kl            | 0.09231286   |
|    average_cost         | 0.0048828125 |
|    clip_fraction        | 0.162        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.687        |
|    cost_value_loss      | 0.0145       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.36        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00466     |
|    mean_cost_advantages | 0.005901461  |
|    mean_reward_advan... | -0.015375765 |
|    n_updates            | 7260         |
|    nu                   | 12.2         |
|    nu_loss              | -0.0593      |
|    policy_gradient_loss | -0.00556     |
|    reward_explained_... | -0.309       |
|    reward_value_loss    | 0.0148       |
|    std                  | 0.291        |
|    total_cost           | 50.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.18e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.19          |
|    forward_reward       | 0.143         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.2          |
|    reward_forward       | 0.143         |
|    reward_survive       | 1             |
|    x_position           | -0.709        |
|    x_velocity           | 0.143         |
|    y_position           | 5.13          |
|    y_velocity           | 0.285         |
| rollout/                |               |
|    adjusted_reward      | 4.13          |
|    ep_len_mean          | 498           |
|    ep_rew_mean          | 2.17e+03      |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 365           |
|    time_elapsed         | 4703          |
|    total_timesteps      | 3737600       |
| torque/                 |               |
|    greater_than_0.25    | 10237         |
|    greater_than_0.3     | 10233         |
|    greater_than_0.5     | 10088         |
|    mean_motor0          | 0.4208172     |
|    mean_motor1          | 0.395507      |
|    mean_motor2          | 0.7166619     |
|    mean_motor3          | 0.42567244    |
|    mean_motor4          | 0.41706282    |
|    mean_motor5          | 0.4782052     |
|    mean_motor6          | 0.45328194    |
|    mean_motor7          | 0.54700714    |
| train/                  |               |
|    approx_kl            | 0.043406144   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.105         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.782         |
|    cost_value_loss      | 1.44e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.33         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000498      |
|    mean_cost_advantages | 0.00032470952 |
|    mean_reward_advan... | -0.016593192  |
|    n_updates            | 7280          |
|    nu                   | 12.3          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00166      |
|    reward_explained_... | -0.204        |
|    reward_value_loss    | 0.0136        |
|    std                  | 0.29          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 411          |
|    mean_reward          | 1.78e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.58         |
|    forward_reward       | 0.199        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.25        |
|    reward_forward       | 0.199        |
|    reward_survive       | 1            |
|    x_position           | -0.566       |
|    x_velocity           | 0.199        |
|    y_position           | 4.52         |
|    y_velocity           | 0.542        |
| rollout/                |              |
|    adjusted_reward      | 4.59         |
|    ep_len_mean          | 498          |
|    ep_rew_mean          | 2.23e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 366          |
|    time_elapsed         | 4716         |
|    total_timesteps      | 3747840      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 10105        |
|    mean_motor0          | 0.41765648   |
|    mean_motor1          | 0.3544106    |
|    mean_motor2          | 0.7174391    |
|    mean_motor3          | 0.44893986   |
|    mean_motor4          | 0.4139692    |
|    mean_motor5          | 0.46369806   |
|    mean_motor6          | 0.4961571    |
|    mean_motor7          | 0.5296238    |
| train/                  |              |
|    approx_kl            | 0.043505356  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0961       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.834        |
|    cost_value_loss      | 5.18e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.31        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00825      |
|    mean_cost_advantages | 0.0003201263 |
|    mean_reward_advan... | -0.019290036 |
|    n_updates            | 7300         |
|    nu                   | 12.3         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00163     |
|    reward_explained_... | 0.792        |
|    reward_value_loss    | 0.0125       |
|    std                  | 0.289        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 1.82e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.85          |
|    forward_reward       | 0.148         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.13         |
|    reward_forward       | 0.148         |
|    reward_survive       | 1             |
|    x_position           | -0.828        |
|    x_velocity           | 0.148         |
|    y_position           | 4.75          |
|    y_velocity           | 0.451         |
| rollout/                |               |
|    adjusted_reward      | 4.35          |
|    ep_len_mean          | 498           |
|    ep_rew_mean          | 2.23e+03      |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 367           |
|    time_elapsed         | 4729          |
|    total_timesteps      | 3758080       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10076         |
|    mean_motor0          | 0.4385515     |
|    mean_motor1          | 0.37250882    |
|    mean_motor2          | 0.69210243    |
|    mean_motor3          | 0.44555646    |
|    mean_motor4          | 0.4112502     |
|    mean_motor5          | 0.48907143    |
|    mean_motor6          | 0.46098456    |
|    mean_motor7          | 0.5387454     |
| train/                  |               |
|    approx_kl            | 0.036833294   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.097         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.767         |
|    cost_value_loss      | 4.73e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.28         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00763       |
|    mean_cost_advantages | 0.00026487035 |
|    mean_reward_advan... | -0.012705003  |
|    n_updates            | 7320          |
|    nu                   | 12.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00138      |
|    reward_explained_... | -0.733        |
|    reward_value_loss    | 0.0135        |
|    std                  | 0.288         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.27e+03      |
| infos/                  |               |
|    cost                 | 0.0354        |
|    distance_from_origin | 5.35          |
|    forward_reward       | 0.0553        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.07         |
|    reward_forward       | 0.0553        |
|    reward_survive       | 1             |
|    x_position           | -1.33         |
|    x_velocity           | 0.0553        |
|    y_position           | 5.1           |
|    y_velocity           | 0.168         |
| rollout/                |               |
|    adjusted_reward      | 4.7           |
|    ep_len_mean          | 497           |
|    ep_rew_mean          | 2.21e+03      |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 368           |
|    time_elapsed         | 4742          |
|    total_timesteps      | 3768320       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10234         |
|    greater_than_0.5     | 10039         |
|    mean_motor0          | 0.4290363     |
|    mean_motor1          | 0.35685784    |
|    mean_motor2          | 0.7062056     |
|    mean_motor3          | 0.4425145     |
|    mean_motor4          | 0.40830335    |
|    mean_motor5          | 0.462035      |
|    mean_motor6          | 0.45770067    |
|    mean_motor7          | 0.50531906    |
| train/                  |               |
|    approx_kl            | 0.04622201    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.106         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.531         |
|    cost_value_loss      | 1.94e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.25         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00287      |
|    mean_cost_advantages | 0.00018966474 |
|    mean_reward_advan... | -0.013952857  |
|    n_updates            | 7340          |
|    nu                   | 12.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00163      |
|    reward_explained_... | 0.612         |
|    reward_value_loss    | 0.0119        |
|    std                  | 0.287         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.21e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 3.51         |
|    forward_reward       | 0.16         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.28        |
|    reward_forward       | 0.16         |
|    reward_survive       | 1            |
|    x_position           | -0.782       |
|    x_velocity           | 0.16         |
|    y_position           | 3.37         |
|    y_velocity           | 0.491        |
| rollout/                |              |
|    adjusted_reward      | 4.22         |
|    ep_len_mean          | 498          |
|    ep_rew_mean          | 2.19e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 369          |
|    time_elapsed         | 4756         |
|    total_timesteps      | 3778560      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10103        |
|    mean_motor0          | 0.54703027   |
|    mean_motor1          | 0.3794343    |
|    mean_motor2          | 0.68908066   |
|    mean_motor3          | 0.42312685   |
|    mean_motor4          | 0.43595958   |
|    mean_motor5          | 0.43821245   |
|    mean_motor6          | 0.4479948    |
|    mean_motor7          | 0.4892583    |
| train/                  |              |
|    approx_kl            | 0.7173338    |
|    average_cost         | 0.015429688  |
|    clip_fraction        | 0.491        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -1.99        |
|    cost_value_loss      | 0.00689      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.24        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00893     |
|    mean_cost_advantages | 0.02452556   |
|    mean_reward_advan... | -0.005951008 |
|    n_updates            | 7360         |
|    nu                   | 12.5         |
|    nu_loss              | -0.191       |
|    policy_gradient_loss | -0.00645     |
|    reward_explained_... | -0.449       |
|    reward_value_loss    | 0.0116       |
|    std                  | 0.287        |
|    total_cost           | 158.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.26e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.71         |
|    forward_reward       | 0.14         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.13        |
|    reward_forward       | 0.14         |
|    reward_survive       | 1            |
|    x_position           | -0.957       |
|    x_velocity           | 0.14         |
|    y_position           | 4.53         |
|    y_velocity           | 0.311        |
| rollout/                |              |
|    adjusted_reward      | 4.52         |
|    ep_len_mean          | 498          |
|    ep_rew_mean          | 2.23e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 370          |
|    time_elapsed         | 4769         |
|    total_timesteps      | 3788800      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10092        |
|    mean_motor0          | 0.5263928    |
|    mean_motor1          | 0.35458618   |
|    mean_motor2          | 0.6997298    |
|    mean_motor3          | 0.4411807    |
|    mean_motor4          | 0.4342037    |
|    mean_motor5          | 0.44060984   |
|    mean_motor6          | 0.4515513    |
|    mean_motor7          | 0.4757804    |
| train/                  |              |
|    approx_kl            | 0.050510716  |
|    average_cost         | 0.0029296875 |
|    clip_fraction        | 0.0989       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.861        |
|    cost_value_loss      | 0.00246      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.23        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00557      |
|    mean_cost_advantages | 0.0011282426 |
|    mean_reward_advan... | -0.016681675 |
|    n_updates            | 7380         |
|    nu                   | 12.5         |
|    nu_loss              | -0.0365      |
|    policy_gradient_loss | -0.00161     |
|    reward_explained_... | 0.614        |
|    reward_value_loss    | 0.0129       |
|    std                  | 0.286        |
|    total_cost           | 30.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.26e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.87          |
|    forward_reward       | 0.0621        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.06         |
|    reward_forward       | 0.0621        |
|    reward_survive       | 1             |
|    x_position           | -1.09         |
|    x_velocity           | 0.0621        |
|    y_position           | 4.74          |
|    y_velocity           | 0.16          |
| rollout/                |               |
|    adjusted_reward      | 4.63          |
|    ep_len_mean          | 498           |
|    ep_rew_mean          | 2.23e+03      |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 371           |
|    time_elapsed         | 4783          |
|    total_timesteps      | 3799040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10069         |
|    mean_motor0          | 0.5532655     |
|    mean_motor1          | 0.36070558    |
|    mean_motor2          | 0.6693236     |
|    mean_motor3          | 0.43609852    |
|    mean_motor4          | 0.4296166     |
|    mean_motor5          | 0.43203086    |
|    mean_motor6          | 0.43968028    |
|    mean_motor7          | 0.48075962    |
| train/                  |               |
|    approx_kl            | 0.04234344    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.898         |
|    cost_value_loss      | 1.77e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.19         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00548       |
|    mean_cost_advantages | 0.00092233904 |
|    mean_reward_advan... | -0.0103291    |
|    n_updates            | 7400          |
|    nu                   | 12.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00172      |
|    reward_explained_... | -0.606        |
|    reward_value_loss    | 0.0117        |
|    std                  | 0.284         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.41e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.94         |
|    forward_reward       | 0.163        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.17        |
|    reward_forward       | 0.163        |
|    reward_survive       | 1            |
|    x_position           | -1.34        |
|    x_velocity           | 0.163        |
|    y_position           | 4.74         |
|    y_velocity           | 0.311        |
| rollout/                |              |
|    adjusted_reward      | 4.51         |
|    ep_len_mean          | 498          |
|    ep_rew_mean          | 2.25e+03     |
| time/                   |              |
|    fps                  | 794          |
|    iterations           | 372          |
|    time_elapsed         | 4796         |
|    total_timesteps      | 3809280      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 10075        |
|    mean_motor0          | 0.5469755    |
|    mean_motor1          | 0.3703596    |
|    mean_motor2          | 0.65132743   |
|    mean_motor3          | 0.41489273   |
|    mean_motor4          | 0.43853036   |
|    mean_motor5          | 0.408538     |
|    mean_motor6          | 0.40930694   |
|    mean_motor7          | 0.4793273    |
| train/                  |              |
|    approx_kl            | 0.04103299   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0977       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.734        |
|    cost_value_loss      | 6.38e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -1.15        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00282      |
|    mean_cost_advantages | 0.0010734674 |
|    mean_reward_advan... | -0.00545522  |
|    n_updates            | 7420         |
|    nu                   | 12.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0014      |
|    reward_explained_... | -0.535       |
|    reward_value_loss    | 0.0114       |
|    std                  | 0.283        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.31e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.33          |
|    forward_reward       | 0.277         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.06         |
|    reward_forward       | 0.277         |
|    reward_survive       | 1             |
|    x_position           | -0.865        |
|    x_velocity           | 0.277         |
|    y_position           | 4.22          |
|    y_velocity           | 0.551         |
| rollout/                |               |
|    adjusted_reward      | 4.61          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.25e+03      |
| time/                   |               |
|    fps                  | 794           |
|    iterations           | 373           |
|    time_elapsed         | 4810          |
|    total_timesteps      | 3819520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10074         |
|    mean_motor0          | 0.5827868     |
|    mean_motor1          | 0.36308342    |
|    mean_motor2          | 0.6356199     |
|    mean_motor3          | 0.42630076    |
|    mean_motor4          | 0.44415718    |
|    mean_motor5          | 0.41552368    |
|    mean_motor6          | 0.4112924     |
|    mean_motor7          | 0.46852365    |
| train/                  |               |
|    approx_kl            | 0.0515652     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.115         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.878         |
|    cost_value_loss      | 0.000496      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.12         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00857       |
|    mean_cost_advantages | -8.427379e-05 |
|    mean_reward_advan... | -0.009638498  |
|    n_updates            | 7440          |
|    nu                   | 12.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00164      |
|    reward_explained_... | -1.04         |
|    reward_value_loss    | 0.0124        |
|    std                  | 0.282         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.24e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.29          |
|    forward_reward       | 0.105         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.12         |
|    reward_forward       | 0.105         |
|    reward_survive       | 1             |
|    x_position           | -1.16         |
|    x_velocity           | 0.105         |
|    y_position           | 5.11          |
|    y_velocity           | 0.213         |
| rollout/                |               |
|    adjusted_reward      | 4.55          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.28e+03      |
| time/                   |               |
|    fps                  | 793           |
|    iterations           | 374           |
|    time_elapsed         | 4823          |
|    total_timesteps      | 3829760       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10116         |
|    mean_motor0          | 0.59805816    |
|    mean_motor1          | 0.3771066     |
|    mean_motor2          | 0.6388482     |
|    mean_motor3          | 0.41709572    |
|    mean_motor4          | 0.44093695    |
|    mean_motor5          | 0.43193159    |
|    mean_motor6          | 0.4281711     |
|    mean_motor7          | 0.46743503    |
| train/                  |               |
|    approx_kl            | 0.042381655   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.09          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.862         |
|    cost_value_loss      | 1.61e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.1          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.012         |
|    mean_cost_advantages | 0.0008053033  |
|    mean_reward_advan... | -0.0031266615 |
|    n_updates            | 7460          |
|    nu                   | 12.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00108      |
|    reward_explained_... | -0.0955       |
|    reward_value_loss    | 0.0115        |
|    std                  | 0.281         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.18e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.09          |
|    forward_reward       | 0.0749        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.06         |
|    reward_forward       | 0.0749        |
|    reward_survive       | 1             |
|    x_position           | -1.16         |
|    x_velocity           | 0.0749        |
|    y_position           | 4.95          |
|    y_velocity           | 0.261         |
| rollout/                |               |
|    adjusted_reward      | 4.69          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.3e+03       |
| time/                   |               |
|    fps                  | 793           |
|    iterations           | 375           |
|    time_elapsed         | 4836          |
|    total_timesteps      | 3840000       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10102         |
|    mean_motor0          | 0.5548421     |
|    mean_motor1          | 0.3677618     |
|    mean_motor2          | 0.64182043    |
|    mean_motor3          | 0.43061003    |
|    mean_motor4          | 0.4405167     |
|    mean_motor5          | 0.42123762    |
|    mean_motor6          | 0.43098187    |
|    mean_motor7          | 0.45990497    |
| train/                  |               |
|    approx_kl            | 0.055139422   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.103         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.904         |
|    cost_value_loss      | 7.91e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.08         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0147        |
|    mean_cost_advantages | 0.00088141894 |
|    mean_reward_advan... | -0.0006271555 |
|    n_updates            | 7480          |
|    nu                   | 12.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00117      |
|    reward_explained_... | 0.729         |
|    reward_value_loss    | 0.011         |
|    std                  | 0.281         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.32e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.11          |
|    forward_reward       | 0.127         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.05         |
|    reward_forward       | 0.127         |
|    reward_survive       | 1             |
|    x_position           | -0.77         |
|    x_velocity           | 0.127         |
|    y_position           | 4             |
|    y_velocity           | 0.292         |
| rollout/                |               |
|    adjusted_reward      | 4.5           |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.29e+03      |
| time/                   |               |
|    fps                  | 793           |
|    iterations           | 376           |
|    time_elapsed         | 4850          |
|    total_timesteps      | 3850240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10061         |
|    mean_motor0          | 0.5487673     |
|    mean_motor1          | 0.37158173    |
|    mean_motor2          | 0.637463      |
|    mean_motor3          | 0.44726962    |
|    mean_motor4          | 0.4151991     |
|    mean_motor5          | 0.41816932    |
|    mean_motor6          | 0.4097568     |
|    mean_motor7          | 0.47857803    |
| train/                  |               |
|    approx_kl            | 0.05371783    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.103         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.954         |
|    cost_value_loss      | 7.06e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.06         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00839       |
|    mean_cost_advantages | -0.0003403382 |
|    mean_reward_advan... | -0.0006867235 |
|    n_updates            | 7500          |
|    nu                   | 12.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00116      |
|    reward_explained_... | -0.768        |
|    reward_value_loss    | 0.0114        |
|    std                  | 0.28          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.14e+03      |
| infos/                  |               |
|    cost                 | 0.0355        |
|    distance_from_origin | 5.06          |
|    forward_reward       | 0.168         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.16         |
|    reward_forward       | 0.168         |
|    reward_survive       | 1             |
|    x_position           | -1.67         |
|    x_velocity           | 0.168         |
|    y_position           | 4.74          |
|    y_velocity           | 0.384         |
| rollout/                |               |
|    adjusted_reward      | 4.51          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.29e+03      |
| time/                   |               |
|    fps                  | 793           |
|    iterations           | 377           |
|    time_elapsed         | 4863          |
|    total_timesteps      | 3860480       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10234         |
|    greater_than_0.5     | 10087         |
|    mean_motor0          | 0.5597401     |
|    mean_motor1          | 0.38199538    |
|    mean_motor2          | 0.6271664     |
|    mean_motor3          | 0.4406897     |
|    mean_motor4          | 0.42426115    |
|    mean_motor5          | 0.43206373    |
|    mean_motor6          | 0.42168418    |
|    mean_motor7          | 0.47650608    |
| train/                  |               |
|    approx_kl            | 0.04453998    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0914        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.921         |
|    cost_value_loss      | 2.42e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -1.03         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0104        |
|    mean_cost_advantages | 0.0003658982  |
|    mean_reward_advan... | -0.0057103643 |
|    n_updates            | 7520          |
|    nu                   | 12.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00109      |
|    reward_explained_... | -0.731        |
|    reward_value_loss    | 0.0118        |
|    std                  | 0.279         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.06e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 4.44        |
|    forward_reward       | 0.0837      |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.22       |
|    reward_forward       | 0.0837      |
|    reward_survive       | 1           |
|    x_position           | -0.803      |
|    x_velocity           | 0.0837      |
|    y_position           | 4.35        |
|    y_velocity           | 0.153       |
| rollout/                |             |
|    adjusted_reward      | 4.11        |
|    ep_len_mean          | 500         |
|    ep_rew_mean          | 2.26e+03    |
| time/                   |             |
|    fps                  | 793         |
|    iterations           | 378         |
|    time_elapsed         | 4877        |
|    total_timesteps      | 3870720     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10140       |
|    mean_motor0          | 0.5343766   |
|    mean_motor1          | 0.3760542   |
|    mean_motor2          | 0.6200738   |
|    mean_motor3          | 0.4011317   |
|    mean_motor4          | 0.46394572  |
|    mean_motor5          | 0.49787456  |
|    mean_motor6          | 0.46636963  |
|    mean_motor7          | 0.55239236  |
| train/                  |             |
|    approx_kl            | 1.4766135   |
|    average_cost         | 0.04638672  |
|    clip_fraction        | 0.707       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.0196      |
|    cost_value_loss      | 0.0163      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -1.01       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0216     |
|    mean_cost_advantages | 0.07099629  |
|    mean_reward_advan... | 0.003088059 |
|    n_updates            | 7540        |
|    nu                   | 12.8        |
|    nu_loss              | -0.59       |
|    policy_gradient_loss | -0.0291     |
|    reward_explained_... | 0.674       |
|    reward_value_loss    | 0.0106      |
|    std                  | 0.278       |
|    total_cost           | 475.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 1.76e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 3.84         |
|    forward_reward       | 0.112        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.23        |
|    reward_forward       | 0.112        |
|    reward_survive       | 1            |
|    x_position           | -0.65        |
|    x_velocity           | 0.112        |
|    y_position           | 3.77         |
|    y_velocity           | 0.235        |
| rollout/                |              |
|    adjusted_reward      | 3.96         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 2.19e+03     |
| time/                   |              |
|    fps                  | 793          |
|    iterations           | 379          |
|    time_elapsed         | 4891         |
|    total_timesteps      | 3880960      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10144        |
|    mean_motor0          | 0.52124065   |
|    mean_motor1          | 0.3805488    |
|    mean_motor2          | 0.6134899    |
|    mean_motor3          | 0.4041976    |
|    mean_motor4          | 0.50061744   |
|    mean_motor5          | 0.49067202   |
|    mean_motor6          | 0.48257285   |
|    mean_motor7          | 0.5422352    |
| train/                  |              |
|    approx_kl            | 0.063433126  |
|    average_cost         | 0.0072265626 |
|    clip_fraction        | 0.138        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.829        |
|    cost_value_loss      | 0.00606      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.993       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0015       |
|    mean_cost_advantages | 0.0054145446 |
|    mean_reward_advan... | -0.013586005 |
|    n_updates            | 7560         |
|    nu                   | 12.8         |
|    nu_loss              | -0.0922      |
|    policy_gradient_loss | -0.00228     |
|    reward_explained_... | -0.645       |
|    reward_value_loss    | 0.0111       |
|    std                  | 0.278        |
|    total_cost           | 74.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 1.96e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 3.77           |
|    forward_reward       | 0.238          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.21          |
|    reward_forward       | 0.238          |
|    reward_survive       | 1              |
|    x_position           | -0.712         |
|    x_velocity           | 0.238          |
|    y_position           | 3.69           |
|    y_velocity           | 0.562          |
| rollout/                |                |
|    adjusted_reward      | 3.78           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.1e+03        |
| time/                   |                |
|    fps                  | 793            |
|    iterations           | 380            |
|    time_elapsed         | 4904           |
|    total_timesteps      | 3891200        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10141          |
|    mean_motor0          | 0.5197027      |
|    mean_motor1          | 0.40234637     |
|    mean_motor2          | 0.60877764     |
|    mean_motor3          | 0.40092006     |
|    mean_motor4          | 0.5025462      |
|    mean_motor5          | 0.48622924     |
|    mean_motor6          | 0.49471587     |
|    mean_motor7          | 0.54418445     |
| train/                  |                |
|    approx_kl            | 0.042256147    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0889         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.787          |
|    cost_value_loss      | 6.39e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.985         |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00108        |
|    mean_cost_advantages | -0.00028681158 |
|    mean_reward_advan... | -0.014916709   |
|    n_updates            | 7580           |
|    nu                   | 12.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00111       |
|    reward_explained_... | 0.526          |
|    reward_value_loss    | 0.0113         |
|    std                  | 0.278          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.1e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.51           |
|    forward_reward       | 0.151          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.26          |
|    reward_forward       | 0.151          |
|    reward_survive       | 1              |
|    x_position           | -0.889         |
|    x_velocity           | 0.151          |
|    y_position           | 4.36           |
|    y_velocity           | 0.276          |
| rollout/                |                |
|    adjusted_reward      | 3.94           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.05e+03       |
| time/                   |                |
|    fps                  | 793            |
|    iterations           | 381            |
|    time_elapsed         | 4918           |
|    total_timesteps      | 3901440        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10141          |
|    mean_motor0          | 0.57543796     |
|    mean_motor1          | 0.38722152     |
|    mean_motor2          | 0.5963117      |
|    mean_motor3          | 0.39752787     |
|    mean_motor4          | 0.46758452     |
|    mean_motor5          | 0.5017781      |
|    mean_motor6          | 0.47573575     |
|    mean_motor7          | 0.54564965     |
| train/                  |                |
|    approx_kl            | 0.040352084    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0937         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.925          |
|    cost_value_loss      | 8.43e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.973         |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00354        |
|    mean_cost_advantages | -0.00014104936 |
|    mean_reward_advan... | -0.014352754   |
|    n_updates            | 7600           |
|    nu                   | 12.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00122       |
|    reward_explained_... | 0.643          |
|    reward_value_loss    | 0.0105         |
|    std                  | 0.277          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.01e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.99          |
|    forward_reward       | 0.0934        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.18         |
|    reward_forward       | 0.0934        |
|    reward_survive       | 1             |
|    x_position           | -1.11         |
|    x_velocity           | 0.0934        |
|    y_position           | 3.83          |
|    y_velocity           | 0.179         |
| rollout/                |               |
|    adjusted_reward      | 4.07          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 1.99e+03      |
| time/                   |               |
|    fps                  | 793           |
|    iterations           | 382           |
|    time_elapsed         | 4931          |
|    total_timesteps      | 3911680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10135         |
|    mean_motor0          | 0.5609618     |
|    mean_motor1          | 0.37623164    |
|    mean_motor2          | 0.60655296    |
|    mean_motor3          | 0.38561112    |
|    mean_motor4          | 0.48551655    |
|    mean_motor5          | 0.4808572     |
|    mean_motor6          | 0.5009248     |
|    mean_motor7          | 0.51197267    |
| train/                  |               |
|    approx_kl            | 0.0382321     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0767        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.913         |
|    cost_value_loss      | 3.13e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.952        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00349       |
|    mean_cost_advantages | -0.0003154219 |
|    mean_reward_advan... | -0.010624861  |
|    n_updates            | 7620          |
|    nu                   | 12.9          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00129      |
|    reward_explained_... | 0.484         |
|    reward_value_loss    | 0.0101        |
|    std                  | 0.276         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.32e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 3.34           |
|    forward_reward       | 0.185          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.27          |
|    reward_forward       | 0.185          |
|    reward_survive       | 1              |
|    x_position           | -0.958         |
|    x_velocity           | 0.185          |
|    y_position           | 3.17           |
|    y_velocity           | 0.294          |
| rollout/                |                |
|    adjusted_reward      | 3.97           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 1.96e+03       |
| time/                   |                |
|    fps                  | 793            |
|    iterations           | 383            |
|    time_elapsed         | 4945           |
|    total_timesteps      | 3921920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10127          |
|    mean_motor0          | 0.54528964     |
|    mean_motor1          | 0.38477564     |
|    mean_motor2          | 0.5829092      |
|    mean_motor3          | 0.3687759      |
|    mean_motor4          | 0.48221764     |
|    mean_motor5          | 0.48433295     |
|    mean_motor6          | 0.480442       |
|    mean_motor7          | 0.5126751      |
| train/                  |                |
|    approx_kl            | 0.03690378     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0975         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.844          |
|    cost_value_loss      | 6.44e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.926         |
|    learning_rate        | 3e-05          |
|    loss                 | -5.33e-05      |
|    mean_cost_advantages | -0.00011667472 |
|    mean_reward_advan... | -0.0061339615  |
|    n_updates            | 7640           |
|    nu                   | 13             |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00109       |
|    reward_explained_... | 0.365          |
|    reward_value_loss    | 0.00997        |
|    std                  | 0.275          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.01e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.02          |
|    forward_reward       | 0.203         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.22         |
|    reward_forward       | 0.203         |
|    reward_survive       | 1             |
|    x_position           | -0.69         |
|    x_velocity           | 0.203         |
|    y_position           | 2.88          |
|    y_velocity           | 0.454         |
| rollout/                |               |
|    adjusted_reward      | 3.77          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 1.95e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 384           |
|    time_elapsed         | 4958          |
|    total_timesteps      | 3932160       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10124         |
|    mean_motor0          | 0.54561216    |
|    mean_motor1          | 0.39679262    |
|    mean_motor2          | 0.6253305     |
|    mean_motor3          | 0.36786398    |
|    mean_motor4          | 0.51356536    |
|    mean_motor5          | 0.45077062    |
|    mean_motor6          | 0.46948785    |
|    mean_motor7          | 0.50347483    |
| train/                  |               |
|    approx_kl            | 0.034787793   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0831        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.861         |
|    cost_value_loss      | 1.82e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.902        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00181       |
|    mean_cost_advantages | -0.0002914093 |
|    mean_reward_advan... | -0.007923676  |
|    n_updates            | 7660          |
|    nu                   | 13            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.001        |
|    reward_explained_... | 0.467         |
|    reward_value_loss    | 0.00973       |
|    std                  | 0.274         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.07e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.18           |
|    forward_reward       | 0.0798         |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.24          |
|    reward_forward       | 0.0798         |
|    reward_survive       | 1              |
|    x_position           | -1.04          |
|    x_velocity           | 0.0798         |
|    y_position           | 3.98           |
|    y_velocity           | 0.134          |
| rollout/                |                |
|    adjusted_reward      | 4.16           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 1.97e+03       |
| time/                   |                |
|    fps                  | 792            |
|    iterations           | 385            |
|    time_elapsed         | 4972           |
|    total_timesteps      | 3942400        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10142          |
|    mean_motor0          | 0.50535995     |
|    mean_motor1          | 0.3690529      |
|    mean_motor2          | 0.62134075     |
|    mean_motor3          | 0.3584667      |
|    mean_motor4          | 0.5225071      |
|    mean_motor5          | 0.45757776     |
|    mean_motor6          | 0.5073295      |
|    mean_motor7          | 0.50084126     |
| train/                  |                |
|    approx_kl            | 0.04113114     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0979         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.841          |
|    cost_value_loss      | 9.17e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.874         |
|    learning_rate        | 3e-05          |
|    loss                 | -0.001         |
|    mean_cost_advantages | -0.00019204854 |
|    mean_reward_advan... | -0.009476354   |
|    n_updates            | 7680           |
|    nu                   | 13             |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00129       |
|    reward_explained_... | 0.69           |
|    reward_value_loss    | 0.0092         |
|    std                  | 0.273          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.07e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.46          |
|    forward_reward       | 0.0753        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.13         |
|    reward_forward       | 0.0753        |
|    reward_survive       | 1             |
|    x_position           | -1.09         |
|    x_velocity           | 0.0753        |
|    y_position           | 4.3           |
|    y_velocity           | 0.189         |
| rollout/                |               |
|    adjusted_reward      | 3.98          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 1.98e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 386           |
|    time_elapsed         | 4985          |
|    total_timesteps      | 3952640       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10116         |
|    mean_motor0          | 0.51086867    |
|    mean_motor1          | 0.39375165    |
|    mean_motor2          | 0.59161365    |
|    mean_motor3          | 0.37108794    |
|    mean_motor4          | 0.49248916    |
|    mean_motor5          | 0.46957612    |
|    mean_motor6          | 0.50286925    |
|    mean_motor7          | 0.5339654     |
| train/                  |               |
|    approx_kl            | 0.04448893    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.118         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.917         |
|    cost_value_loss      | 1.78e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.859        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0037        |
|    mean_cost_advantages | -0.0002160118 |
|    mean_reward_advan... | -0.0026580358 |
|    n_updates            | 7700          |
|    nu                   | 13            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00125      |
|    reward_explained_... | -0.374        |
|    reward_value_loss    | 0.00955       |
|    std                  | 0.273         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.13e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.37           |
|    forward_reward       | 0.114          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.15          |
|    reward_forward       | 0.114          |
|    reward_survive       | 1              |
|    x_position           | -1.31          |
|    x_velocity           | 0.114          |
|    y_position           | 4.16           |
|    y_velocity           | 0.279          |
| rollout/                |                |
|    adjusted_reward      | 4.21           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.01e+03       |
| time/                   |                |
|    fps                  | 792            |
|    iterations           | 387            |
|    time_elapsed         | 4999           |
|    total_timesteps      | 3962880        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10093          |
|    mean_motor0          | 0.50618637     |
|    mean_motor1          | 0.37880686     |
|    mean_motor2          | 0.5894681      |
|    mean_motor3          | 0.37741178     |
|    mean_motor4          | 0.4842947      |
|    mean_motor5          | 0.49194527     |
|    mean_motor6          | 0.44865528     |
|    mean_motor7          | 0.51256263     |
| train/                  |                |
|    approx_kl            | 0.0419612      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0831         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.912          |
|    cost_value_loss      | 2.46e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.843         |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00276        |
|    mean_cost_advantages | -2.6297017e-05 |
|    mean_reward_advan... | -0.0038608634  |
|    n_updates            | 7720           |
|    nu                   | 13.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000909      |
|    reward_explained_... | 0.493          |
|    reward_value_loss    | 0.00923        |
|    std                  | 0.272          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 1.74e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.49          |
|    forward_reward       | 0.104         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.22         |
|    reward_forward       | 0.104         |
|    reward_survive       | 1             |
|    x_position           | -0.765        |
|    x_velocity           | 0.104         |
|    y_position           | 3.34          |
|    y_velocity           | 0.123         |
| rollout/                |               |
|    adjusted_reward      | 4.12          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.01e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 388           |
|    time_elapsed         | 5012          |
|    total_timesteps      | 3973120       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10086         |
|    mean_motor0          | 0.53927237    |
|    mean_motor1          | 0.37190804    |
|    mean_motor2          | 0.6127521     |
|    mean_motor3          | 0.35894164    |
|    mean_motor4          | 0.51904607    |
|    mean_motor5          | 0.5021829     |
|    mean_motor6          | 0.4705449     |
|    mean_motor7          | 0.46744165    |
| train/                  |               |
|    approx_kl            | 0.041554116   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.106         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.812         |
|    cost_value_loss      | 1.22e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.823        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0055        |
|    mean_cost_advantages | 2.7562667e-06 |
|    mean_reward_advan... | 0.0020474482  |
|    n_updates            | 7740          |
|    nu                   | 13.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00115      |
|    reward_explained_... | 0.328         |
|    reward_value_loss    | 0.00888       |
|    std                  | 0.272         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.19e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.35          |
|    forward_reward       | 0.0639        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.15         |
|    reward_forward       | 0.0639        |
|    reward_survive       | 1             |
|    x_position           | -1.11         |
|    x_velocity           | 0.0639        |
|    y_position           | 4.05          |
|    y_velocity           | 0.0934        |
| rollout/                |               |
|    adjusted_reward      | 4.17          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.05e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 389           |
|    time_elapsed         | 5026          |
|    total_timesteps      | 3983360       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10115         |
|    mean_motor0          | 0.5720628     |
|    mean_motor1          | 0.36428303    |
|    mean_motor2          | 0.5913794     |
|    mean_motor3          | 0.35391125    |
|    mean_motor4          | 0.50056064    |
|    mean_motor5          | 0.5350192     |
|    mean_motor6          | 0.44504538    |
|    mean_motor7          | 0.47059622    |
| train/                  |               |
|    approx_kl            | 0.053125422   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.11          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.889         |
|    cost_value_loss      | 0.000604      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.8          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00122       |
|    mean_cost_advantages | -0.0015193324 |
|    mean_reward_advan... | 0.0005958054  |
|    n_updates            | 7760          |
|    nu                   | 13.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00132      |
|    reward_explained_... | 0.785         |
|    reward_value_loss    | 0.00934       |
|    std                  | 0.271         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.28e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 3.12           |
|    forward_reward       | 0.345          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.2           |
|    reward_forward       | 0.345          |
|    reward_survive       | 1              |
|    x_position           | -0.498         |
|    x_velocity           | 0.345          |
|    y_position           | 3.04           |
|    y_velocity           | 0.622          |
| rollout/                |                |
|    adjusted_reward      | 4.29           |
|    ep_len_mean          | 495            |
|    ep_rew_mean          | 2.06e+03       |
| time/                   |                |
|    fps                  | 792            |
|    iterations           | 390            |
|    time_elapsed         | 5040           |
|    total_timesteps      | 3993600        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10119          |
|    mean_motor0          | 0.51264757     |
|    mean_motor1          | 0.3720632      |
|    mean_motor2          | 0.5950695      |
|    mean_motor3          | 0.35578027     |
|    mean_motor4          | 0.54884225     |
|    mean_motor5          | 0.5055677      |
|    mean_motor6          | 0.4370556      |
|    mean_motor7          | 0.4833654      |
| train/                  |                |
|    approx_kl            | 0.049246542    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.125          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.815          |
|    cost_value_loss      | 1.03e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.783         |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00128        |
|    mean_cost_advantages | -0.00020414982 |
|    mean_reward_advan... | 0.004490399    |
|    n_updates            | 7780           |
|    nu                   | 13.1           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00151       |
|    reward_explained_... | 0.733          |
|    reward_value_loss    | 0.00925        |
|    std                  | 0.271          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.13e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.9          |
|    forward_reward       | 0.21         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.1         |
|    reward_forward       | 0.21         |
|    reward_survive       | 1            |
|    x_position           | -1.3         |
|    x_velocity           | 0.21         |
|    y_position           | 4.65         |
|    y_velocity           | 0.51         |
| rollout/                |              |
|    adjusted_reward      | 4.23         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.08e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 391          |
|    time_elapsed         | 5053         |
|    total_timesteps      | 4003840      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10114        |
|    mean_motor0          | 0.49712768   |
|    mean_motor1          | 0.36215037   |
|    mean_motor2          | 0.59778476   |
|    mean_motor3          | 0.36760104   |
|    mean_motor4          | 0.5628586    |
|    mean_motor5          | 0.48151168   |
|    mean_motor6          | 0.47637558   |
|    mean_motor7          | 0.47128525   |
| train/                  |              |
|    approx_kl            | 0.051535457  |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.114        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.639        |
|    cost_value_loss      | 0.000291     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.766       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00186     |
|    mean_cost_advantages | 0.0012773258 |
|    mean_reward_advan... | 0.0048779445 |
|    n_updates            | 7800         |
|    nu                   | 13.1         |
|    nu_loss              | -0.0167      |
|    policy_gradient_loss | -0.00196     |
|    reward_explained_... | 0.513        |
|    reward_value_loss    | 0.00941      |
|    std                  | 0.27         |
|    total_cost           | 13.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.36e+03     |
| infos/                  |              |
|    cost                 | 0.0713       |
|    distance_from_origin | 5.58         |
|    forward_reward       | 0.105        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.22        |
|    reward_forward       | 0.105        |
|    reward_survive       | 1            |
|    x_position           | -2.07        |
|    x_velocity           | 0.105        |
|    y_position           | 5.11         |
|    y_velocity           | 0.165        |
| rollout/                |              |
|    adjusted_reward      | 4.2          |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.09e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 392          |
|    time_elapsed         | 5067         |
|    total_timesteps      | 4014080      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10125        |
|    mean_motor0          | 0.49084067   |
|    mean_motor1          | 0.37201896   |
|    mean_motor2          | 0.62604684   |
|    mean_motor3          | 0.3676209    |
|    mean_motor4          | 0.5612472    |
|    mean_motor5          | 0.5016223    |
|    mean_motor6          | 0.49547553   |
|    mean_motor7          | 0.45063528   |
| train/                  |              |
|    approx_kl            | 0.091432616  |
|    average_cost         | 0.006152344  |
|    clip_fraction        | 0.178        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -0.325       |
|    cost_value_loss      | 0.00562      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.752       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0072      |
|    mean_cost_advantages | 0.007794944  |
|    mean_reward_advan... | 0.0041079754 |
|    n_updates            | 7820         |
|    nu                   | 13.2         |
|    nu_loss              | -0.0808      |
|    policy_gradient_loss | -0.00191     |
|    reward_explained_... | 0.71         |
|    reward_value_loss    | 0.00936      |
|    std                  | 0.27         |
|    total_cost           | 63.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.15e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.38         |
|    forward_reward       | 0.13         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.34        |
|    reward_forward       | 0.13         |
|    reward_survive       | 1            |
|    x_position           | -1.32        |
|    x_velocity           | 0.13         |
|    y_position           | 4.03         |
|    y_velocity           | 0.208        |
| rollout/                |              |
|    adjusted_reward      | 4.37         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 2.17e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 393          |
|    time_elapsed         | 5080         |
|    total_timesteps      | 4024320      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10141        |
|    mean_motor0          | 0.66208017   |
|    mean_motor1          | 0.3383975    |
|    mean_motor2          | 0.7145153    |
|    mean_motor3          | 0.32856935   |
|    mean_motor4          | 0.58410203   |
|    mean_motor5          | 0.5001197    |
|    mean_motor6          | 0.4650566    |
|    mean_motor7          | 0.38528344   |
| train/                  |              |
|    approx_kl            | 2.56955      |
|    average_cost         | 0.07373047   |
|    clip_fraction        | 0.719        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.708        |
|    cost_value_loss      | 0.0328       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.744       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0622      |
|    mean_cost_advantages | 0.109184906  |
|    mean_reward_advan... | 0.0067122234 |
|    n_updates            | 7840         |
|    nu                   | 13.2         |
|    nu_loss              | -0.97        |
|    policy_gradient_loss | -0.0415      |
|    reward_explained_... | 0.716        |
|    reward_value_loss    | 0.00962      |
|    std                  | 0.269        |
|    total_cost           | 755.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 340          |
|    mean_reward          | 1.49e+03     |
| infos/                  |              |
|    cost                 | 0.0344       |
|    distance_from_origin | 4.27         |
|    forward_reward       | 0.367        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.39        |
|    reward_forward       | 0.367        |
|    reward_survive       | 1            |
|    x_position           | -1.18        |
|    x_velocity           | 0.367        |
|    y_position           | 3.9          |
|    y_velocity           | 0.324        |
| rollout/                |              |
|    adjusted_reward      | 4            |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 2.18e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 394          |
|    time_elapsed         | 5093         |
|    total_timesteps      | 4034560      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10162        |
|    mean_motor0          | 0.55612147   |
|    mean_motor1          | 0.36769187   |
|    mean_motor2          | 0.74482703   |
|    mean_motor3          | 0.32038048   |
|    mean_motor4          | 0.5680406    |
|    mean_motor5          | 0.601581     |
|    mean_motor6          | 0.56456804   |
|    mean_motor7          | 0.45473114   |
| train/                  |              |
|    approx_kl            | 0.40704185   |
|    average_cost         | 0.031933594  |
|    clip_fraction        | 0.391        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.923        |
|    cost_value_loss      | 0.0437       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.722       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0107       |
|    mean_cost_advantages | 0.025135826  |
|    mean_reward_advan... | 0.0074638985 |
|    n_updates            | 7860         |
|    nu                   | 13.3         |
|    nu_loss              | -0.422       |
|    policy_gradient_loss | -0.0106      |
|    reward_explained_... | 0.421        |
|    reward_value_loss    | 0.0102       |
|    std                  | 0.269        |
|    total_cost           | 327.0        |
------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 365         |
|    mean_reward          | 1.74e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.37        |
|    forward_reward       | 0.478       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.29       |
|    reward_forward       | 0.478       |
|    reward_survive       | 1           |
|    x_position           | -1.36       |
|    x_velocity           | 0.478       |
|    y_position           | 5.14        |
|    y_velocity           | 0.512       |
| rollout/                |             |
|    adjusted_reward      | 5.29        |
|    ep_len_mean          | 476         |
|    ep_rew_mean          | 2.21e+03    |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 395         |
|    time_elapsed         | 5105        |
|    total_timesteps      | 4044800     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10239       |
|    greater_than_0.5     | 10094       |
|    mean_motor0          | 0.73817456  |
|    mean_motor1          | 0.3647427   |
|    mean_motor2          | 0.7004355   |
|    mean_motor3          | 0.31101832  |
|    mean_motor4          | 0.48433918  |
|    mean_motor5          | 0.5863999   |
|    mean_motor6          | 0.48524374  |
|    mean_motor7          | 0.41111845  |
| train/                  |             |
|    approx_kl            | 2.1964443   |
|    average_cost         | 0.17451172  |
|    clip_fraction        | 0.554       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.913       |
|    cost_value_loss      | 0.244       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.716      |
|    learning_rate        | 3e-05       |
|    loss                 | -0.299      |
|    mean_cost_advantages | 0.18677182  |
|    mean_reward_advan... | 0.015156627 |
|    n_updates            | 7880        |
|    nu                   | 13.5        |
|    nu_loss              | -2.32       |
|    policy_gradient_loss | -0.072      |
|    reward_explained_... | 0.845       |
|    reward_value_loss    | 0.00947     |
|    std                  | 0.268       |
|    total_cost           | 1787.0      |
-----------------------------------------
(8,)
----------------------------------------
| eval/                   |            |
|    best_mean_reward     | 5e+03      |
|    mean_ep_length       | 292        |
|    mean_reward          | 776        |
| infos/                  |            |
|    cost                 | 0          |
|    distance_from_origin | 4.95       |
|    forward_reward       | 0.514      |
|    reward_contact       | 0          |
|    reward_ctrl          | -1.26      |
|    reward_forward       | 0.514      |
|    reward_survive       | 1          |
|    x_position           | -1.65      |
|    x_velocity           | 0.514      |
|    y_position           | 4.61       |
|    y_velocity           | 0.681      |
| rollout/                |            |
|    adjusted_reward      | 4.46       |
|    ep_len_mean          | 459        |
|    ep_rew_mean          | 2.17e+03   |
| time/                   |            |
|    fps                  | 792        |
|    iterations           | 396        |
|    time_elapsed         | 5118       |
|    total_timesteps      | 4055040    |
| torque/                 |            |
|    greater_than_0.25    | 10240      |
|    greater_than_0.3     | 10236      |
|    greater_than_0.5     | 10141      |
|    mean_motor0          | 1.0641046  |
|    mean_motor1          | 0.33250752 |
|    mean_motor2          | 0.66750306 |
|    mean_motor3          | 0.3025183  |
|    mean_motor4          | 0.51969504 |
|    mean_motor5          | 0.5537495  |
|    mean_motor6          | 0.50981987 |
|    mean_motor7          | 0.47085086 |
| train/                  |            |
|    approx_kl            | 1.3148032  |
|    average_cost         | 0.1772461  |
|    clip_fraction        | 0.534      |
|    clip_range           | 0.4        |
|    cost_explained_va... | 0.936      |
|    cost_value_loss      | 0.293      |
|    early_stop_epoch     | 20         |
|    entropy_loss         | -0.7       |
|    learning_rate        | 3e-05      |
|    loss                 | -0.0266    |
|    mean_cost_advantages | 0.14386973 |
|    mean_reward_advan... | 0.03680728 |
|    n_updates            | 7900       |
|    nu                   | 13.7       |
|    nu_loss              | -2.38      |
|    policy_gradient_loss | -0.0798    |
|    reward_explained_... | 0.482      |
|    reward_value_loss    | 0.0113     |
|    std                  | 0.268      |
|    total_cost           | 1815.0     |
----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 352         |
|    mean_reward          | 1.59e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.09        |
|    forward_reward       | 0.18        |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.54       |
|    reward_forward       | 0.18        |
|    reward_survive       | 1           |
|    x_position           | -0.92       |
|    x_velocity           | 0.18        |
|    y_position           | 4.84        |
|    y_velocity           | 0.199       |
| rollout/                |             |
|    adjusted_reward      | 4.78        |
|    ep_len_mean          | 441         |
|    ep_rew_mean          | 2.13e+03    |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 397         |
|    time_elapsed         | 5130        |
|    total_timesteps      | 4065280     |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10152       |
|    mean_motor0          | 0.9239415   |
|    mean_motor1          | 0.351803    |
|    mean_motor2          | 0.6676467   |
|    mean_motor3          | 0.31649238  |
|    mean_motor4          | 0.537496    |
|    mean_motor5          | 0.58786535  |
|    mean_motor6          | 0.5292632   |
|    mean_motor7          | 0.46484423  |
| train/                  |             |
|    approx_kl            | 0.51453793  |
|    average_cost         | 0.06289063  |
|    clip_fraction        | 0.381       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.958       |
|    cost_value_loss      | 0.119       |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.678      |
|    learning_rate        | 3e-05       |
|    loss                 | 0.0156      |
|    mean_cost_advantages | 0.040029727 |
|    mean_reward_advan... | 0.013813508 |
|    n_updates            | 7920        |
|    nu                   | 13.9        |
|    nu_loss              | -0.859      |
|    policy_gradient_loss | -0.0398     |
|    reward_explained_... | 0.722       |
|    reward_value_loss    | 0.00943     |
|    std                  | 0.267       |
|    total_cost           | 644.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 432         |
|    mean_reward          | 1.74e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 2.95        |
|    forward_reward       | 0.351       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.55       |
|    reward_forward       | 0.351       |
|    reward_survive       | 1           |
|    x_position           | -0.403      |
|    x_velocity           | 0.351       |
|    y_position           | 2.9         |
|    y_velocity           | 0.635       |
| rollout/                |             |
|    adjusted_reward      | 4.61        |
|    ep_len_mean          | 437         |
|    ep_rew_mean          | 2.16e+03    |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 398         |
|    time_elapsed         | 5143        |
|    total_timesteps      | 4075520     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10172       |
|    mean_motor0          | 0.77939665  |
|    mean_motor1          | 0.36670724  |
|    mean_motor2          | 0.7374747   |
|    mean_motor3          | 0.35365883  |
|    mean_motor4          | 0.5718337   |
|    mean_motor5          | 0.51379186  |
|    mean_motor6          | 0.5910564   |
|    mean_motor7          | 0.4764238   |
| train/                  |             |
|    approx_kl            | 0.3517484   |
|    average_cost         | 0.024609376 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.86        |
|    cost_value_loss      | 0.0778      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.664      |
|    learning_rate        | 3e-05       |
|    loss                 | 0.00335     |
|    mean_cost_advantages | 0.010685312 |
|    mean_reward_advan... | 0.024672255 |
|    n_updates            | 7940        |
|    nu                   | 14.1        |
|    nu_loss              | -0.341      |
|    policy_gradient_loss | -0.0223     |
|    reward_explained_... | 0.872       |
|    reward_value_loss    | 0.00884     |
|    std                  | 0.267       |
|    total_cost           | 252.0       |
-----------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 456         |
|    mean_reward          | 1.94e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.09        |
|    forward_reward       | 0.184       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.1        |
|    reward_forward       | 0.184       |
|    reward_survive       | 1           |
|    x_position           | -0.274      |
|    x_velocity           | 0.184       |
|    y_position           | 5.04        |
|    y_velocity           | 0.373       |
| rollout/                |             |
|    adjusted_reward      | 4.41        |
|    ep_len_mean          | 445         |
|    ep_rew_mean          | 2.14e+03    |
| time/                   |             |
|    fps                  | 792         |
|    iterations           | 399         |
|    time_elapsed         | 5157        |
|    total_timesteps      | 4085760     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10142       |
|    mean_motor0          | 0.65358824  |
|    mean_motor1          | 0.35817438  |
|    mean_motor2          | 0.78766584  |
|    mean_motor3          | 0.31224555  |
|    mean_motor4          | 0.5876851   |
|    mean_motor5          | 0.49233764  |
|    mean_motor6          | 0.61781126  |
|    mean_motor7          | 0.4606917   |
| train/                  |             |
|    approx_kl            | 0.50924426  |
|    average_cost         | 0.04082031  |
|    clip_fraction        | 0.391       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.905       |
|    cost_value_loss      | 0.0949      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.648      |
|    learning_rate        | 3e-05       |
|    loss                 | -0.112      |
|    mean_cost_advantages | 0.027614012 |
|    mean_reward_advan... | 0.024072358 |
|    n_updates            | 7960        |
|    nu                   | 14.3        |
|    nu_loss              | -0.575      |
|    policy_gradient_loss | -0.0308     |
|    reward_explained_... | 0.839       |
|    reward_value_loss    | 0.00884     |
|    std                  | 0.266       |
|    total_cost           | 418.0       |
-----------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.1e+03       |
| infos/                  |               |
|    cost                 | 0.0331        |
|    distance_from_origin | 5.08          |
|    forward_reward       | 0.28          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.16         |
|    reward_forward       | 0.28          |
|    reward_survive       | 1             |
|    x_position           | -0.1          |
|    x_velocity           | 0.28          |
|    y_position           | 4.8           |
|    y_velocity           | 0.344         |
| rollout/                |               |
|    adjusted_reward      | 4.46          |
|    ep_len_mean          | 458           |
|    ep_rew_mean          | 2.12e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 400           |
|    time_elapsed         | 5170          |
|    total_timesteps      | 4096000       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10168         |
|    mean_motor0          | 0.6727729     |
|    mean_motor1          | 0.3480452     |
|    mean_motor2          | 0.7436663     |
|    mean_motor3          | 0.31935388    |
|    mean_motor4          | 0.5658635     |
|    mean_motor5          | 0.5100193     |
|    mean_motor6          | 0.58920133    |
|    mean_motor7          | 0.46127218    |
| train/                  |               |
|    approx_kl            | 0.07802844    |
|    average_cost         | 0.0026367188  |
|    clip_fraction        | 0.177         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.74          |
|    cost_value_loss      | 0.00903       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.605        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00571       |
|    mean_cost_advantages | 0.00095136586 |
|    mean_reward_advan... | 0.012693693   |
|    n_updates            | 7980          |
|    nu                   | 14.5          |
|    nu_loss              | -0.0377       |
|    policy_gradient_loss | -0.00648      |
|    reward_explained_... | 0.712         |
|    reward_value_loss    | 0.00868       |
|    std                  | 0.264         |
|    total_cost           | 27.0          |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 493          |
|    mean_reward          | 1.97e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.81         |
|    forward_reward       | 0.187        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.47        |
|    reward_forward       | 0.187        |
|    reward_survive       | 1            |
|    x_position           | -0.531       |
|    x_velocity           | 0.187        |
|    y_position           | 4.67         |
|    y_velocity           | 0.261        |
| rollout/                |              |
|    adjusted_reward      | 3.92         |
|    ep_len_mean          | 463          |
|    ep_rew_mean          | 2.05e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 401          |
|    time_elapsed         | 5183         |
|    total_timesteps      | 4106240      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10180        |
|    mean_motor0          | 0.60170317   |
|    mean_motor1          | 0.36568832   |
|    mean_motor2          | 0.7503995    |
|    mean_motor3          | 0.35478806   |
|    mean_motor4          | 0.58596414   |
|    mean_motor5          | 0.53044754   |
|    mean_motor6          | 0.6092521    |
|    mean_motor7          | 0.5234604    |
| train/                  |              |
|    approx_kl            | 0.081695095  |
|    average_cost         | 0.0052734376 |
|    clip_fraction        | 0.16         |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.838        |
|    cost_value_loss      | 0.00953      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.576       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00272      |
|    mean_cost_advantages | 0.008656087  |
|    mean_reward_advan... | 0.016591523  |
|    n_updates            | 8000         |
|    nu                   | 14.6         |
|    nu_loss              | -0.0763      |
|    policy_gradient_loss | -0.00522     |
|    reward_explained_... | 0.756        |
|    reward_value_loss    | 0.00801      |
|    std                  | 0.264        |
|    total_cost           | 54.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 467           |
|    mean_reward          | 1.47e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.98          |
|    forward_reward       | 0.205         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.22         |
|    reward_forward       | 0.205         |
|    reward_survive       | 1             |
|    x_position           | -0.424        |
|    x_velocity           | 0.205         |
|    y_position           | 4.88          |
|    y_velocity           | 0.26          |
| rollout/                |               |
|    adjusted_reward      | 4.16          |
|    ep_len_mean          | 473           |
|    ep_rew_mean          | 2.06e+03      |
| time/                   |               |
|    fps                  | 792           |
|    iterations           | 402           |
|    time_elapsed         | 5196          |
|    total_timesteps      | 4116480       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10172         |
|    mean_motor0          | 0.7432071     |
|    mean_motor1          | 0.3417154     |
|    mean_motor2          | 0.7549616     |
|    mean_motor3          | 0.31414965    |
|    mean_motor4          | 0.5878085     |
|    mean_motor5          | 0.46882063    |
|    mean_motor6          | 0.6101633     |
|    mean_motor7          | 0.4904965     |
| train/                  |               |
|    approx_kl            | 0.06356351    |
|    average_cost         | 0.00068359374 |
|    clip_fraction        | 0.126         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.58          |
|    cost_value_loss      | 0.0014        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.562        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000493      |
|    mean_cost_advantages | -0.0021249938 |
|    mean_reward_advan... | 0.0030912142  |
|    n_updates            | 8020          |
|    nu                   | 14.8          |
|    nu_loss              | -0.01         |
|    policy_gradient_loss | -0.00377      |
|    reward_explained_... | 0.899         |
|    reward_value_loss    | 0.00795       |
|    std                  | 0.263         |
|    total_cost           | 7.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 492            |
|    mean_reward          | 2.14e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.78           |
|    forward_reward       | 0.29           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.59          |
|    reward_forward       | 0.29           |
|    reward_survive       | 1              |
|    x_position           | -0.483         |
|    x_velocity           | 0.29           |
|    y_position           | 4.68           |
|    y_velocity           | 0.25           |
| rollout/                |                |
|    adjusted_reward      | 4.41           |
|    ep_len_mean          | 474            |
|    ep_rew_mean          | 2.04e+03       |
| time/                   |                |
|    fps                  | 792            |
|    iterations           | 403            |
|    time_elapsed         | 5209           |
|    total_timesteps      | 4126720        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10170          |
|    mean_motor0          | 0.7632327      |
|    mean_motor1          | 0.34056702     |
|    mean_motor2          | 0.76721495     |
|    mean_motor3          | 0.3128559      |
|    mean_motor4          | 0.6199299      |
|    mean_motor5          | 0.50494987     |
|    mean_motor6          | 0.60218275     |
|    mean_motor7          | 0.48197812     |
| train/                  |                |
|    approx_kl            | 0.050438244    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.119          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.909          |
|    cost_value_loss      | 0.000179       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.537         |
|    learning_rate        | 3e-05          |
|    loss                 | -0.00106       |
|    mean_cost_advantages | -0.00062746764 |
|    mean_reward_advan... | 0.0052592335   |
|    n_updates            | 8040           |
|    nu                   | 14.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00204       |
|    reward_explained_... | 0.728          |
|    reward_value_loss    | 0.00836        |
|    std                  | 0.262          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.3e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.99         |
|    forward_reward       | 0.375        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.375        |
|    reward_survive       | 1            |
|    x_position           | 0.416        |
|    x_velocity           | 0.375        |
|    y_position           | 4.79         |
|    y_velocity           | 0.481        |
| rollout/                |              |
|    adjusted_reward      | 4.48         |
|    ep_len_mean          | 477          |
|    ep_rew_mean          | 2.03e+03     |
| time/                   |              |
|    fps                  | 792          |
|    iterations           | 404          |
|    time_elapsed         | 5223         |
|    total_timesteps      | 4136960      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10176        |
|    mean_motor0          | 0.6508757    |
|    mean_motor1          | 0.3366809    |
|    mean_motor2          | 0.74801004   |
|    mean_motor3          | 0.32482138   |
|    mean_motor4          | 0.6215131    |
|    mean_motor5          | 0.5339886    |
|    mean_motor6          | 0.57146996   |
|    mean_motor7          | 0.47943896   |
| train/                  |              |
|    approx_kl            | 0.043652914  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0981       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.893        |
|    cost_value_loss      | 8.38e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.508       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00497      |
|    mean_cost_advantages | -0.000478731 |
|    mean_reward_advan... | 0.0088238595 |
|    n_updates            | 8060         |
|    nu                   | 15.1         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00193     |
|    reward_explained_... | 0.745        |
|    reward_value_loss    | 0.00896      |
|    std                  | 0.261        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.12e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.21          |
|    forward_reward       | 0.319         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.25         |
|    reward_forward       | 0.319         |
|    reward_survive       | 1             |
|    x_position           | -0.0556       |
|    x_velocity           | 0.319         |
|    y_position           | 4.13          |
|    y_velocity           | 0.468         |
| rollout/                |               |
|    adjusted_reward      | 4.43          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 2.07e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 405           |
|    time_elapsed         | 5236          |
|    total_timesteps      | 4147200       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10178         |
|    mean_motor0          | 0.5832136     |
|    mean_motor1          | 0.33516234    |
|    mean_motor2          | 0.78223085    |
|    mean_motor3          | 0.31079638    |
|    mean_motor4          | 0.6698879     |
|    mean_motor5          | 0.49365872    |
|    mean_motor6          | 0.5728575     |
|    mean_motor7          | 0.4693645     |
| train/                  |               |
|    approx_kl            | 0.051352985   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.132         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.904         |
|    cost_value_loss      | 7.7e-05       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.484        |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00136      |
|    mean_cost_advantages | -0.0004934606 |
|    mean_reward_advan... | 0.011913497   |
|    n_updates            | 8080          |
|    nu                   | 15.2          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.002        |
|    reward_explained_... | 0.881         |
|    reward_value_loss    | 0.0083        |
|    std                  | 0.261         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.25e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.22         |
|    forward_reward       | 0.314        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.27        |
|    reward_forward       | 0.314        |
|    reward_survive       | 1            |
|    x_position           | -0.397       |
|    x_velocity           | 0.314        |
|    y_position           | 5.13         |
|    y_velocity           | 0.408        |
| rollout/                |              |
|    adjusted_reward      | 4.71         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 2.18e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 406          |
|    time_elapsed         | 5249         |
|    total_timesteps      | 4157440      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10173        |
|    mean_motor0          | 0.570856     |
|    mean_motor1          | 0.32944232   |
|    mean_motor2          | 0.7670481    |
|    mean_motor3          | 0.3119199    |
|    mean_motor4          | 0.65677583   |
|    mean_motor5          | 0.47652897   |
|    mean_motor6          | 0.5859965    |
|    mean_motor7          | 0.46393076   |
| train/                  |              |
|    approx_kl            | 0.04346777   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.123        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.939        |
|    cost_value_loss      | 3.47e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.466       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.000609    |
|    mean_cost_advantages | 0.0003559575 |
|    mean_reward_advan... | 0.012258488  |
|    n_updates            | 8100         |
|    nu                   | 15.3         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00185     |
|    reward_explained_... | 0.866        |
|    reward_value_loss    | 0.00788      |
|    std                  | 0.26         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 479          |
|    mean_reward          | 2.39e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.07         |
|    forward_reward       | 0.29         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.4         |
|    reward_forward       | 0.29         |
|    reward_survive       | 1            |
|    x_position           | -0.214       |
|    x_velocity           | 0.29         |
|    y_position           | 6.02         |
|    y_velocity           | 0.407        |
| rollout/                |              |
|    adjusted_reward      | 5.12         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 2.26e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 407          |
|    time_elapsed         | 5262         |
|    total_timesteps      | 4167680      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10131        |
|    mean_motor0          | 0.5990338    |
|    mean_motor1          | 0.3187125    |
|    mean_motor2          | 0.7450207    |
|    mean_motor3          | 0.30277577   |
|    mean_motor4          | 0.6081499    |
|    mean_motor5          | 0.50020444   |
|    mean_motor6          | 0.5768693    |
|    mean_motor7          | 0.45561513   |
| train/                  |              |
|    approx_kl            | 0.040329706  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.117        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.929        |
|    cost_value_loss      | 1.84e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.451       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00058      |
|    mean_cost_advantages | 9.420667e-05 |
|    mean_reward_advan... | 0.009141101  |
|    n_updates            | 8120         |
|    nu                   | 15.4         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00178     |
|    reward_explained_... | 0.616        |
|    reward_value_loss    | 0.00958      |
|    std                  | 0.26         |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.54e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.81          |
|    forward_reward       | 0.23          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.4          |
|    reward_forward       | 0.23          |
|    reward_survive       | 1             |
|    x_position           | -0.653        |
|    x_velocity           | 0.23          |
|    y_position           | 3.7           |
|    y_velocity           | 0.591         |
| rollout/                |               |
|    adjusted_reward      | 4.67          |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 2.31e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 408           |
|    time_elapsed         | 5276          |
|    total_timesteps      | 4177920       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10152         |
|    mean_motor0          | 0.5416464     |
|    mean_motor1          | 0.31547728    |
|    mean_motor2          | 0.8118666     |
|    mean_motor3          | 0.309431      |
|    mean_motor4          | 0.6560838     |
|    mean_motor5          | 0.48252973    |
|    mean_motor6          | 0.6031749     |
|    mean_motor7          | 0.4432631     |
| train/                  |               |
|    approx_kl            | 0.039715618   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.104         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.851         |
|    cost_value_loss      | 6.17e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.444        |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00064      |
|    mean_cost_advantages | -0.0004927454 |
|    mean_reward_advan... | 0.015136508   |
|    n_updates            | 8140          |
|    nu                   | 15.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00179      |
|    reward_explained_... | 0.635         |
|    reward_value_loss    | 0.011         |
|    std                  | 0.26          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.41e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.46          |
|    forward_reward       | 0.294         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.32         |
|    reward_forward       | 0.294         |
|    reward_survive       | 1             |
|    x_position           | -0.474        |
|    x_velocity           | 0.294         |
|    y_position           | 4.4           |
|    y_velocity           | 0.533         |
| rollout/                |               |
|    adjusted_reward      | 4.96          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.34e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 409           |
|    time_elapsed         | 5289          |
|    total_timesteps      | 4188160       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10148         |
|    mean_motor0          | 0.53463465    |
|    mean_motor1          | 0.3193086     |
|    mean_motor2          | 0.7877679     |
|    mean_motor3          | 0.2933839     |
|    mean_motor4          | 0.5868524     |
|    mean_motor5          | 0.5220088     |
|    mean_motor6          | 0.59141695    |
|    mean_motor7          | 0.44781908    |
| train/                  |               |
|    approx_kl            | 0.056798745   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.145         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.793         |
|    cost_value_loss      | 0.000116      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.439        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00194       |
|    mean_cost_advantages | -0.0006088296 |
|    mean_reward_advan... | 0.009501907   |
|    n_updates            | 8160          |
|    nu                   | 15.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00205      |
|    reward_explained_... | 0.867         |
|    reward_value_loss    | 0.0117        |
|    std                  | 0.26          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.6e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.96         |
|    forward_reward       | 0.367        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.21        |
|    reward_forward       | 0.367        |
|    reward_survive       | 1            |
|    x_position           | -0.106       |
|    x_velocity           | 0.367        |
|    y_position           | 5.94         |
|    y_velocity           | 0.755        |
| rollout/                |              |
|    adjusted_reward      | 5.14         |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 2.42e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 410          |
|    time_elapsed         | 5302         |
|    total_timesteps      | 4198400      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10133        |
|    mean_motor0          | 0.531788     |
|    mean_motor1          | 0.31402326   |
|    mean_motor2          | 0.7612538    |
|    mean_motor3          | 0.28516862   |
|    mean_motor4          | 0.61517286   |
|    mean_motor5          | 0.51508605   |
|    mean_motor6          | 0.55157405   |
|    mean_motor7          | 0.46215397   |
| train/                  |              |
|    approx_kl            | 0.043591805  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.118        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.904        |
|    cost_value_loss      | 1.06e-05     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.42        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00624      |
|    mean_cost_advantages | 0.0001286697 |
|    mean_reward_advan... | 0.013277246  |
|    n_updates            | 8180         |
|    nu                   | 15.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00189     |
|    reward_explained_... | 0.769        |
|    reward_value_loss    | 0.0109       |
|    std                  | 0.259        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 386          |
|    mean_reward          | 2.2e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.76         |
|    forward_reward       | 0.251        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.35        |
|    reward_forward       | 0.251        |
|    reward_survive       | 1            |
|    x_position           | -0.43        |
|    x_velocity           | 0.251        |
|    y_position           | 4.5          |
|    y_velocity           | 0.514        |
| rollout/                |              |
|    adjusted_reward      | 4.4          |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 2.39e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 411          |
|    time_elapsed         | 5315         |
|    total_timesteps      | 4208640      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10129        |
|    mean_motor0          | 0.529853     |
|    mean_motor1          | 0.34478655   |
|    mean_motor2          | 0.7426812    |
|    mean_motor3          | 0.30677933   |
|    mean_motor4          | 0.5853076    |
|    mean_motor5          | 0.52174133   |
|    mean_motor6          | 0.65702057   |
|    mean_motor7          | 0.47327596   |
| train/                  |              |
|    approx_kl            | 0.042654414  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.109        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.919        |
|    cost_value_loss      | 6.12e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.396       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00805      |
|    mean_cost_advantages | 9.568214e-05 |
|    mean_reward_advan... | 0.011679021  |
|    n_updates            | 8200         |
|    nu                   | 15.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0017      |
|    reward_explained_... | 0.553        |
|    reward_value_loss    | 0.012        |
|    std                  | 0.258        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.55e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.74          |
|    forward_reward       | 0.356         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.17         |
|    reward_forward       | 0.356         |
|    reward_survive       | 1             |
|    x_position           | -0.437        |
|    x_velocity           | 0.356         |
|    y_position           | 5.58          |
|    y_velocity           | 0.28          |
| rollout/                |               |
|    adjusted_reward      | 5.17          |
|    ep_len_mean          | 493           |
|    ep_rew_mean          | 2.4e+03       |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 412           |
|    time_elapsed         | 5329          |
|    total_timesteps      | 4218880       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10138         |
|    mean_motor0          | 0.49969754    |
|    mean_motor1          | 0.30704984    |
|    mean_motor2          | 0.7752435     |
|    mean_motor3          | 0.2748404     |
|    mean_motor4          | 0.61154306    |
|    mean_motor5          | 0.5202426     |
|    mean_motor6          | 0.56645334    |
|    mean_motor7          | 0.43376908    |
| train/                  |               |
|    approx_kl            | 0.045474753   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.1           |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.949         |
|    cost_value_loss      | 1.34e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.388        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000502      |
|    mean_cost_advantages | -0.0004002811 |
|    mean_reward_advan... | 0.010176284   |
|    n_updates            | 8220          |
|    nu                   | 15.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00182      |
|    reward_explained_... | 0.937         |
|    reward_value_loss    | 0.00939       |
|    std                  | 0.258         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.69e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5           |
|    forward_reward       | 0.201       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.18       |
|    reward_forward       | 0.201       |
|    reward_survive       | 1           |
|    x_position           | -0.332      |
|    x_velocity           | 0.201       |
|    y_position           | 4.95        |
|    y_velocity           | 0.464       |
| rollout/                |             |
|    adjusted_reward      | 5.4         |
|    ep_len_mean          | 496         |
|    ep_rew_mean          | 2.49e+03    |
| time/                   |             |
|    fps                  | 791         |
|    iterations           | 413         |
|    time_elapsed         | 5342        |
|    total_timesteps      | 4229120     |
| torque/                 |             |
|    greater_than_0.25    | 10239       |
|    greater_than_0.3     | 10233       |
|    greater_than_0.5     | 10104       |
|    mean_motor0          | 0.5084923   |
|    mean_motor1          | 0.2997045   |
|    mean_motor2          | 0.752062    |
|    mean_motor3          | 0.2731204   |
|    mean_motor4          | 0.639353    |
|    mean_motor5          | 0.5328448   |
|    mean_motor6          | 0.53968716  |
|    mean_motor7          | 0.43470865  |
| train/                  |             |
|    approx_kl            | 0.048164427 |
|    average_cost         | 0.0         |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.881       |
|    cost_value_loss      | 5.03e-06    |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.375      |
|    learning_rate        | 3e-05       |
|    loss                 | 0.000184    |
|    mean_cost_advantages | 7.79632e-05 |
|    mean_reward_advan... | 0.010258632 |
|    n_updates            | 8240        |
|    nu                   | 15.7        |
|    nu_loss              | -0          |
|    policy_gradient_loss | -0.00168    |
|    reward_explained_... | 0.779       |
|    reward_value_loss    | 0.0128      |
|    std                  | 0.257       |
|    total_cost           | 0.0         |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 464          |
|    mean_reward          | 2.71e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.89         |
|    forward_reward       | 0.3          |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.25        |
|    reward_forward       | 0.3          |
|    reward_survive       | 1            |
|    x_position           | -0.754       |
|    x_velocity           | 0.3          |
|    y_position           | 5.73         |
|    y_velocity           | 0.414        |
| rollout/                |              |
|    adjusted_reward      | 5.53         |
|    ep_len_mean          | 497          |
|    ep_rew_mean          | 2.54e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 414          |
|    time_elapsed         | 5355         |
|    total_timesteps      | 4239360      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10160        |
|    mean_motor0          | 0.4949909    |
|    mean_motor1          | 0.3027064    |
|    mean_motor2          | 0.776425     |
|    mean_motor3          | 0.2715324    |
|    mean_motor4          | 0.6536005    |
|    mean_motor5          | 0.5439545    |
|    mean_motor6          | 0.5570791    |
|    mean_motor7          | 0.4251105    |
| train/                  |              |
|    approx_kl            | 0.05040902   |
|    average_cost         | 0.0013671875 |
|    clip_fraction        | 0.141        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.0816       |
|    cost_value_loss      | 0.000835     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.346       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0107       |
|    mean_cost_advantages | 0.0015262975 |
|    mean_reward_advan... | 0.008639331  |
|    n_updates            | 8260         |
|    nu                   | 15.8         |
|    nu_loss              | -0.0215      |
|    policy_gradient_loss | -0.00365     |
|    reward_explained_... | 0.587        |
|    reward_value_loss    | 0.0151       |
|    std                  | 0.256        |
|    total_cost           | 14.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 440            |
|    mean_reward          | 2.32e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.69           |
|    forward_reward       | 0.324          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.34          |
|    reward_forward       | 0.324          |
|    reward_survive       | 1              |
|    x_position           | -1.16          |
|    x_velocity           | 0.324          |
|    y_position           | 5.53           |
|    y_velocity           | 0.654          |
| rollout/                |                |
|    adjusted_reward      | 5.48           |
|    ep_len_mean          | 490            |
|    ep_rew_mean          | 2.54e+03       |
| time/                   |                |
|    fps                  | 791            |
|    iterations           | 415            |
|    time_elapsed         | 5368           |
|    total_timesteps      | 4249600        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10157          |
|    mean_motor0          | 0.4995957      |
|    mean_motor1          | 0.3076959      |
|    mean_motor2          | 0.7470432      |
|    mean_motor3          | 0.29366308     |
|    mean_motor4          | 0.6586881      |
|    mean_motor5          | 0.58988523     |
|    mean_motor6          | 0.54126966     |
|    mean_motor7          | 0.42868137     |
| train/                  |                |
|    approx_kl            | 0.03994652     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.103          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.633          |
|    cost_value_loss      | 3.01e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.316         |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00319        |
|    mean_cost_advantages | -1.0958605e-05 |
|    mean_reward_advan... | 0.010090615    |
|    n_updates            | 8280           |
|    nu                   | 15.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00142       |
|    reward_explained_... | 0.614          |
|    reward_value_loss    | 0.0148         |
|    std                  | 0.256          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.48e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.47         |
|    forward_reward       | 0.298        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.08        |
|    reward_forward       | 0.298        |
|    reward_survive       | 1            |
|    x_position           | -1.06        |
|    x_velocity           | 0.298        |
|    y_position           | 5.34         |
|    y_velocity           | 0.574        |
| rollout/                |              |
|    adjusted_reward      | 5.31         |
|    ep_len_mean          | 485          |
|    ep_rew_mean          | 2.64e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 416          |
|    time_elapsed         | 5381         |
|    total_timesteps      | 4259840      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10234        |
|    greater_than_0.5     | 10090        |
|    mean_motor0          | 0.47818628   |
|    mean_motor1          | 0.3209349    |
|    mean_motor2          | 0.73425287   |
|    mean_motor3          | 0.26965216   |
|    mean_motor4          | 0.5768693    |
|    mean_motor5          | 0.5489877    |
|    mean_motor6          | 0.5466312    |
|    mean_motor7          | 0.44735685   |
| train/                  |              |
|    approx_kl            | 0.12394102   |
|    average_cost         | 0.0072265626 |
|    clip_fraction        | 0.236        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.129        |
|    cost_value_loss      | 0.00311      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.303       |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0153       |
|    mean_cost_advantages | 0.009681923  |
|    mean_reward_advan... | 0.009572918  |
|    n_updates            | 8300         |
|    nu                   | 15.9         |
|    nu_loss              | -0.114       |
|    policy_gradient_loss | -0.00445     |
|    reward_explained_... | 0.868        |
|    reward_value_loss    | 0.0163       |
|    std                  | 0.256        |
|    total_cost           | 74.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.74e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.41         |
|    forward_reward       | 0.191        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.22        |
|    reward_forward       | 0.191        |
|    reward_survive       | 1            |
|    x_position           | -1.04        |
|    x_velocity           | 0.191        |
|    y_position           | 6.26         |
|    y_velocity           | 0.358        |
| rollout/                |              |
|    adjusted_reward      | 5.21         |
|    ep_len_mean          | 486          |
|    ep_rew_mean          | 2.63e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 417          |
|    time_elapsed         | 5395         |
|    total_timesteps      | 4270080      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10237        |
|    greater_than_0.5     | 10140        |
|    mean_motor0          | 0.53238463   |
|    mean_motor1          | 0.30247915   |
|    mean_motor2          | 0.75993824   |
|    mean_motor3          | 0.26341376   |
|    mean_motor4          | 0.6447344    |
|    mean_motor5          | 0.5702342    |
|    mean_motor6          | 0.5262215    |
|    mean_motor7          | 0.47463346   |
| train/                  |              |
|    approx_kl            | 0.4378655    |
|    average_cost         | 0.017773438  |
|    clip_fraction        | 0.403        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.532        |
|    cost_value_loss      | 0.014        |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.293       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0065      |
|    mean_cost_advantages | 0.022838902  |
|    mean_reward_advan... | 0.0057639917 |
|    n_updates            | 8320         |
|    nu                   | 15.9         |
|    nu_loss              | -0.282       |
|    policy_gradient_loss | -0.0114      |
|    reward_explained_... | 0.811        |
|    reward_value_loss    | 0.0178       |
|    std                  | 0.255        |
|    total_cost           | 182.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.65e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.12          |
|    forward_reward       | 0.209         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.45         |
|    reward_forward       | 0.209         |
|    reward_survive       | 1             |
|    x_position           | -0.418        |
|    x_velocity           | 0.209         |
|    y_position           | 6.07          |
|    y_velocity           | 0.251         |
| rollout/                |               |
|    adjusted_reward      | 5.35          |
|    ep_len_mean          | 484           |
|    ep_rew_mean          | 2.6e+03       |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 418           |
|    time_elapsed         | 5408          |
|    total_timesteps      | 4280320       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10159         |
|    mean_motor0          | 0.54874754    |
|    mean_motor1          | 0.29955763    |
|    mean_motor2          | 0.7464837     |
|    mean_motor3          | 0.26266378    |
|    mean_motor4          | 0.67277545    |
|    mean_motor5          | 0.5982015     |
|    mean_motor6          | 0.51840085    |
|    mean_motor7          | 0.4874341     |
| train/                  |               |
|    approx_kl            | 0.05874424    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.122         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.838         |
|    cost_value_loss      | 0.000454      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.276        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00645       |
|    mean_cost_advantages | -0.0008903401 |
|    mean_reward_advan... | 0.0071300617  |
|    n_updates            | 8340          |
|    nu                   | 16            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00166      |
|    reward_explained_... | 0.861         |
|    reward_value_loss    | 0.0137        |
|    std                  | 0.255         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.69e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.64          |
|    forward_reward       | 0.267         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.33         |
|    reward_forward       | 0.267         |
|    reward_survive       | 1             |
|    x_position           | -0.515        |
|    x_velocity           | 0.267         |
|    y_position           | 5.6           |
|    y_velocity           | 0.609         |
| rollout/                |               |
|    adjusted_reward      | 5.37          |
|    ep_len_mean          | 484           |
|    ep_rew_mean          | 2.59e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 419           |
|    time_elapsed         | 5421          |
|    total_timesteps      | 4290560       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10182         |
|    mean_motor0          | 0.5060133     |
|    mean_motor1          | 0.30733046    |
|    mean_motor2          | 0.7397241     |
|    mean_motor3          | 0.26368016    |
|    mean_motor4          | 0.6553366     |
|    mean_motor5          | 0.673668      |
|    mean_motor6          | 0.49926138    |
|    mean_motor7          | 0.47535396    |
| train/                  |               |
|    approx_kl            | 0.049626835   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.125         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.751         |
|    cost_value_loss      | 0.000135      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.275        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00394       |
|    mean_cost_advantages | -0.0007876414 |
|    mean_reward_advan... | 9.800932e-05  |
|    n_updates            | 8360          |
|    nu                   | 16            |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00142      |
|    reward_explained_... | 0.534         |
|    reward_value_loss    | 0.0155        |
|    std                  | 0.255         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.67e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 6.2         |
|    forward_reward       | 0.311       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.54       |
|    reward_forward       | 0.311       |
|    reward_survive       | 1           |
|    x_position           | -0.278      |
|    x_velocity           | 0.311       |
|    y_position           | 6.13        |
|    y_velocity           | 0.301       |
| rollout/                |             |
|    adjusted_reward      | 5.36        |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 2.63e+03    |
| time/                   |             |
|    fps                  | 791         |
|    iterations           | 420         |
|    time_elapsed         | 5435        |
|    total_timesteps      | 4300800     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10184       |
|    mean_motor0          | 0.7509995   |
|    mean_motor1          | 0.30315244  |
|    mean_motor2          | 0.75602376  |
|    mean_motor3          | 0.26378328  |
|    mean_motor4          | 0.70774674  |
|    mean_motor5          | 0.80515957  |
|    mean_motor6          | 0.44574174  |
|    mean_motor7          | 0.50335467  |
| train/                  |             |
|    approx_kl            | 1.4792683   |
|    average_cost         | 0.029785156 |
|    clip_fraction        | 0.686       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.397       |
|    cost_value_loss      | 0.00753     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | -0.263      |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0299     |
|    mean_cost_advantages | 0.040961165 |
|    mean_reward_advan... | 0.010704502 |
|    n_updates            | 8380        |
|    nu                   | 16          |
|    nu_loss              | -0.476      |
|    policy_gradient_loss | -0.0101     |
|    reward_explained_... | 0.851       |
|    reward_value_loss    | 0.0152      |
|    std                  | 0.254       |
|    total_cost           | 305.0       |
-----------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 417          |
|    mean_reward          | 2.18e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.97         |
|    forward_reward       | 0.247        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.42        |
|    reward_forward       | 0.247        |
|    reward_survive       | 1            |
|    x_position           | -1.04        |
|    x_velocity           | 0.247        |
|    y_position           | 6.86         |
|    y_velocity           | 0.355        |
| rollout/                |              |
|    adjusted_reward      | 5.36         |
|    ep_len_mean          | 495          |
|    ep_rew_mean          | 2.64e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 421          |
|    time_elapsed         | 5448         |
|    total_timesteps      | 4311040      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 10161        |
|    mean_motor0          | 0.620917     |
|    mean_motor1          | 0.29792657   |
|    mean_motor2          | 0.71027434   |
|    mean_motor3          | 0.2788486    |
|    mean_motor4          | 0.6974606    |
|    mean_motor5          | 0.731332     |
|    mean_motor6          | 0.4719717    |
|    mean_motor7          | 0.46209806   |
| train/                  |              |
|    approx_kl            | 0.21973014   |
|    average_cost         | 0.008007812  |
|    clip_fraction        | 0.236        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.872        |
|    cost_value_loss      | 0.00263      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.24        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00466      |
|    mean_cost_advantages | 0.0086159855 |
|    mean_reward_advan... | 0.0004758215 |
|    n_updates            | 8400         |
|    nu                   | 16.1         |
|    nu_loss              | -0.128       |
|    policy_gradient_loss | -0.00369     |
|    reward_explained_... | 0.557        |
|    reward_value_loss    | 0.0156       |
|    std                  | 0.254        |
|    total_cost           | 82.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.85e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.08          |
|    forward_reward       | 0.104         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.104         |
|    reward_survive       | 1             |
|    x_position           | -1.28         |
|    x_velocity           | 0.104         |
|    y_position           | 6.94          |
|    y_velocity           | 0.172         |
| rollout/                |               |
|    adjusted_reward      | 5.48          |
|    ep_len_mean          | 494           |
|    ep_rew_mean          | 2.67e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 422           |
|    time_elapsed         | 5461          |
|    total_timesteps      | 4321280       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10168         |
|    mean_motor0          | 0.6937565     |
|    mean_motor1          | 0.3147688     |
|    mean_motor2          | 0.71810746    |
|    mean_motor3          | 0.2763792     |
|    mean_motor4          | 0.71030295    |
|    mean_motor5          | 0.773162      |
|    mean_motor6          | 0.4567445     |
|    mean_motor7          | 0.44799012    |
| train/                  |               |
|    approx_kl            | 0.046027415   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.122         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.712         |
|    cost_value_loss      | 4.17e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.216        |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00169      |
|    mean_cost_advantages | -0.0008365473 |
|    mean_reward_advan... | 0.008741822   |
|    n_updates            | 8420          |
|    nu                   | 16.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00163      |
|    reward_explained_... | 0.814         |
|    reward_value_loss    | 0.0136        |
|    std                  | 0.253         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.88e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.59          |
|    forward_reward       | 0.185         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.56         |
|    reward_forward       | 0.185         |
|    reward_survive       | 1             |
|    x_position           | -0.47         |
|    x_velocity           | 0.185         |
|    y_position           | 7.5           |
|    y_velocity           | 0.222         |
| rollout/                |               |
|    adjusted_reward      | 5.39          |
|    ep_len_mean          | 494           |
|    ep_rew_mean          | 2.66e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 423           |
|    time_elapsed         | 5474          |
|    total_timesteps      | 4331520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10156         |
|    mean_motor0          | 0.6823839     |
|    mean_motor1          | 0.30629894    |
|    mean_motor2          | 0.69993657    |
|    mean_motor3          | 0.26341152    |
|    mean_motor4          | 0.7245892     |
|    mean_motor5          | 0.7766339     |
|    mean_motor6          | 0.44693565    |
|    mean_motor7          | 0.44568166    |
| train/                  |               |
|    approx_kl            | 0.07146891    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.158         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.854         |
|    cost_value_loss      | 0.000438      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.194        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00933       |
|    mean_cost_advantages | -0.0015496794 |
|    mean_reward_advan... | 0.0073285615  |
|    n_updates            | 8440          |
|    nu                   | 16.2          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00223      |
|    reward_explained_... | 0.881         |
|    reward_value_loss    | 0.0152        |
|    std                  | 0.252         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.75e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.53          |
|    forward_reward       | 0.367         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.52         |
|    reward_forward       | 0.367         |
|    reward_survive       | 1             |
|    x_position           | -0.851        |
|    x_velocity           | 0.367         |
|    y_position           | 6.45          |
|    y_velocity           | 0.435         |
| rollout/                |               |
|    adjusted_reward      | 5.51          |
|    ep_len_mean          | 494           |
|    ep_rew_mean          | 2.68e+03      |
| time/                   |               |
|    fps                  | 791           |
|    iterations           | 424           |
|    time_elapsed         | 5488          |
|    total_timesteps      | 4341760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10172         |
|    mean_motor0          | 0.7715016     |
|    mean_motor1          | 0.30551237    |
|    mean_motor2          | 0.66971356    |
|    mean_motor3          | 0.27447373    |
|    mean_motor4          | 0.70192945    |
|    mean_motor5          | 0.8120756     |
|    mean_motor6          | 0.48041433    |
|    mean_motor7          | 0.4677015     |
| train/                  |               |
|    approx_kl            | 0.05721859    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.111         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.795         |
|    cost_value_loss      | 3.12e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.183        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0096        |
|    mean_cost_advantages | -0.0007852696 |
|    mean_reward_advan... | 0.0037944936  |
|    n_updates            | 8460          |
|    nu                   | 16.2          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00132      |
|    reward_explained_... | 0.83          |
|    reward_value_loss    | 0.0164        |
|    std                  | 0.252         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.9e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.88         |
|    forward_reward       | 0.228        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.29        |
|    reward_forward       | 0.228        |
|    reward_survive       | 1            |
|    x_position           | -0.368       |
|    x_velocity           | 0.228        |
|    y_position           | 5.87         |
|    y_velocity           | 0.717        |
| rollout/                |              |
|    adjusted_reward      | 5.54         |
|    ep_len_mean          | 494          |
|    ep_rew_mean          | 2.71e+03     |
| time/                   |              |
|    fps                  | 791          |
|    iterations           | 425          |
|    time_elapsed         | 5501         |
|    total_timesteps      | 4352000      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10159        |
|    mean_motor0          | 0.7993452    |
|    mean_motor1          | 0.2872795    |
|    mean_motor2          | 0.64829165   |
|    mean_motor3          | 0.2772588    |
|    mean_motor4          | 0.67975914   |
|    mean_motor5          | 0.7863293    |
|    mean_motor6          | 0.49099794   |
|    mean_motor7          | 0.44266057   |
| train/                  |              |
|    approx_kl            | 0.084749594  |
|    average_cost         | 0.0041015623 |
|    clip_fraction        | 0.183        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.918        |
|    cost_value_loss      | 0.00195      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.177       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.000218    |
|    mean_cost_advantages | 0.0031910203 |
|    mean_reward_advan... | 0.0058497125 |
|    n_updates            | 8480         |
|    nu                   | 16.2         |
|    nu_loss              | -0.0664      |
|    policy_gradient_loss | -0.00228     |
|    reward_explained_... | 0.762        |
|    reward_value_loss    | 0.0162       |
|    std                  | 0.252        |
|    total_cost           | 42.0         |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.87e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.34         |
|    forward_reward       | 0.347        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.51        |
|    reward_forward       | 0.347        |
|    reward_survive       | 1            |
|    x_position           | -0.297       |
|    x_velocity           | 0.347        |
|    y_position           | 6.27         |
|    y_velocity           | 0.399        |
| rollout/                |              |
|    adjusted_reward      | 5.05         |
|    ep_len_mean          | 496          |
|    ep_rew_mean          | 2.67e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 426          |
|    time_elapsed         | 5514         |
|    total_timesteps      | 4362240      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10175        |
|    mean_motor0          | 0.7644278    |
|    mean_motor1          | 0.31931642   |
|    mean_motor2          | 0.6506711    |
|    mean_motor3          | 0.2720416    |
|    mean_motor4          | 0.6438229    |
|    mean_motor5          | 0.70112497   |
|    mean_motor6          | 0.5358357    |
|    mean_motor7          | 0.43671164   |
| train/                  |              |
|    approx_kl            | 0.47278625   |
|    average_cost         | 0.03261719   |
|    clip_fraction        | 0.275        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.854        |
|    cost_value_loss      | 0.0287       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.163       |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0367      |
|    mean_cost_advantages | 0.0400401    |
|    mean_reward_advan... | 0.0147204045 |
|    n_updates            | 8500         |
|    nu                   | 16.3         |
|    nu_loss              | -0.529       |
|    policy_gradient_loss | -0.0349      |
|    reward_explained_... | 0.741        |
|    reward_value_loss    | 0.0132       |
|    std                  | 0.251        |
|    total_cost           | 334.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.1e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.37          |
|    forward_reward       | 0.195         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.46         |
|    reward_forward       | 0.195         |
|    reward_survive       | 1             |
|    x_position           | -0.96         |
|    x_velocity           | 0.195         |
|    y_position           | 7.21          |
|    y_velocity           | 0.402         |
| rollout/                |               |
|    adjusted_reward      | 5.91          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.74e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 427           |
|    time_elapsed         | 5528          |
|    total_timesteps      | 4372480       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10171         |
|    mean_motor0          | 0.749787      |
|    mean_motor1          | 0.27710524    |
|    mean_motor2          | 0.68954825    |
|    mean_motor3          | 0.26524946    |
|    mean_motor4          | 0.68725586    |
|    mean_motor5          | 0.7644474     |
|    mean_motor6          | 0.5111044     |
|    mean_motor7          | 0.39494392    |
| train/                  |               |
|    approx_kl            | 0.043438308   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.106         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.847         |
|    cost_value_loss      | 5.36e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.137        |
|    learning_rate        | 3e-05         |
|    loss                 | -0.000432     |
|    mean_cost_advantages | -0.0011862491 |
|    mean_reward_advan... | 0.004924617   |
|    n_updates            | 8520          |
|    nu                   | 16.3          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00145      |
|    reward_explained_... | 0.882         |
|    reward_value_loss    | 0.0143        |
|    std                  | 0.25          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.89e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.9           |
|    forward_reward       | 0.274         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.43         |
|    reward_forward       | 0.274         |
|    reward_survive       | 1             |
|    x_position           | -0.527        |
|    x_velocity           | 0.274         |
|    y_position           | 5.82          |
|    y_velocity           | 0.271         |
| rollout/                |               |
|    adjusted_reward      | 5.46          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.78e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 428           |
|    time_elapsed         | 5541          |
|    total_timesteps      | 4382720       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10142         |
|    mean_motor0          | 0.82382184    |
|    mean_motor1          | 0.30299532    |
|    mean_motor2          | 0.6503156     |
|    mean_motor3          | 0.27135086    |
|    mean_motor4          | 0.6685957     |
|    mean_motor5          | 0.7064032     |
|    mean_motor6          | 0.52501744    |
|    mean_motor7          | 0.39806014    |
| train/                  |               |
|    approx_kl            | 0.04683318    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.136         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.726         |
|    cost_value_loss      | 4.17e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.122        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00553       |
|    mean_cost_advantages | -0.0006075098 |
|    mean_reward_advan... | 0.013710712   |
|    n_updates            | 8540          |
|    nu                   | 16.3          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00146      |
|    reward_explained_... | 0.7           |
|    reward_value_loss    | 0.0153        |
|    std                  | 0.25          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.07e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.95         |
|    forward_reward       | 0.442        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.4         |
|    reward_forward       | 0.442        |
|    reward_survive       | 1            |
|    x_position           | -0.783       |
|    x_velocity           | 0.442        |
|    y_position           | 4.81         |
|    y_velocity           | 0.552        |
| rollout/                |              |
|    adjusted_reward      | 6.18         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 2.83e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 429          |
|    time_elapsed         | 5554         |
|    total_timesteps      | 4392960      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10171        |
|    mean_motor0          | 0.9323107    |
|    mean_motor1          | 0.29318467   |
|    mean_motor2          | 0.6652363    |
|    mean_motor3          | 0.26048937   |
|    mean_motor4          | 0.7325131    |
|    mean_motor5          | 0.75816005   |
|    mean_motor6          | 0.4572608    |
|    mean_motor7          | 0.46465164   |
| train/                  |              |
|    approx_kl            | 0.95544636   |
|    average_cost         | 0.021582032  |
|    clip_fraction        | 0.486        |
|    clip_range           | 0.4          |
|    cost_explained_va... | -0.771       |
|    cost_value_loss      | 0.00728      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | -0.0988      |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0154      |
|    mean_cost_advantages | 0.02855081   |
|    mean_reward_advan... | 0.0055478946 |
|    n_updates            | 8560         |
|    nu                   | 16.4         |
|    nu_loss              | -0.353       |
|    policy_gradient_loss | -0.0183      |
|    reward_explained_... | 0.852        |
|    reward_value_loss    | 0.0173       |
|    std                  | 0.249        |
|    total_cost           | 221.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.48e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.4           |
|    forward_reward       | 0.322         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.44         |
|    reward_forward       | 0.322         |
|    reward_survive       | 1             |
|    x_position           | -0.664        |
|    x_velocity           | 0.322         |
|    y_position           | 5.29          |
|    y_velocity           | 0.782         |
| rollout/                |               |
|    adjusted_reward      | 5.8           |
|    ep_len_mean          | 498           |
|    ep_rew_mean          | 2.84e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 430           |
|    time_elapsed         | 5568          |
|    total_timesteps      | 4403200       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10167         |
|    mean_motor0          | 0.9729701     |
|    mean_motor1          | 0.28685337    |
|    mean_motor2          | 0.6725981     |
|    mean_motor3          | 0.2585202     |
|    mean_motor4          | 0.6963661     |
|    mean_motor5          | 0.7121524     |
|    mean_motor6          | 0.4841288     |
|    mean_motor7          | 0.4591207     |
| train/                  |               |
|    approx_kl            | 0.08681075    |
|    average_cost         | 9.765625e-05  |
|    clip_fraction        | 0.176         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.766         |
|    cost_value_loss      | 0.00127       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | -0.0537       |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00129       |
|    mean_cost_advantages | -0.0019394854 |
|    mean_reward_advan... | 0.01588135    |
|    n_updates            | 8580          |
|    nu                   | 16.4          |
|    nu_loss              | -0.0016       |
|    policy_gradient_loss | -0.00493      |
|    reward_explained_... | 0.733         |
|    reward_value_loss    | 0.0188        |
|    std                  | 0.247         |
|    total_cost           | 1.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 364            |
|    mean_reward          | 2.07e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.51           |
|    forward_reward       | 0.122          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.43          |
|    reward_forward       | 0.122          |
|    reward_survive       | 1              |
|    x_position           | -0.665         |
|    x_velocity           | 0.122          |
|    y_position           | 5.21           |
|    y_velocity           | 0.181          |
| rollout/                |                |
|    adjusted_reward      | 6.05           |
|    ep_len_mean          | 498            |
|    ep_rew_mean          | 2.94e+03       |
| time/                   |                |
|    fps                  | 790            |
|    iterations           | 431            |
|    time_elapsed         | 5580           |
|    total_timesteps      | 4413440        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 10155          |
|    mean_motor0          | 0.9458478      |
|    mean_motor1          | 0.30697626     |
|    mean_motor2          | 0.64694583     |
|    mean_motor3          | 0.24754381     |
|    mean_motor4          | 0.69153136     |
|    mean_motor5          | 0.7703009      |
|    mean_motor6          | 0.45988154     |
|    mean_motor7          | 0.4409213      |
| train/                  |                |
|    approx_kl            | 0.049470156    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.111          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.841          |
|    cost_value_loss      | 7.72e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | -0.0223        |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00581        |
|    mean_cost_advantages | -0.00068276795 |
|    mean_reward_advan... | 0.0053601665   |
|    n_updates            | 8600           |
|    nu                   | 16.5           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00132       |
|    reward_explained_... | 0.588          |
|    reward_value_loss    | 0.0181         |
|    std                  | 0.247          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.98e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.41         |
|    forward_reward       | 0.371        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.31        |
|    reward_forward       | 0.371        |
|    reward_survive       | 1            |
|    x_position           | -0.801       |
|    x_velocity           | 0.371        |
|    y_position           | 7.36         |
|    y_velocity           | 0.442        |
| rollout/                |              |
|    adjusted_reward      | 5.1          |
|    ep_len_mean          | 494          |
|    ep_rew_mean          | 2.84e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 432          |
|    time_elapsed         | 5594         |
|    total_timesteps      | 4423680      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10160        |
|    mean_motor0          | 1.0369602    |
|    mean_motor1          | 0.3417331    |
|    mean_motor2          | 0.6012827    |
|    mean_motor3          | 0.27096063   |
|    mean_motor4          | 0.5954486    |
|    mean_motor5          | 0.6217824    |
|    mean_motor6          | 0.60186726   |
|    mean_motor7          | 0.46288165   |
| train/                  |              |
|    approx_kl            | 0.06561588   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.883        |
|    cost_value_loss      | 0.000708     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.00397      |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00371      |
|    mean_cost_advantages | -0.002396112 |
|    mean_reward_advan... | 0.010478507  |
|    n_updates            | 8620         |
|    nu                   | 16.5         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00232     |
|    reward_explained_... | 0.67         |
|    reward_value_loss    | 0.0177       |
|    std                  | 0.246        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.97e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.07          |
|    forward_reward       | 0.271         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.45         |
|    reward_forward       | 0.271         |
|    reward_survive       | 1             |
|    x_position           | -1.03         |
|    x_velocity           | 0.271         |
|    y_position           | 5.97          |
|    y_velocity           | 0.306         |
| rollout/                |               |
|    adjusted_reward      | 6.04          |
|    ep_len_mean          | 490           |
|    ep_rew_mean          | 2.86e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 433           |
|    time_elapsed         | 5607          |
|    total_timesteps      | 4433920       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10235         |
|    greater_than_0.5     | 10093         |
|    mean_motor0          | 0.83936805    |
|    mean_motor1          | 0.30078486    |
|    mean_motor2          | 0.6663929     |
|    mean_motor3          | 0.25350428    |
|    mean_motor4          | 0.65814626    |
|    mean_motor5          | 0.6375166     |
|    mean_motor6          | 0.45243597    |
|    mean_motor7          | 0.40056267    |
| train/                  |               |
|    approx_kl            | 0.05319972    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.126         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.946         |
|    cost_value_loss      | 0.000446      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.0333        |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000865      |
|    mean_cost_advantages | -0.0028501856 |
|    mean_reward_advan... | 0.0063796462  |
|    n_updates            | 8640          |
|    nu                   | 16.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00175      |
|    reward_explained_... | 0.915         |
|    reward_value_loss    | 0.0137        |
|    std                  | 0.245         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 500         |
|    mean_reward          | 2.58e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.28        |
|    forward_reward       | 0.32        |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.45       |
|    reward_forward       | 0.32        |
|    reward_survive       | 1           |
|    x_position           | -0.735      |
|    x_velocity           | 0.32        |
|    y_position           | 5.19        |
|    y_velocity           | 0.763       |
| rollout/                |             |
|    adjusted_reward      | 5.74        |
|    ep_len_mean          | 492         |
|    ep_rew_mean          | 2.83e+03    |
| time/                   |             |
|    fps                  | 790         |
|    iterations           | 434         |
|    time_elapsed         | 5621        |
|    total_timesteps      | 4444160     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10240       |
|    greater_than_0.5     | 10187       |
|    mean_motor0          | 0.8588557   |
|    mean_motor1          | 0.3074472   |
|    mean_motor2          | 0.69129586  |
|    mean_motor3          | 0.25839522  |
|    mean_motor4          | 0.7057869   |
|    mean_motor5          | 0.6656856   |
|    mean_motor6          | 0.46512356  |
|    mean_motor7          | 0.40483412  |
| train/                  |             |
|    approx_kl            | 0.2068077   |
|    average_cost         | 0.008300781 |
|    clip_fraction        | 0.302       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.628       |
|    cost_value_loss      | 0.00846     |
|    early_stop_epoch     | 20          |
|    entropy_loss         | 0.0562      |
|    learning_rate        | 3e-05       |
|    loss                 | -4.04e-05   |
|    mean_cost_advantages | 0.008242758 |
|    mean_reward_advan... | 0.007967991 |
|    n_updates            | 8660        |
|    nu                   | 16.6        |
|    nu_loss              | -0.137      |
|    policy_gradient_loss | -0.006      |
|    reward_explained_... | 0.659       |
|    reward_value_loss    | 0.0183      |
|    std                  | 0.244       |
|    total_cost           | 85.0        |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.9e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.88           |
|    forward_reward       | 0.222          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.31          |
|    reward_forward       | 0.222          |
|    reward_survive       | 1              |
|    x_position           | -0.706         |
|    x_velocity           | 0.222          |
|    y_position           | 6.81           |
|    y_velocity           | 0.588          |
| rollout/                |                |
|    adjusted_reward      | 6.1            |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 2.85e+03       |
| time/                   |                |
|    fps                  | 790            |
|    iterations           | 435            |
|    time_elapsed         | 5634           |
|    total_timesteps      | 4454400        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10152          |
|    mean_motor0          | 0.95719403     |
|    mean_motor1          | 0.29567793     |
|    mean_motor2          | 0.63483304     |
|    mean_motor3          | 0.25509325     |
|    mean_motor4          | 0.69917125     |
|    mean_motor5          | 0.70670563     |
|    mean_motor6          | 0.42130098     |
|    mean_motor7          | 0.41598883     |
| train/                  |                |
|    approx_kl            | 0.102342345    |
|    average_cost         | 0.004199219    |
|    clip_fraction        | 0.193          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.581          |
|    cost_value_loss      | 0.00254        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.099          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00159        |
|    mean_cost_advantages | 0.0030983982   |
|    mean_reward_advan... | -0.00014502411 |
|    n_updates            | 8680           |
|    nu                   | 16.6           |
|    nu_loss              | -0.0695        |
|    policy_gradient_loss | -0.00364       |
|    reward_explained_... | 0.713          |
|    reward_value_loss    | 0.0193         |
|    std                  | 0.243          |
|    total_cost           | 43.0           |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.13e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.09          |
|    forward_reward       | 0.291         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.32         |
|    reward_forward       | 0.291         |
|    reward_survive       | 1             |
|    x_position           | -0.653        |
|    x_velocity           | 0.291         |
|    y_position           | 4.02          |
|    y_velocity           | 0.837         |
| rollout/                |               |
|    adjusted_reward      | 6.2           |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 2.9e+03       |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 436           |
|    time_elapsed         | 5647          |
|    total_timesteps      | 4464640       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10162         |
|    mean_motor0          | 0.8727223     |
|    mean_motor1          | 0.30404267    |
|    mean_motor2          | 0.6415845     |
|    mean_motor3          | 0.2654464     |
|    mean_motor4          | 0.65687853    |
|    mean_motor5          | 0.71353024    |
|    mean_motor6          | 0.4512601     |
|    mean_motor7          | 0.3981399     |
| train/                  |               |
|    approx_kl            | 0.11026466    |
|    average_cost         | 0.0010742188  |
|    clip_fraction        | 0.2           |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.773         |
|    cost_value_loss      | 0.00127       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.121         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00743       |
|    mean_cost_advantages | 0.00021258257 |
|    mean_reward_advan... | 0.008534873   |
|    n_updates            | 8700          |
|    nu                   | 16.6          |
|    nu_loss              | -0.0178       |
|    policy_gradient_loss | -0.00332      |
|    reward_explained_... | 0.618         |
|    reward_value_loss    | 0.0168        |
|    std                  | 0.243         |
|    total_cost           | 11.0          |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.04e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.73          |
|    forward_reward       | 0.155         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.21         |
|    reward_forward       | 0.155         |
|    reward_survive       | 1             |
|    x_position           | -1.33         |
|    x_velocity           | 0.155         |
|    y_position           | 6.56          |
|    y_velocity           | 0.221         |
| rollout/                |               |
|    adjusted_reward      | 5.9           |
|    ep_len_mean          | 492           |
|    ep_rew_mean          | 2.96e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 437           |
|    time_elapsed         | 5661          |
|    total_timesteps      | 4474880       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10148         |
|    mean_motor0          | 0.9299995     |
|    mean_motor1          | 0.3199242     |
|    mean_motor2          | 0.6629371     |
|    mean_motor3          | 0.28544307    |
|    mean_motor4          | 0.61348915    |
|    mean_motor5          | 0.5924555     |
|    mean_motor6          | 0.54898405    |
|    mean_motor7          | 0.4141399     |
| train/                  |               |
|    approx_kl            | 0.04818933    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.128         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.855         |
|    cost_value_loss      | 0.000115      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.132         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00636       |
|    mean_cost_advantages | -0.0008595125 |
|    mean_reward_advan... | 0.0077443644  |
|    n_updates            | 8720          |
|    nu                   | 16.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00147      |
|    reward_explained_... | 0.571         |
|    reward_value_loss    | 0.0193        |
|    std                  | 0.242         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-----------------------------------------
| eval/                   |             |
|    best_mean_reward     | 5e+03       |
|    mean_ep_length       | 462         |
|    mean_reward          | 2.19e+03    |
| infos/                  |             |
|    cost                 | 0           |
|    distance_from_origin | 5.4         |
|    forward_reward       | 0.274       |
|    reward_contact       | 0           |
|    reward_ctrl          | -1.33       |
|    reward_forward       | 0.274       |
|    reward_survive       | 1           |
|    x_position           | -0.413      |
|    x_velocity           | 0.274       |
|    y_position           | 5.38        |
|    y_velocity           | 0.682       |
| rollout/                |             |
|    adjusted_reward      | 5.81        |
|    ep_len_mean          | 496         |
|    ep_rew_mean          | 2.96e+03    |
| time/                   |             |
|    fps                  | 790         |
|    iterations           | 438         |
|    time_elapsed         | 5674        |
|    total_timesteps      | 4485120     |
| torque/                 |             |
|    greater_than_0.25    | 10240       |
|    greater_than_0.3     | 10238       |
|    greater_than_0.5     | 10143       |
|    mean_motor0          | 0.7787159   |
|    mean_motor1          | 0.2960915   |
|    mean_motor2          | 0.6425967   |
|    mean_motor3          | 0.2825037   |
|    mean_motor4          | 0.6200763   |
|    mean_motor5          | 0.6408113   |
|    mean_motor6          | 0.46578985  |
|    mean_motor7          | 0.44799966  |
| train/                  |             |
|    approx_kl            | 0.72224855  |
|    average_cost         | 0.032910157 |
|    clip_fraction        | 0.495       |
|    clip_range           | 0.4         |
|    cost_explained_va... | 0.956       |
|    cost_value_loss      | 0.0417      |
|    early_stop_epoch     | 20          |
|    entropy_loss         | 0.157       |
|    learning_rate        | 3e-05       |
|    loss                 | -0.0227     |
|    mean_cost_advantages | 0.026171874 |
|    mean_reward_advan... | 0.00913744  |
|    n_updates            | 8740        |
|    nu                   | 16.7        |
|    nu_loss              | -0.548      |
|    policy_gradient_loss | -0.0215     |
|    reward_explained_... | 0.836       |
|    reward_value_loss    | 0.0181      |
|    std                  | 0.241       |
|    total_cost           | 337.0       |
-----------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 324            |
|    mean_reward          | 1.91e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.08           |
|    forward_reward       | 0.451          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.18          |
|    reward_forward       | 0.451          |
|    reward_survive       | 1              |
|    x_position           | -0.532         |
|    x_velocity           | 0.451          |
|    y_position           | 5.02           |
|    y_velocity           | 1.02           |
| rollout/                |                |
|    adjusted_reward      | 5.96           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 2.97e+03       |
| time/                   |                |
|    fps                  | 790            |
|    iterations           | 439            |
|    time_elapsed         | 5686           |
|    total_timesteps      | 4495360        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10237          |
|    greater_than_0.5     | 10141          |
|    mean_motor0          | 0.7010073      |
|    mean_motor1          | 0.30288336     |
|    mean_motor2          | 0.6647378      |
|    mean_motor3          | 0.27950227     |
|    mean_motor4          | 0.6137246      |
|    mean_motor5          | 0.65683377     |
|    mean_motor6          | 0.4535831      |
|    mean_motor7          | 0.43258542     |
| train/                  |                |
|    approx_kl            | 0.04013973     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.102          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.759          |
|    cost_value_loss      | 8.02e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.189          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00796        |
|    mean_cost_advantages | -0.00052647275 |
|    mean_reward_advan... | -1.4477317e-05 |
|    n_updates            | 8760           |
|    nu                   | 16.7           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00118       |
|    reward_explained_... | 0.567          |
|    reward_value_loss    | 0.0192         |
|    std                  | 0.241          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 487          |
|    mean_reward          | 2.9e+03      |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.53         |
|    forward_reward       | 0.378        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.32        |
|    reward_forward       | 0.378        |
|    reward_survive       | 1            |
|    x_position           | -0.595       |
|    x_velocity           | 0.378        |
|    y_position           | 5.45         |
|    y_velocity           | 0.892        |
| rollout/                |              |
|    adjusted_reward      | 5.84         |
|    ep_len_mean          | 493          |
|    ep_rew_mean          | 2.95e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 440          |
|    time_elapsed         | 5700         |
|    total_timesteps      | 4505600      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10238        |
|    greater_than_0.5     | 10155        |
|    mean_motor0          | 0.96392715   |
|    mean_motor1          | 0.31491095   |
|    mean_motor2          | 0.6597765    |
|    mean_motor3          | 0.2839979    |
|    mean_motor4          | 0.6657473    |
|    mean_motor5          | 0.6466624    |
|    mean_motor6          | 0.43077606   |
|    mean_motor7          | 0.44392118   |
| train/                  |              |
|    approx_kl            | 0.23504457   |
|    average_cost         | 0.011132812  |
|    clip_fraction        | 0.319        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.511        |
|    cost_value_loss      | 0.00664      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.2          |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0144       |
|    mean_cost_advantages | 0.014790198  |
|    mean_reward_advan... | 0.0046442766 |
|    n_updates            | 8780         |
|    nu                   | 16.7         |
|    nu_loss              | -0.186       |
|    policy_gradient_loss | -0.00601     |
|    reward_explained_... | 0.622        |
|    reward_value_loss    | 0.0203       |
|    std                  | 0.24         |
|    total_cost           | 114.0        |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.72e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.41           |
|    forward_reward       | 0.223          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.14          |
|    reward_forward       | 0.223          |
|    reward_survive       | 1              |
|    x_position           | -0.481         |
|    x_velocity           | 0.223          |
|    y_position           | 6.39           |
|    y_velocity           | 0.458          |
| rollout/                |                |
|    adjusted_reward      | 5.54           |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 2.89e+03       |
| time/                   |                |
|    fps                  | 790            |
|    iterations           | 441            |
|    time_elapsed         | 5713           |
|    total_timesteps      | 4515840        |
| torque/                 |                |
|    greater_than_0.25    | 10239          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 10126          |
|    mean_motor0          | 0.8436788      |
|    mean_motor1          | 0.34686044     |
|    mean_motor2          | 0.61578566     |
|    mean_motor3          | 0.30200076     |
|    mean_motor4          | 0.6644371      |
|    mean_motor5          | 0.66778266     |
|    mean_motor6          | 0.4398231      |
|    mean_motor7          | 0.46813416     |
| train/                  |                |
|    approx_kl            | 0.13972673     |
|    average_cost         | 0.004003906    |
|    clip_fraction        | 0.233          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.794          |
|    cost_value_loss      | 0.00577        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.216          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00148        |
|    mean_cost_advantages | 0.0012306437   |
|    mean_reward_advan... | -9.8126664e-05 |
|    n_updates            | 8800           |
|    nu                   | 16.8           |
|    nu_loss              | -0.0671        |
|    policy_gradient_loss | -0.00478       |
|    reward_explained_... | 0.573          |
|    reward_value_loss    | 0.0202         |
|    std                  | 0.24           |
|    total_cost           | 41.0           |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.54e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.27         |
|    forward_reward       | 0.147        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.52        |
|    reward_forward       | 0.147        |
|    reward_survive       | 1            |
|    x_position           | -1.86        |
|    x_velocity           | 0.147        |
|    y_position           | 7            |
|    y_velocity           | 0.152        |
| rollout/                |              |
|    adjusted_reward      | 5.71         |
|    ep_len_mean          | 493          |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 442          |
|    time_elapsed         | 5726         |
|    total_timesteps      | 4526080      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10134        |
|    mean_motor0          | 0.7633084    |
|    mean_motor1          | 0.37364715   |
|    mean_motor2          | 0.67614436   |
|    mean_motor3          | 0.2918275    |
|    mean_motor4          | 0.70222443   |
|    mean_motor5          | 0.6976488    |
|    mean_motor6          | 0.39357084   |
|    mean_motor7          | 0.44406167   |
| train/                  |              |
|    approx_kl            | 0.42168474   |
|    average_cost         | 0.017285157  |
|    clip_fraction        | 0.326        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.607        |
|    cost_value_loss      | 0.0137       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.233        |
|    learning_rate        | 3e-05        |
|    loss                 | -0.0349      |
|    mean_cost_advantages | 0.023337804  |
|    mean_reward_advan... | -0.005046442 |
|    n_updates            | 8820         |
|    nu                   | 16.8         |
|    nu_loss              | -0.29        |
|    policy_gradient_loss | -0.0182      |
|    reward_explained_... | 0.515        |
|    reward_value_loss    | 0.0194       |
|    std                  | 0.24         |
|    total_cost           | 177.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 442           |
|    mean_reward          | 2.54e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.5           |
|    forward_reward       | 0.131         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.25         |
|    reward_forward       | 0.131         |
|    reward_survive       | 1             |
|    x_position           | -1.06         |
|    x_velocity           | 0.131         |
|    y_position           | 6.33          |
|    y_velocity           | 0.222         |
| rollout/                |               |
|    adjusted_reward      | 5.85          |
|    ep_len_mean          | 493           |
|    ep_rew_mean          | 2.9e+03       |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 443           |
|    time_elapsed         | 5740          |
|    total_timesteps      | 4536320       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10240         |
|    greater_than_0.5     | 10121         |
|    mean_motor0          | 0.6633181     |
|    mean_motor1          | 0.3674406     |
|    mean_motor2          | 0.67167974    |
|    mean_motor3          | 0.36577588    |
|    mean_motor4          | 0.6519812     |
|    mean_motor5          | 0.5606618     |
|    mean_motor6          | 0.4482441     |
|    mean_motor7          | 0.40673047    |
| train/                  |               |
|    approx_kl            | 1.6862217     |
|    average_cost         | 0.041796874   |
|    clip_fraction        | 0.619         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.762         |
|    cost_value_loss      | 0.035         |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.251         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00912      |
|    mean_cost_advantages | 0.053035952   |
|    mean_reward_advan... | -0.0030543308 |
|    n_updates            | 8840          |
|    nu                   | 16.9          |
|    nu_loss              | -0.703        |
|    policy_gradient_loss | -0.0332       |
|    reward_explained_... | 0.527         |
|    reward_value_loss    | 0.021         |
|    std                  | 0.238         |
|    total_cost           | 428.0         |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 452           |
|    mean_reward          | 2.54e+03      |
| infos/                  |               |
|    cost                 | 0.0331        |
|    distance_from_origin | 6.01          |
|    forward_reward       | 0.179         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.54         |
|    reward_forward       | 0.179         |
|    reward_survive       | 1             |
|    x_position           | -1.55         |
|    x_velocity           | 0.179         |
|    y_position           | 5.32          |
|    y_velocity           | 0.246         |
| rollout/                |               |
|    adjusted_reward      | 5.58          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 2.91e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 444           |
|    time_elapsed         | 5753          |
|    total_timesteps      | 4546560       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10158         |
|    mean_motor0          | 0.69024867    |
|    mean_motor1          | 0.3575291     |
|    mean_motor2          | 0.57974243    |
|    mean_motor3          | 0.4371822     |
|    mean_motor4          | 0.629579      |
|    mean_motor5          | 0.7067064     |
|    mean_motor6          | 0.50362       |
|    mean_motor7          | 0.45436698    |
| train/                  |               |
|    approx_kl            | 2.4899602     |
|    average_cost         | 0.09765625    |
|    clip_fraction        | 0.569         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.923         |
|    cost_value_loss      | 0.113         |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.292         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.803        |
|    mean_cost_advantages | 0.09005013    |
|    mean_reward_advan... | -0.0034242324 |
|    n_updates            | 8860          |
|    nu                   | 17            |
|    nu_loss              | -1.65         |
|    policy_gradient_loss | -0.0639       |
|    reward_explained_... | 0.699         |
|    reward_value_loss    | 0.0258        |
|    std                  | 0.237         |
|    total_cost           | 1000.0        |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.77e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 8.15          |
|    forward_reward       | 0.126         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.47         |
|    reward_forward       | 0.126         |
|    reward_survive       | 1             |
|    x_position           | -1.51         |
|    x_velocity           | 0.126         |
|    y_position           | 7.98          |
|    y_velocity           | 0.167         |
| rollout/                |               |
|    adjusted_reward      | 5.72          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 2.86e+03      |
| time/                   |               |
|    fps                  | 790           |
|    iterations           | 445           |
|    time_elapsed         | 5766          |
|    total_timesteps      | 4556800       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 10123         |
|    mean_motor0          | 0.61111224    |
|    mean_motor1          | 0.3604154     |
|    mean_motor2          | 0.6047495     |
|    mean_motor3          | 0.3775764     |
|    mean_motor4          | 0.62439275    |
|    mean_motor5          | 0.778349      |
|    mean_motor6          | 0.42946345    |
|    mean_motor7          | 0.43709493    |
| train/                  |               |
|    approx_kl            | 0.78528035    |
|    average_cost         | 0.026757812   |
|    clip_fraction        | 0.49          |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.843         |
|    cost_value_loss      | 0.0272        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.305         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.135        |
|    mean_cost_advantages | 0.024776911   |
|    mean_reward_advan... | -0.0055352813 |
|    n_updates            | 8880          |
|    nu                   | 17.1          |
|    nu_loss              | -0.454        |
|    policy_gradient_loss | -0.0196       |
|    reward_explained_... | 0.867         |
|    reward_value_loss    | 0.0242        |
|    std                  | 0.238         |
|    total_cost           | 274.0         |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.48e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.8          |
|    forward_reward       | 0.134        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.29        |
|    reward_forward       | 0.134        |
|    reward_survive       | 1            |
|    x_position           | -0.825       |
|    x_velocity           | 0.134        |
|    y_position           | 6.7          |
|    y_velocity           | 0.315        |
| rollout/                |              |
|    adjusted_reward      | 5.39         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 2.87e+03     |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 446          |
|    time_elapsed         | 5780         |
|    total_timesteps      | 4567040      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10145        |
|    mean_motor0          | 0.664554     |
|    mean_motor1          | 0.34722084   |
|    mean_motor2          | 0.60897565   |
|    mean_motor3          | 0.4034752    |
|    mean_motor4          | 0.69283617   |
|    mean_motor5          | 0.6630572    |
|    mean_motor6          | 0.5091924    |
|    mean_motor7          | 0.47912398   |
| train/                  |              |
|    approx_kl            | 0.7590395    |
|    average_cost         | 0.039257813  |
|    clip_fraction        | 0.407        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.945        |
|    cost_value_loss      | 0.0686       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.316        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0046       |
|    mean_cost_advantages | 0.023944367  |
|    mean_reward_advan... | 0.0022017718 |
|    n_updates            | 8900         |
|    nu                   | 17.2         |
|    nu_loss              | -0.67        |
|    policy_gradient_loss | -0.0202      |
|    reward_explained_... | 0.852        |
|    reward_value_loss    | 0.0205       |
|    std                  | 0.237        |
|    total_cost           | 402.0        |
------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.71e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.24         |
|    forward_reward       | 0.25         |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.41        |
|    reward_forward       | 0.25         |
|    reward_survive       | 1            |
|    x_position           | -0.44        |
|    x_velocity           | 0.25         |
|    y_position           | 6.14         |
|    y_velocity           | 0.439        |
| rollout/                |              |
|    adjusted_reward      | 4.9          |
|    ep_len_mean          | 499          |
|    ep_rew_mean          | 2.8e+03      |
| time/                   |              |
|    fps                  | 790          |
|    iterations           | 447          |
|    time_elapsed         | 5793         |
|    total_timesteps      | 4577280      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10148        |
|    mean_motor0          | 0.70088583   |
|    mean_motor1          | 0.36818      |
|    mean_motor2          | 0.65076864   |
|    mean_motor3          | 0.4596036    |
|    mean_motor4          | 0.775303     |
|    mean_motor5          | 0.65911907   |
|    mean_motor6          | 0.58964145   |
|    mean_motor7          | 0.50198793   |
| train/                  |              |
|    approx_kl            | 0.17174344   |
|    average_cost         | 0.0078125    |
|    clip_fraction        | 0.263        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.87         |
|    cost_value_loss      | 0.0186       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.343        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00968      |
|    mean_cost_advantages | 0.0038427792 |
|    mean_reward_advan... | -0.004058105 |
|    n_updates            | 8920         |
|    nu                   | 17.3         |
|    nu_loss              | -0.134       |
|    policy_gradient_loss | -0.00832     |
|    reward_explained_... | 0.869        |
|    reward_value_loss    | 0.0213       |
|    std                  | 0.237        |
|    total_cost           | 80.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.08e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 4.94          |
|    forward_reward       | 0.348         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.35         |
|    reward_forward       | 0.348         |
|    reward_survive       | 1             |
|    x_position           | -0.386        |
|    x_velocity           | 0.348         |
|    y_position           | 4.84          |
|    y_velocity           | 0.419         |
| rollout/                |               |
|    adjusted_reward      | 5.67          |
|    ep_len_mean          | 499           |
|    ep_rew_mean          | 2.76e+03      |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 448           |
|    time_elapsed         | 5807          |
|    total_timesteps      | 4587520       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 10167         |
|    mean_motor0          | 0.67108357    |
|    mean_motor1          | 0.32022193    |
|    mean_motor2          | 0.64638317    |
|    mean_motor3          | 0.37289008    |
|    mean_motor4          | 0.6545476     |
|    mean_motor5          | 0.66347456    |
|    mean_motor6          | 0.5349692     |
|    mean_motor7          | 0.5419972     |
| train/                  |               |
|    approx_kl            | 1.603625      |
|    average_cost         | 0.051953126   |
|    clip_fraction        | 0.513         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.945         |
|    cost_value_loss      | 0.0478        |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.36          |
|    learning_rate        | 3e-05         |
|    loss                 | -0.00784      |
|    mean_cost_advantages | 0.044675015   |
|    mean_reward_advan... | -0.0061676335 |
|    n_updates            | 8940          |
|    nu                   | 17.4          |
|    nu_loss              | -0.898        |
|    policy_gradient_loss | -0.0427       |
|    reward_explained_... | 0.88          |
|    reward_value_loss    | 0.02          |
|    std                  | 0.236         |
|    total_cost           | 532.0         |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.04e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.43          |
|    forward_reward       | 0.251         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.17         |
|    reward_forward       | 0.251         |
|    reward_survive       | 1             |
|    x_position           | -0.311        |
|    x_velocity           | 0.251         |
|    y_position           | 6.37          |
|    y_velocity           | 0.613         |
| rollout/                |               |
|    adjusted_reward      | 5.55          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.73e+03      |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 449           |
|    time_elapsed         | 5820          |
|    total_timesteps      | 4597760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10151         |
|    mean_motor0          | 0.54905784    |
|    mean_motor1          | 0.35086578    |
|    mean_motor2          | 0.5829554     |
|    mean_motor3          | 0.41748634    |
|    mean_motor4          | 0.5531158     |
|    mean_motor5          | 0.6384458     |
|    mean_motor6          | 0.59283775    |
|    mean_motor7          | 0.48010325    |
| train/                  |               |
|    approx_kl            | 0.31683102    |
|    average_cost         | 0.029199218   |
|    clip_fraction        | 0.324         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.978         |
|    cost_value_loss      | 0.025         |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.387         |
|    learning_rate        | 3e-05         |
|    loss                 | -0.000715     |
|    mean_cost_advantages | 0.018325329   |
|    mean_reward_advan... | -0.0018622514 |
|    n_updates            | 8960          |
|    nu                   | 17.5          |
|    nu_loss              | -0.508        |
|    policy_gradient_loss | -0.00929      |
|    reward_explained_... | 0.596         |
|    reward_value_loss    | 0.0221        |
|    std                  | 0.235         |
|    total_cost           | 299.0         |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.11e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.07         |
|    forward_reward       | 0.243        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.52        |
|    reward_forward       | 0.243        |
|    reward_survive       | 1            |
|    x_position           | -0.379       |
|    x_velocity           | 0.243        |
|    y_position           | 4.87         |
|    y_velocity           | 0.454        |
| rollout/                |              |
|    adjusted_reward      | 5.48         |
|    ep_len_mean          | 493          |
|    ep_rew_mean          | 2.69e+03     |
| time/                   |              |
|    fps                  | 789          |
|    iterations           | 450          |
|    time_elapsed         | 5834         |
|    total_timesteps      | 4608000      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10143        |
|    mean_motor0          | 0.57171667   |
|    mean_motor1          | 0.3716342    |
|    mean_motor2          | 0.57202935   |
|    mean_motor3          | 0.41691345   |
|    mean_motor4          | 0.5822575    |
|    mean_motor5          | 0.6472401    |
|    mean_motor6          | 0.53393275   |
|    mean_motor7          | 0.47167626   |
| train/                  |              |
|    approx_kl            | 0.062473185  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.126        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.85         |
|    cost_value_loss      | 0.000131     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.413        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00324      |
|    mean_cost_advantages | 0.0003262031 |
|    mean_reward_advan... | 0.0028480208 |
|    n_updates            | 8980         |
|    nu                   | 17.6         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.00162     |
|    reward_explained_... | 0.853        |
|    reward_value_loss    | 0.0166       |
|    std                  | 0.234        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3e+03         |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.12          |
|    forward_reward       | 0.303         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.12         |
|    reward_forward       | 0.303         |
|    reward_survive       | 1             |
|    x_position           | -0.679        |
|    x_velocity           | 0.303         |
|    y_position           | 6.07          |
|    y_velocity           | 0.547         |
| rollout/                |               |
|    adjusted_reward      | 5.59          |
|    ep_len_mean          | 494           |
|    ep_rew_mean          | 2.71e+03      |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 451           |
|    time_elapsed         | 5847          |
|    total_timesteps      | 4618240       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10141         |
|    mean_motor0          | 0.58889455    |
|    mean_motor1          | 0.32739195    |
|    mean_motor2          | 0.59627146    |
|    mean_motor3          | 0.4264005     |
|    mean_motor4          | 0.56341016    |
|    mean_motor5          | 0.621261      |
|    mean_motor6          | 0.53135544    |
|    mean_motor7          | 0.47742137    |
| train/                  |               |
|    approx_kl            | 0.19138174    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.245         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.93          |
|    cost_value_loss      | 0.00112       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.442         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00237       |
|    mean_cost_advantages | -0.0052456926 |
|    mean_reward_advan... | 0.0004547395  |
|    n_updates            | 9000          |
|    nu                   | 17.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.0042       |
|    reward_explained_... | 0.887         |
|    reward_value_loss    | 0.0187        |
|    std                  | 0.233         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 2.78e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 4.97         |
|    forward_reward       | 0.212        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.36        |
|    reward_forward       | 0.212        |
|    reward_survive       | 1            |
|    x_position           | 0.0643       |
|    x_velocity           | 0.212        |
|    y_position           | 4.95         |
|    y_velocity           | 0.562        |
| rollout/                |              |
|    adjusted_reward      | 5.51         |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 2.76e+03     |
| time/                   |              |
|    fps                  | 789          |
|    iterations           | 452          |
|    time_elapsed         | 5861         |
|    total_timesteps      | 4628480      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10142        |
|    mean_motor0          | 0.5125479    |
|    mean_motor1          | 0.32963437   |
|    mean_motor2          | 0.5915386    |
|    mean_motor3          | 0.4086658    |
|    mean_motor4          | 0.59646136   |
|    mean_motor5          | 0.6978702    |
|    mean_motor6          | 0.53873575   |
|    mean_motor7          | 0.51015127   |
| train/                  |              |
|    approx_kl            | 0.117920116  |
|    average_cost         | 0.0036132813 |
|    clip_fraction        | 0.225        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.918        |
|    cost_value_loss      | 0.00597      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.479        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0112       |
|    mean_cost_advantages | -0.000494999 |
|    mean_reward_advan... | 0.0014461193 |
|    n_updates            | 9020         |
|    nu                   | 17.8         |
|    nu_loss              | -0.064       |
|    policy_gradient_loss | -0.00558     |
|    reward_explained_... | 0.834        |
|    reward_value_loss    | 0.0164       |
|    std                  | 0.232        |
|    total_cost           | 37.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 425            |
|    mean_reward          | 2.54e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.71           |
|    forward_reward       | 0.145          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.39          |
|    reward_forward       | 0.145          |
|    reward_survive       | 1              |
|    x_position           | -0.609         |
|    x_velocity           | 0.145          |
|    y_position           | 6.67           |
|    y_velocity           | 0.195          |
| rollout/                |                |
|    adjusted_reward      | 6.04           |
|    ep_len_mean          | 494            |
|    ep_rew_mean          | 2.78e+03       |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 453            |
|    time_elapsed         | 5874           |
|    total_timesteps      | 4638720        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10120          |
|    mean_motor0          | 0.56346583     |
|    mean_motor1          | 0.31173754     |
|    mean_motor2          | 0.5865711      |
|    mean_motor3          | 0.39477158     |
|    mean_motor4          | 0.5881641      |
|    mean_motor5          | 0.7064287      |
|    mean_motor6          | 0.5028645      |
|    mean_motor7          | 0.47926217     |
| train/                  |                |
|    approx_kl            | 0.05594306     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.112          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.865          |
|    cost_value_loss      | 4.85e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.504          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00891        |
|    mean_cost_advantages | -4.7932176e-06 |
|    mean_reward_advan... | -0.0030076092  |
|    n_updates            | 9040           |
|    nu                   | 17.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00135       |
|    reward_explained_... | 0.825          |
|    reward_value_loss    | 0.0183         |
|    std                  | 0.232          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.03e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.68         |
|    forward_reward       | 0.276        |
|    reward_contact       | 0            |
|    reward_ctrl          | -1.43        |
|    reward_forward       | 0.276        |
|    reward_survive       | 1            |
|    x_position           | -0.108       |
|    x_velocity           | 0.276        |
|    y_position           | 7.66         |
|    y_velocity           | 0.339        |
| rollout/                |              |
|    adjusted_reward      | 6.19         |
|    ep_len_mean          | 490          |
|    ep_rew_mean          | 2.82e+03     |
| time/                   |              |
|    fps                  | 789          |
|    iterations           | 454          |
|    time_elapsed         | 5887         |
|    total_timesteps      | 4648960      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10239        |
|    greater_than_0.5     | 10141        |
|    mean_motor0          | 0.5139122    |
|    mean_motor1          | 0.3340539    |
|    mean_motor2          | 0.5858375    |
|    mean_motor3          | 0.4049105    |
|    mean_motor4          | 0.5136337    |
|    mean_motor5          | 0.69217426   |
|    mean_motor6          | 0.50977457   |
|    mean_motor7          | 0.48145348   |
| train/                  |              |
|    approx_kl            | 0.20644088   |
|    average_cost         | 0.0065429686 |
|    clip_fraction        | 0.283        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.936        |
|    cost_value_loss      | 0.00312      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.51         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0103       |
|    mean_cost_advantages | 0.0055746255 |
|    mean_reward_advan... | 0.007160805  |
|    n_updates            | 9060         |
|    nu                   | 18           |
|    nu_loss              | -0.117       |
|    policy_gradient_loss | -0.00495     |
|    reward_explained_... | 0.679        |
|    reward_value_loss    | 0.0193       |
|    std                  | 0.232        |
|    total_cost           | 67.0         |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.94e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 7.02           |
|    forward_reward       | 0.21           |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.28          |
|    reward_forward       | 0.21           |
|    reward_survive       | 1              |
|    x_position           | -0.271         |
|    x_velocity           | 0.21           |
|    y_position           | 6.97           |
|    y_velocity           | 0.366          |
| rollout/                |                |
|    adjusted_reward      | 6              |
|    ep_len_mean          | 493            |
|    ep_rew_mean          | 2.88e+03       |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 455            |
|    time_elapsed         | 5901           |
|    total_timesteps      | 4659200        |
| torque/                 |                |
|    greater_than_0.25    | 10238          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 10130          |
|    mean_motor0          | 0.517315       |
|    mean_motor1          | 0.34281963     |
|    mean_motor2          | 0.5843535      |
|    mean_motor3          | 0.4208989      |
|    mean_motor4          | 0.5075712      |
|    mean_motor5          | 0.6034091      |
|    mean_motor6          | 0.518518       |
|    mean_motor7          | 0.45796242     |
| train/                  |                |
|    approx_kl            | 0.05079208     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.121          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.72           |
|    cost_value_loss      | 0.000373       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.531          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0231         |
|    mean_cost_advantages | -0.00034068973 |
|    mean_reward_advan... | 0.012940357    |
|    n_updates            | 9080           |
|    nu                   | 18             |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00172       |
|    reward_explained_... | 0.646          |
|    reward_value_loss    | 0.0173         |
|    std                  | 0.231          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.97e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.24          |
|    forward_reward       | 0.0846        |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.1          |
|    reward_forward       | 0.0846        |
|    reward_survive       | 1             |
|    x_position           | 0.0698        |
|    x_velocity           | 0.0846        |
|    y_position           | 7.14          |
|    y_velocity           | 0.18          |
| rollout/                |               |
|    adjusted_reward      | 5.9           |
|    ep_len_mean          | 488           |
|    ep_rew_mean          | 2.89e+03      |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 456           |
|    time_elapsed         | 5914          |
|    total_timesteps      | 4669440       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10100         |
|    mean_motor0          | 0.4807821     |
|    mean_motor1          | 0.34719184    |
|    mean_motor2          | 0.57231104    |
|    mean_motor3          | 0.44325304    |
|    mean_motor4          | 0.44899884    |
|    mean_motor5          | 0.55067253    |
|    mean_motor6          | 0.52204186    |
|    mean_motor7          | 0.47186032    |
| train/                  |               |
|    approx_kl            | 0.054083306   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0952        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.821         |
|    cost_value_loss      | 0.000195      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.56          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00192       |
|    mean_cost_advantages | -0.0004389889 |
|    mean_reward_advan... | 0.012792682   |
|    n_updates            | 9100          |
|    nu                   | 18.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00143      |
|    reward_explained_... | 0.865         |
|    reward_value_loss    | 0.0165        |
|    std                  | 0.23          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.17e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 7.58          |
|    forward_reward       | 0.13          |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.06         |
|    reward_forward       | 0.13          |
|    reward_survive       | 1             |
|    x_position           | -0.677        |
|    x_velocity           | 0.13          |
|    y_position           | 7.52          |
|    y_velocity           | 0.299         |
| rollout/                |               |
|    adjusted_reward      | 6.27          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 457           |
|    time_elapsed         | 5927          |
|    total_timesteps      | 4679680       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10111         |
|    mean_motor0          | 0.48664397    |
|    mean_motor1          | 0.32202643    |
|    mean_motor2          | 0.6077875     |
|    mean_motor3          | 0.40258756    |
|    mean_motor4          | 0.51037556    |
|    mean_motor5          | 0.6090656     |
|    mean_motor6          | 0.49729362    |
|    mean_motor7          | 0.47561568    |
| train/                  |               |
|    approx_kl            | 0.106880546   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.183         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.874         |
|    cost_value_loss      | 0.000749      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.587         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0142        |
|    mean_cost_advantages | -0.0017623423 |
|    mean_reward_advan... | 0.009200878   |
|    n_updates            | 9120          |
|    nu                   | 18.1          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00332      |
|    reward_explained_... | 0.856         |
|    reward_value_loss    | 0.0173        |
|    std                  | 0.229         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.17e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.55           |
|    forward_reward       | 0.167          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.09          |
|    reward_forward       | 0.167          |
|    reward_survive       | 1              |
|    x_position           | -0.558         |
|    x_velocity           | 0.167          |
|    y_position           | 6.5            |
|    y_velocity           | 0.45           |
| rollout/                |                |
|    adjusted_reward      | 6.15           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3e+03          |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 458            |
|    time_elapsed         | 5941           |
|    total_timesteps      | 4689920        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 10079          |
|    mean_motor0          | 0.4080851      |
|    mean_motor1          | 0.34499627     |
|    mean_motor2          | 0.58195055     |
|    mean_motor3          | 0.43287414     |
|    mean_motor4          | 0.42381516     |
|    mean_motor5          | 0.5047039      |
|    mean_motor6          | 0.50248355     |
|    mean_motor7          | 0.4554062      |
| train/                  |                |
|    approx_kl            | 0.055803817    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.16           |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.792          |
|    cost_value_loss      | 0.000251       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.62           |
|    learning_rate        | 3e-05          |
|    loss                 | -0.00116       |
|    mean_cost_advantages | -0.00090618094 |
|    mean_reward_advan... | 0.00853567     |
|    n_updates            | 9140           |
|    nu                   | 18.2           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00173       |
|    reward_explained_... | 0.586          |
|    reward_value_loss    | 0.0178         |
|    std                  | 0.228          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.24e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.97           |
|    forward_reward       | 0.121          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.01          |
|    reward_forward       | 0.121          |
|    reward_survive       | 1              |
|    x_position           | 0.121          |
|    x_velocity           | 0.121          |
|    y_position           | 6.95           |
|    y_velocity           | 0.244          |
| rollout/                |                |
|    adjusted_reward      | 6.36           |
|    ep_len_mean          | 492            |
|    ep_rew_mean          | 3.01e+03       |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 459            |
|    time_elapsed         | 5954           |
|    total_timesteps      | 4700160        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10056          |
|    mean_motor0          | 0.4170937      |
|    mean_motor1          | 0.3266733      |
|    mean_motor2          | 0.5887316      |
|    mean_motor3          | 0.43094072     |
|    mean_motor4          | 0.4269821      |
|    mean_motor5          | 0.52449936     |
|    mean_motor6          | 0.47920078     |
|    mean_motor7          | 0.43936986     |
| train/                  |                |
|    approx_kl            | 0.03868847     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0888         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.723          |
|    cost_value_loss      | 2.07e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.652          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00442        |
|    mean_cost_advantages | -0.00036453776 |
|    mean_reward_advan... | 0.0028149113   |
|    n_updates            | 9160           |
|    nu                   | 18.2           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000966      |
|    reward_explained_... | 0.633          |
|    reward_value_loss    | 0.0198         |
|    std                  | 0.227          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.17e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.21           |
|    forward_reward       | 0.235          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.16          |
|    reward_forward       | 0.235          |
|    reward_survive       | 1              |
|    x_position           | -0.382         |
|    x_velocity           | 0.235          |
|    y_position           | 6.18           |
|    y_velocity           | 0.353          |
| rollout/                |                |
|    adjusted_reward      | 6.37           |
|    ep_len_mean          | 496            |
|    ep_rew_mean          | 3.08e+03       |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 460            |
|    time_elapsed         | 5968           |
|    total_timesteps      | 4710400        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10239          |
|    greater_than_0.5     | 10072          |
|    mean_motor0          | 0.4074575      |
|    mean_motor1          | 0.32899266     |
|    mean_motor2          | 0.58636725     |
|    mean_motor3          | 0.42946902     |
|    mean_motor4          | 0.3985321      |
|    mean_motor5          | 0.51731646     |
|    mean_motor6          | 0.48456192     |
|    mean_motor7          | 0.44022304     |
| train/                  |                |
|    approx_kl            | 0.043932218    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.106          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.777          |
|    cost_value_loss      | 3.22e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.681          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00592        |
|    mean_cost_advantages | -0.00086804153 |
|    mean_reward_advan... | 0.0063273944   |
|    n_updates            | 9180           |
|    nu                   | 18.3           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000918      |
|    reward_explained_... | 0.552          |
|    reward_value_loss    | 0.0195         |
|    std                  | 0.227          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.07e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 7.26         |
|    forward_reward       | 0.0825       |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.989       |
|    reward_forward       | 0.0825       |
|    reward_survive       | 1            |
|    x_position           | -0.869       |
|    x_velocity           | 0.0825       |
|    y_position           | 7.16         |
|    y_velocity           | 0.135        |
| rollout/                |              |
|    adjusted_reward      | 6.34         |
|    ep_len_mean          | 488          |
|    ep_rew_mean          | 3.08e+03     |
| time/                   |              |
|    fps                  | 789          |
|    iterations           | 461          |
|    time_elapsed         | 5981         |
|    total_timesteps      | 4720640      |
| torque/                 |              |
|    greater_than_0.25    | 10240        |
|    greater_than_0.3     | 10240        |
|    greater_than_0.5     | 10037        |
|    mean_motor0          | 0.41745296   |
|    mean_motor1          | 0.3299238    |
|    mean_motor2          | 0.5751764    |
|    mean_motor3          | 0.43087435   |
|    mean_motor4          | 0.40210348   |
|    mean_motor5          | 0.5115529    |
|    mean_motor6          | 0.45517737   |
|    mean_motor7          | 0.42838746   |
| train/                  |              |
|    approx_kl            | 0.059589554  |
|    average_cost         | 0.0012695312 |
|    clip_fraction        | 0.137        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.0372       |
|    cost_value_loss      | 0.000267     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.707        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00728      |
|    mean_cost_advantages | 0.0014548871 |
|    mean_reward_advan... | 0.0055339597 |
|    n_updates            | 9200         |
|    nu                   | 18.3         |
|    nu_loss              | -0.0232      |
|    policy_gradient_loss | -0.00176     |
|    reward_explained_... | 0.6          |
|    reward_value_loss    | 0.0195       |
|    std                  | 0.226        |
|    total_cost           | 13.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 421           |
|    mean_reward          | 2.53e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.88          |
|    forward_reward       | 0.185         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.23         |
|    reward_forward       | 0.185         |
|    reward_survive       | 1             |
|    x_position           | -0.911        |
|    x_velocity           | 0.185         |
|    y_position           | 6.79          |
|    y_velocity           | 0.405         |
| rollout/                |               |
|    adjusted_reward      | 6.28          |
|    ep_len_mean          | 487           |
|    ep_rew_mean          | 3.08e+03      |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 462           |
|    time_elapsed         | 5994          |
|    total_timesteps      | 4730880       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10238         |
|    greater_than_0.5     | 10025         |
|    mean_motor0          | 0.42130995    |
|    mean_motor1          | 0.33202085    |
|    mean_motor2          | 0.569892      |
|    mean_motor3          | 0.43757567    |
|    mean_motor4          | 0.4232604     |
|    mean_motor5          | 0.5074661     |
|    mean_motor6          | 0.44168243    |
|    mean_motor7          | 0.43547946    |
| train/                  |               |
|    approx_kl            | 0.035757888   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.106         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.681         |
|    cost_value_loss      | 3.64e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.729         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0151        |
|    mean_cost_advantages | -0.0010974118 |
|    mean_reward_advan... | 0.0022430145  |
|    n_updates            | 9220          |
|    nu                   | 18.3          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00112      |
|    reward_explained_... | 0.649         |
|    reward_value_loss    | 0.0208        |
|    std                  | 0.225         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.2e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.63           |
|    forward_reward       | 0.219          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.08          |
|    reward_forward       | 0.219          |
|    reward_survive       | 1              |
|    x_position           | -0.205         |
|    x_velocity           | 0.219          |
|    y_position           | 5.52           |
|    y_velocity           | 0.319          |
| rollout/                |                |
|    adjusted_reward      | 6.16           |
|    ep_len_mean          | 487            |
|    ep_rew_mean          | 3.07e+03       |
| time/                   |                |
|    fps                  | 789            |
|    iterations           | 463            |
|    time_elapsed         | 6008           |
|    total_timesteps      | 4741120        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10238          |
|    greater_than_0.5     | 10056          |
|    mean_motor0          | 0.41018072     |
|    mean_motor1          | 0.35970134     |
|    mean_motor2          | 0.5803107      |
|    mean_motor3          | 0.45855355     |
|    mean_motor4          | 0.404238       |
|    mean_motor5          | 0.4880568      |
|    mean_motor6          | 0.41534233     |
|    mean_motor7          | 0.45783305     |
| train/                  |                |
|    approx_kl            | 0.0514822      |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.112          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.836          |
|    cost_value_loss      | 1.3e-05        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.754          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00424        |
|    mean_cost_advantages | -0.001186239   |
|    mean_reward_advan... | -0.00012313128 |
|    n_updates            | 9240           |
|    nu                   | 18.4           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00104       |
|    reward_explained_... | 0.625          |
|    reward_value_loss    | 0.0209         |
|    std                  | 0.224          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.03e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.66          |
|    forward_reward       | 0.176         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.02         |
|    reward_forward       | 0.176         |
|    reward_survive       | 1             |
|    x_position           | -0.701        |
|    x_velocity           | 0.176         |
|    y_position           | 6.6           |
|    y_velocity           | 0.354         |
| rollout/                |               |
|    adjusted_reward      | 5.93          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 3e+03         |
| time/                   |               |
|    fps                  | 789           |
|    iterations           | 464           |
|    time_elapsed         | 6021          |
|    total_timesteps      | 4751360       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 10017         |
|    mean_motor0          | 0.40005603    |
|    mean_motor1          | 0.36433682    |
|    mean_motor2          | 0.5757359     |
|    mean_motor3          | 0.45963058    |
|    mean_motor4          | 0.40871185    |
|    mean_motor5          | 0.46379814    |
|    mean_motor6          | 0.43391457    |
|    mean_motor7          | 0.45736367    |
| train/                  |               |
|    approx_kl            | 0.03411435    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0797        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.804         |
|    cost_value_loss      | 7.11e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.778         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00932       |
|    mean_cost_advantages | -0.0008492113 |
|    mean_reward_advan... | -0.0056695873 |
|    n_updates            | 9260          |
|    nu                   | 18.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000824     |
|    reward_explained_... | 0.613         |
|    reward_value_loss    | 0.0215        |
|    std                  | 0.224         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.03e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 3.38          |
|    forward_reward       | 0.259         |
|    reward_contact       | 0             |
|    reward_ctrl          | -1.21         |
|    reward_forward       | 0.259         |
|    reward_survive       | 1             |
|    x_position           | 0.59          |
|    x_velocity           | 0.259         |
|    y_position           | 3.02          |
|    y_velocity           | 0.751         |
| rollout/                |               |
|    adjusted_reward      | 5.98          |
|    ep_len_mean          | 482           |
|    ep_rew_mean          | 2.98e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 465           |
|    time_elapsed         | 6034          |
|    total_timesteps      | 4761600       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10234         |
|    greater_than_0.5     | 10068         |
|    mean_motor0          | 0.42304835    |
|    mean_motor1          | 0.3642296     |
|    mean_motor2          | 0.5740991     |
|    mean_motor3          | 0.45824924    |
|    mean_motor4          | 0.41435534    |
|    mean_motor5          | 0.49895206    |
|    mean_motor6          | 0.40101427    |
|    mean_motor7          | 0.45174152    |
| train/                  |               |
|    approx_kl            | 0.04113865    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.106         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.875         |
|    cost_value_loss      | 1.26e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.802         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0285        |
|    mean_cost_advantages | -0.0006885478 |
|    mean_reward_advan... | -0.010450902  |
|    n_updates            | 9280          |
|    nu                   | 18.4          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000959     |
|    reward_explained_... | 0.737         |
|    reward_value_loss    | 0.0215        |
|    std                  | 0.223         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.6e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.84           |
|    forward_reward       | 0.263          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.05          |
|    reward_forward       | 0.263          |
|    reward_survive       | 1              |
|    x_position           | -0.323         |
|    x_velocity           | 0.263          |
|    y_position           | 5.8            |
|    y_velocity           | 0.694          |
| rollout/                |                |
|    adjusted_reward      | 5.87           |
|    ep_len_mean          | 491            |
|    ep_rew_mean          | 2.97e+03       |
| time/                   |                |
|    fps                  | 788            |
|    iterations           | 466            |
|    time_elapsed         | 6048           |
|    total_timesteps      | 4771840        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10240          |
|    greater_than_0.5     | 10029          |
|    mean_motor0          | 0.40187043     |
|    mean_motor1          | 0.3757228      |
|    mean_motor2          | 0.56470335     |
|    mean_motor3          | 0.4575153      |
|    mean_motor4          | 0.41217828     |
|    mean_motor5          | 0.4773981      |
|    mean_motor6          | 0.42373365     |
|    mean_motor7          | 0.45034543     |
| train/                  |                |
|    approx_kl            | 0.038761754    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0995         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.756          |
|    cost_value_loss      | 1.33e-05       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.829          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00518        |
|    mean_cost_advantages | -0.00053215056 |
|    mean_reward_advan... | -0.0054811025  |
|    n_updates            | 9300           |
|    nu                   | 18.4           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000993      |
|    reward_explained_... | 0.816          |
|    reward_value_loss    | 0.0192         |
|    std                  | 0.222          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 499          |
|    mean_reward          | 2.89e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.61         |
|    forward_reward       | 0.138        |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.958       |
|    reward_forward       | 0.138        |
|    reward_survive       | 1            |
|    x_position           | -0.705       |
|    x_velocity           | 0.138        |
|    y_position           | 6.56         |
|    y_velocity           | 0.253        |
| rollout/                |              |
|    adjusted_reward      | 6.32         |
|    ep_len_mean          | 491          |
|    ep_rew_mean          | 2.98e+03     |
| time/                   |              |
|    fps                  | 788          |
|    iterations           | 467          |
|    time_elapsed         | 6061         |
|    total_timesteps      | 4782080      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10236        |
|    greater_than_0.5     | 9949         |
|    mean_motor0          | 0.41136187   |
|    mean_motor1          | 0.38093156   |
|    mean_motor2          | 0.54961      |
|    mean_motor3          | 0.46203047   |
|    mean_motor4          | 0.37798256   |
|    mean_motor5          | 0.48051804   |
|    mean_motor6          | 0.3688591    |
|    mean_motor7          | 0.43785748   |
| train/                  |              |
|    approx_kl            | 0.14119647   |
|    average_cost         | 0.005566406  |
|    clip_fraction        | 0.232        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.755        |
|    cost_value_loss      | 0.0182       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.855        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.00662      |
|    mean_cost_advantages | 0.0039003077 |
|    mean_reward_advan... | -0.008150334 |
|    n_updates            | 9320         |
|    nu                   | 18.5         |
|    nu_loss              | -0.103       |
|    policy_gradient_loss | -0.00633     |
|    reward_explained_... | 0.83         |
|    reward_value_loss    | 0.0181       |
|    std                  | 0.221        |
|    total_cost           | 57.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 490           |
|    mean_reward          | 2.59e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.65          |
|    forward_reward       | 0.118         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.968        |
|    reward_forward       | 0.118         |
|    reward_survive       | 1             |
|    x_position           | -0.397        |
|    x_velocity           | 0.118         |
|    y_position           | 5.59          |
|    y_velocity           | 0.257         |
| rollout/                |               |
|    adjusted_reward      | 5.99          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.98e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 468           |
|    time_elapsed         | 6075          |
|    total_timesteps      | 4792320       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 9894          |
|    mean_motor0          | 0.3876789     |
|    mean_motor1          | 0.39464313    |
|    mean_motor2          | 0.52076334    |
|    mean_motor3          | 0.4571022     |
|    mean_motor4          | 0.37309062    |
|    mean_motor5          | 0.44028282    |
|    mean_motor6          | 0.37302858    |
|    mean_motor7          | 0.45515117    |
| train/                  |               |
|    approx_kl            | 0.050777763   |
|    average_cost         | 0.00048828125 |
|    clip_fraction        | 0.141         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.74          |
|    cost_value_loss      | 0.00281       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.893         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00868       |
|    mean_cost_advantages | -0.001612834  |
|    mean_reward_advan... | 0.0014754424  |
|    n_updates            | 9340          |
|    nu                   | 18.5          |
|    nu_loss              | -0.00902      |
|    policy_gradient_loss | -0.00181      |
|    reward_explained_... | 0.755         |
|    reward_value_loss    | 0.0162        |
|    std                  | 0.22          |
|    total_cost           | 5.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.01e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.08          |
|    forward_reward       | 0.12          |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.962        |
|    reward_forward       | 0.12          |
|    reward_survive       | 1             |
|    x_position           | -0.495        |
|    x_velocity           | 0.12          |
|    y_position           | 6.02          |
|    y_velocity           | 0.287         |
| rollout/                |               |
|    adjusted_reward      | 6.12          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 3.01e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 469           |
|    time_elapsed         | 6088          |
|    total_timesteps      | 4802560       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 9899          |
|    mean_motor0          | 0.39533776    |
|    mean_motor1          | 0.40709248    |
|    mean_motor2          | 0.52367467    |
|    mean_motor3          | 0.46598393    |
|    mean_motor4          | 0.36403722    |
|    mean_motor5          | 0.45403987    |
|    mean_motor6          | 0.35840526    |
|    mean_motor7          | 0.4403878     |
| train/                  |               |
|    approx_kl            | 0.028293367   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0902        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.817         |
|    cost_value_loss      | 1.32e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.918         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0222        |
|    mean_cost_advantages | 3.1145806e-05 |
|    mean_reward_advan... | -0.015822148  |
|    n_updates            | 9360          |
|    nu                   | 18.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000679     |
|    reward_explained_... | 0.536         |
|    reward_value_loss    | 0.0228        |
|    std                  | 0.22          |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.01e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.51         |
|    forward_reward       | 0.0914       |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.966       |
|    reward_forward       | 0.0914       |
|    reward_survive       | 1            |
|    x_position           | -0.666       |
|    x_velocity           | 0.0914       |
|    y_position           | 6.45         |
|    y_velocity           | 0.199        |
| rollout/                |              |
|    adjusted_reward      | 5.69         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 3.02e+03     |
| time/                   |              |
|    fps                  | 788          |
|    iterations           | 470          |
|    time_elapsed         | 6102         |
|    total_timesteps      | 4812800      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10235        |
|    greater_than_0.5     | 9976         |
|    mean_motor0          | 0.42731458   |
|    mean_motor1          | 0.42399603   |
|    mean_motor2          | 0.55799913   |
|    mean_motor3          | 0.4855075    |
|    mean_motor4          | 0.39863563   |
|    mean_motor5          | 0.39835072   |
|    mean_motor6          | 0.38749567   |
|    mean_motor7          | 0.48737058   |
| train/                  |              |
|    approx_kl            | 0.33802417   |
|    average_cost         | 0.012011719  |
|    clip_fraction        | 0.394        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.715        |
|    cost_value_loss      | 0.0219       |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 0.938        |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0138       |
|    mean_cost_advantages | 0.014533061  |
|    mean_reward_advan... | -0.009920821 |
|    n_updates            | 9380         |
|    nu                   | 18.5         |
|    nu_loss              | -0.222       |
|    policy_gradient_loss | -0.00778     |
|    reward_explained_... | 0.617        |
|    reward_value_loss    | 0.0205       |
|    std                  | 0.219        |
|    total_cost           | 123.0        |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.08e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.77          |
|    forward_reward       | 0.0938        |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.861        |
|    reward_forward       | 0.0938        |
|    reward_survive       | 1             |
|    x_position           | -1.01         |
|    x_velocity           | 0.0938        |
|    y_position           | 6.66          |
|    y_velocity           | 0.184         |
| rollout/                |               |
|    adjusted_reward      | 6.04          |
|    ep_len_mean          | 496           |
|    ep_rew_mean          | 2.99e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 471           |
|    time_elapsed         | 6115          |
|    total_timesteps      | 4823040       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10239         |
|    greater_than_0.5     | 9866          |
|    mean_motor0          | 0.415027      |
|    mean_motor1          | 0.40043694    |
|    mean_motor2          | 0.51523054    |
|    mean_motor3          | 0.46061307    |
|    mean_motor4          | 0.37334245    |
|    mean_motor5          | 0.4023717     |
|    mean_motor6          | 0.33011892    |
|    mean_motor7          | 0.4800797     |
| train/                  |               |
|    approx_kl            | 0.047124296   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0842        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.949         |
|    cost_value_loss      | 5.71e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.959         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0102        |
|    mean_cost_advantages | -0.0010569046 |
|    mean_reward_advan... | -0.022568867  |
|    n_updates            | 9400          |
|    nu                   | 18.5          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000611     |
|    reward_explained_... | 0.601         |
|    reward_value_loss    | 0.023         |
|    std                  | 0.219         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.04e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 5.84           |
|    forward_reward       | 0.13           |
|    reward_contact       | 0              |
|    reward_ctrl          | -0.913         |
|    reward_forward       | 0.13           |
|    reward_survive       | 1              |
|    x_position           | -0.971         |
|    x_velocity           | 0.13           |
|    y_position           | 5.74           |
|    y_velocity           | 0.474          |
| rollout/                |                |
|    adjusted_reward      | 5.94           |
|    ep_len_mean          | 491            |
|    ep_rew_mean          | 2.93e+03       |
| time/                   |                |
|    fps                  | 788            |
|    iterations           | 472            |
|    time_elapsed         | 6129           |
|    total_timesteps      | 4833280        |
| torque/                 |                |
|    greater_than_0.25    | 10240          |
|    greater_than_0.3     | 10236          |
|    greater_than_0.5     | 9877           |
|    mean_motor0          | 0.40990576     |
|    mean_motor1          | 0.3998231      |
|    mean_motor2          | 0.5139335      |
|    mean_motor3          | 0.47240725     |
|    mean_motor4          | 0.37118894     |
|    mean_motor5          | 0.40062267     |
|    mean_motor6          | 0.32555196     |
|    mean_motor7          | 0.4858728      |
| train/                  |                |
|    approx_kl            | 0.037607957    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0998         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.912          |
|    cost_value_loss      | 0.000188       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 0.971          |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00645        |
|    mean_cost_advantages | -0.00080305367 |
|    mean_reward_advan... | -0.015572667   |
|    n_updates            | 9420           |
|    nu                   | 18.6           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00104       |
|    reward_explained_... | 0.521          |
|    reward_value_loss    | 0.0231         |
|    std                  | 0.218          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.93e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.25          |
|    forward_reward       | 0.237         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.952        |
|    reward_forward       | 0.237         |
|    reward_survive       | 1             |
|    x_position           | -0.868        |
|    x_velocity           | 0.237         |
|    y_position           | 5.11          |
|    y_velocity           | 0.499         |
| rollout/                |               |
|    adjusted_reward      | 5.9           |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.92e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 473           |
|    time_elapsed         | 6142          |
|    total_timesteps      | 4843520       |
| torque/                 |               |
|    greater_than_0.25    | 10238         |
|    greater_than_0.3     | 10236         |
|    greater_than_0.5     | 9908          |
|    mean_motor0          | 0.41715974    |
|    mean_motor1          | 0.3916119     |
|    mean_motor2          | 0.5112168     |
|    mean_motor3          | 0.47580576    |
|    mean_motor4          | 0.36578426    |
|    mean_motor5          | 0.40834087    |
|    mean_motor6          | 0.31425872    |
|    mean_motor7          | 0.5051874     |
| train/                  |               |
|    approx_kl            | 0.031313173   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0951        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.802         |
|    cost_value_loss      | 0.000249      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.979         |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0078        |
|    mean_cost_advantages | -0.0005120804 |
|    mean_reward_advan... | -0.018618613  |
|    n_updates            | 9440          |
|    nu                   | 18.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000722     |
|    reward_explained_... | 0.516         |
|    reward_value_loss    | 0.0216        |
|    std                  | 0.218         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.97e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.55          |
|    forward_reward       | 0.111         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.947        |
|    reward_forward       | 0.111         |
|    reward_survive       | 1             |
|    x_position           | -0.525        |
|    x_velocity           | 0.111         |
|    y_position           | 6.5           |
|    y_velocity           | 0.358         |
| rollout/                |               |
|    adjusted_reward      | 6.07          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.91e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 474           |
|    time_elapsed         | 6156          |
|    total_timesteps      | 4853760       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10234         |
|    greater_than_0.5     | 9774          |
|    mean_motor0          | 0.39663538    |
|    mean_motor1          | 0.38567144    |
|    mean_motor2          | 0.49988875    |
|    mean_motor3          | 0.46464032    |
|    mean_motor4          | 0.3600189     |
|    mean_motor5          | 0.40190253    |
|    mean_motor6          | 0.32916206    |
|    mean_motor7          | 0.48320955    |
| train/                  |               |
|    approx_kl            | 0.043412447   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.112         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.926         |
|    cost_value_loss      | 4.18e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 0.99          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.000796      |
|    mean_cost_advantages | -0.0005141529 |
|    mean_reward_advan... | -0.016883072  |
|    n_updates            | 9460          |
|    nu                   | 18.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000739     |
|    reward_explained_... | 0.544         |
|    reward_value_loss    | 0.0213        |
|    std                  | 0.218         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.02e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.51          |
|    forward_reward       | 0.15          |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.884        |
|    reward_forward       | 0.15          |
|    reward_survive       | 1             |
|    x_position           | -0.73         |
|    x_velocity           | 0.15          |
|    y_position           | 5.43          |
|    y_velocity           | 0.528         |
| rollout/                |               |
|    adjusted_reward      | 5.96          |
|    ep_len_mean          | 491           |
|    ep_rew_mean          | 2.94e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 475           |
|    time_elapsed         | 6169          |
|    total_timesteps      | 4864000       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10237         |
|    greater_than_0.5     | 9745          |
|    mean_motor0          | 0.39739954    |
|    mean_motor1          | 0.38729936    |
|    mean_motor2          | 0.49278754    |
|    mean_motor3          | 0.46050796    |
|    mean_motor4          | 0.36102587    |
|    mean_motor5          | 0.4047328     |
|    mean_motor6          | 0.33924976    |
|    mean_motor7          | 0.4877879     |
| train/                  |               |
|    approx_kl            | 0.028332125   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.091         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.854         |
|    cost_value_loss      | 3.36e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.01          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00777       |
|    mean_cost_advantages | -0.0003877547 |
|    mean_reward_advan... | -0.0108557325 |
|    n_updates            | 9480          |
|    nu                   | 18.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000786     |
|    reward_explained_... | 0.466         |
|    reward_value_loss    | 0.02          |
|    std                  | 0.217         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.91e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.2           |
|    forward_reward       | 0.0846        |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.825        |
|    reward_forward       | 0.0846        |
|    reward_survive       | 1             |
|    x_position           | -0.547        |
|    x_velocity           | 0.0846        |
|    y_position           | 6.07          |
|    y_velocity           | 0.32          |
| rollout/                |               |
|    adjusted_reward      | 6.09          |
|    ep_len_mean          | 495           |
|    ep_rew_mean          | 2.97e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 476           |
|    time_elapsed         | 6183          |
|    total_timesteps      | 4874240       |
| torque/                 |               |
|    greater_than_0.25    | 10240         |
|    greater_than_0.3     | 10233         |
|    greater_than_0.5     | 9697          |
|    mean_motor0          | 0.41056213    |
|    mean_motor1          | 0.36609718    |
|    mean_motor2          | 0.4717527     |
|    mean_motor3          | 0.44439477    |
|    mean_motor4          | 0.3580988     |
|    mean_motor5          | 0.4112815     |
|    mean_motor6          | 0.3264946     |
|    mean_motor7          | 0.46160793    |
| train/                  |               |
|    approx_kl            | 0.045901496   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.102         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.87          |
|    cost_value_loss      | 6.03e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.03          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0126        |
|    mean_cost_advantages | -0.0002194563 |
|    mean_reward_advan... | -0.012361026  |
|    n_updates            | 9500          |
|    nu                   | 18.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000755     |
|    reward_explained_... | 0.609         |
|    reward_value_loss    | 0.0215        |
|    std                  | 0.217         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.9e+03       |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.32          |
|    forward_reward       | 0.0668        |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.755        |
|    reward_forward       | 0.0668        |
|    reward_survive       | 1             |
|    x_position           | -1.13         |
|    x_velocity           | 0.0668        |
|    y_position           | 6.18          |
|    y_velocity           | 0.134         |
| rollout/                |               |
|    adjusted_reward      | 5.8           |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.98e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 477           |
|    time_elapsed         | 6196          |
|    total_timesteps      | 4884480       |
| torque/                 |               |
|    greater_than_0.25    | 10239         |
|    greater_than_0.3     | 10232         |
|    greater_than_0.5     | 9598          |
|    mean_motor0          | 0.4015945     |
|    mean_motor1          | 0.37018186    |
|    mean_motor2          | 0.45659465    |
|    mean_motor3          | 0.4567814     |
|    mean_motor4          | 0.3669197     |
|    mean_motor5          | 0.37984115    |
|    mean_motor6          | 0.3232223     |
|    mean_motor7          | 0.46644288    |
| train/                  |               |
|    approx_kl            | 0.036635544   |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0925        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.917         |
|    cost_value_loss      | 8.96e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.04          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00223       |
|    mean_cost_advantages | -0.0002927244 |
|    mean_reward_advan... | -0.009024797  |
|    n_updates            | 9520          |
|    nu                   | 18.6          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.00063      |
|    reward_explained_... | 0.569         |
|    reward_value_loss    | 0.0201        |
|    std                  | 0.216         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.91e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.32           |
|    forward_reward       | 0.0569         |
|    reward_contact       | 0              |
|    reward_ctrl          | -0.823         |
|    reward_forward       | 0.0569         |
|    reward_survive       | 1              |
|    x_position           | -0.428         |
|    x_velocity           | 0.0569         |
|    y_position           | 6.27           |
|    y_velocity           | 0.119          |
| rollout/                |                |
|    adjusted_reward      | 5.65           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.97e+03       |
| time/                   |                |
|    fps                  | 788            |
|    iterations           | 478            |
|    time_elapsed         | 6210           |
|    total_timesteps      | 4894720        |
| torque/                 |                |
|    greater_than_0.25    | 10238          |
|    greater_than_0.3     | 10230          |
|    greater_than_0.5     | 9633           |
|    mean_motor0          | 0.38667703     |
|    mean_motor1          | 0.3684493      |
|    mean_motor2          | 0.44987994     |
|    mean_motor3          | 0.45383683     |
|    mean_motor4          | 0.38876837     |
|    mean_motor5          | 0.43151313     |
|    mean_motor6          | 0.36024994     |
|    mean_motor7          | 0.4490723      |
| train/                  |                |
|    approx_kl            | 0.040701054    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.106          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.874          |
|    cost_value_loss      | 5.68e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 1.05           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00132        |
|    mean_cost_advantages | -0.00021625504 |
|    mean_reward_advan... | -0.023785766   |
|    n_updates            | 9540           |
|    nu                   | 18.6           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.00067       |
|    reward_explained_... | 0.36           |
|    reward_value_loss    | 0.0234         |
|    std                  | 0.217          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 422           |
|    mean_reward          | 2.73e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 5.16          |
|    forward_reward       | 0.167         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.899        |
|    reward_forward       | 0.167         |
|    reward_survive       | 1             |
|    x_position           | -0.448        |
|    x_velocity           | 0.167         |
|    y_position           | 5.14          |
|    y_velocity           | 0.605         |
| rollout/                |               |
|    adjusted_reward      | 6.31          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.99e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 479           |
|    time_elapsed         | 6223          |
|    total_timesteps      | 4904960       |
| torque/                 |               |
|    greater_than_0.25    | 10237         |
|    greater_than_0.3     | 10229         |
|    greater_than_0.5     | 9714          |
|    mean_motor0          | 0.4622119     |
|    mean_motor1          | 0.3146388     |
|    mean_motor2          | 0.54250956    |
|    mean_motor3          | 0.3033805     |
|    mean_motor4          | 0.48667878    |
|    mean_motor5          | 0.43554276    |
|    mean_motor6          | 0.42809567    |
|    mean_motor7          | 0.45569307    |
| train/                  |               |
|    approx_kl            | 2.0251853     |
|    average_cost         | 0.03652344    |
|    clip_fraction        | 0.744         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.724         |
|    cost_value_loss      | 0.00988       |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.05          |
|    learning_rate        | 3e-05         |
|    loss                 | -0.0266       |
|    mean_cost_advantages | 0.04706434    |
|    mean_reward_advan... | -0.0134313125 |
|    n_updates            | 9560          |
|    nu                   | 18.7          |
|    nu_loss              | -0.681        |
|    policy_gradient_loss | -0.0315       |
|    reward_explained_... | 0.822         |
|    reward_value_loss    | 0.0206        |
|    std                  | 0.216         |
|    total_cost           | 374.0         |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.13e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.57         |
|    forward_reward       | 0.272        |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.893       |
|    reward_forward       | 0.272        |
|    reward_survive       | 1            |
|    x_position           | -0.136       |
|    x_velocity           | 0.272        |
|    y_position           | 6.48         |
|    y_velocity           | 0.605        |
| rollout/                |              |
|    adjusted_reward      | 6.34         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 3.03e+03     |
| time/                   |              |
|    fps                  | 788          |
|    iterations           | 480          |
|    time_elapsed         | 6236         |
|    total_timesteps      | 4915200      |
| torque/                 |              |
|    greater_than_0.25    | 10238        |
|    greater_than_0.3     | 10222        |
|    greater_than_0.5     | 9496         |
|    mean_motor0          | 0.37877148   |
|    mean_motor1          | 0.31928068   |
|    mean_motor2          | 0.52131456   |
|    mean_motor3          | 0.30512023   |
|    mean_motor4          | 0.41629362   |
|    mean_motor5          | 0.35778263   |
|    mean_motor6          | 0.38218555   |
|    mean_motor7          | 0.4627338    |
| train/                  |              |
|    approx_kl            | 0.20488515   |
|    average_cost         | 0.0068359375 |
|    clip_fraction        | 0.282        |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.149        |
|    cost_value_loss      | 0.00537      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 1.07         |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00425     |
|    mean_cost_advantages | 0.007904041  |
|    mean_reward_advan... | 0.034418542  |
|    n_updates            | 9580         |
|    nu                   | 18.7         |
|    nu_loss              | -0.128       |
|    policy_gradient_loss | -0.00458     |
|    reward_explained_... | 0.918        |
|    reward_value_loss    | 0.0112       |
|    std                  | 0.216        |
|    total_cost           | 70.0         |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 3.14e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.77          |
|    forward_reward       | 0.073         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.74         |
|    reward_forward       | 0.073         |
|    reward_survive       | 1             |
|    x_position           | -0.229        |
|    x_velocity           | 0.073         |
|    y_position           | 6.73          |
|    y_velocity           | 0.207         |
| rollout/                |               |
|    adjusted_reward      | 6.15          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 3.04e+03      |
| time/                   |               |
|    fps                  | 788           |
|    iterations           | 481           |
|    time_elapsed         | 6250          |
|    total_timesteps      | 4925440       |
| torque/                 |               |
|    greater_than_0.25    | 10236         |
|    greater_than_0.3     | 10222         |
|    greater_than_0.5     | 9459          |
|    mean_motor0          | 0.37194723    |
|    mean_motor1          | 0.32271296    |
|    mean_motor2          | 0.5134225     |
|    mean_motor3          | 0.3006767     |
|    mean_motor4          | 0.4198408     |
|    mean_motor5          | 0.3593886     |
|    mean_motor6          | 0.3936056     |
|    mean_motor7          | 0.46755123    |
| train/                  |               |
|    approx_kl            | 0.04561221    |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0954        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.769         |
|    cost_value_loss      | 1.11e-05      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.1           |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00135       |
|    mean_cost_advantages | 0.00010997129 |
|    mean_reward_advan... | 0.015259087   |
|    n_updates            | 9600          |
|    nu                   | 18.7          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000982     |
|    reward_explained_... | 0.827         |
|    reward_value_loss    | 0.0131        |
|    std                  | 0.215         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.11e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 6.24         |
|    forward_reward       | 0.11         |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.913       |
|    reward_forward       | 0.11         |
|    reward_survive       | 1            |
|    x_position           | -0.348       |
|    x_velocity           | 0.11         |
|    y_position           | 6.19         |
|    y_velocity           | 0.291        |
| rollout/                |              |
|    adjusted_reward      | 6.13         |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 3.08e+03     |
| time/                   |              |
|    fps                  | 787          |
|    iterations           | 482          |
|    time_elapsed         | 6263         |
|    total_timesteps      | 4935680      |
| torque/                 |              |
|    greater_than_0.25    | 10236        |
|    greater_than_0.3     | 10218        |
|    greater_than_0.5     | 9427         |
|    mean_motor0          | 0.35791907   |
|    mean_motor1          | 0.3279937    |
|    mean_motor2          | 0.50953186   |
|    mean_motor3          | 0.29960412   |
|    mean_motor4          | 0.41274697   |
|    mean_motor5          | 0.34497896   |
|    mean_motor6          | 0.39312857   |
|    mean_motor7          | 0.46564347   |
| train/                  |              |
|    approx_kl            | 0.042501427  |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0968       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.844        |
|    cost_value_loss      | 5.73e-06     |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 1.11         |
|    learning_rate        | 3e-05        |
|    loss                 | -0.00102     |
|    mean_cost_advantages | 7.752936e-05 |
|    mean_reward_advan... | 0.006607593  |
|    n_updates            | 9620         |
|    nu                   | 18.7         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.0009      |
|    reward_explained_... | 0.779        |
|    reward_value_loss    | 0.0155       |
|    std                  | 0.214        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.78e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.25          |
|    forward_reward       | 0.135         |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.922        |
|    reward_forward       | 0.135         |
|    reward_survive       | 1             |
|    x_position           | -0.586        |
|    x_velocity           | 0.135         |
|    y_position           | 6.18          |
|    y_velocity           | 0.405         |
| rollout/                |               |
|    adjusted_reward      | 6.13          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 3.12e+03      |
| time/                   |               |
|    fps                  | 787           |
|    iterations           | 483           |
|    time_elapsed         | 6277          |
|    total_timesteps      | 4945920       |
| torque/                 |               |
|    greater_than_0.25    | 10234         |
|    greater_than_0.3     | 10215         |
|    greater_than_0.5     | 9431          |
|    mean_motor0          | 0.35156164    |
|    mean_motor1          | 0.3344592     |
|    mean_motor2          | 0.4927209     |
|    mean_motor3          | 0.31096572    |
|    mean_motor4          | 0.40187836    |
|    mean_motor5          | 0.3484124     |
|    mean_motor6          | 0.38809827    |
|    mean_motor7          | 0.47266197    |
| train/                  |               |
|    approx_kl            | 0.0424835     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.0858        |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.882         |
|    cost_value_loss      | 3.38e-06      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.13          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.00408       |
|    mean_cost_advantages | 2.7289749e-05 |
|    mean_reward_advan... | -0.0031315282 |
|    n_updates            | 9640          |
|    nu                   | 18.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000764     |
|    reward_explained_... | 0.546         |
|    reward_value_loss    | 0.0178        |
|    std                  | 0.214         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.01e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.06           |
|    forward_reward       | 0.178          |
|    reward_contact       | 0              |
|    reward_ctrl          | -0.878         |
|    reward_forward       | 0.178          |
|    reward_survive       | 1              |
|    x_position           | 0.0664         |
|    x_velocity           | 0.178          |
|    y_position           | 6              |
|    y_velocity           | 0.515          |
| rollout/                |                |
|    adjusted_reward      | 6.2            |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 3.09e+03       |
| time/                   |                |
|    fps                  | 787            |
|    iterations           | 484            |
|    time_elapsed         | 6290           |
|    total_timesteps      | 4956160        |
| torque/                 |                |
|    greater_than_0.25    | 10236          |
|    greater_than_0.3     | 10222          |
|    greater_than_0.5     | 9458           |
|    mean_motor0          | 0.35150963     |
|    mean_motor1          | 0.32008424     |
|    mean_motor2          | 0.4948131      |
|    mean_motor3          | 0.29195192     |
|    mean_motor4          | 0.42427167     |
|    mean_motor5          | 0.34895974     |
|    mean_motor6          | 0.39829943     |
|    mean_motor7          | 0.46085173     |
| train/                  |                |
|    approx_kl            | 0.049698718    |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.113          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.854          |
|    cost_value_loss      | 2.31e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 1.16           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0113         |
|    mean_cost_advantages | -8.1041835e-05 |
|    mean_reward_advan... | 0.00037910364  |
|    n_updates            | 9660           |
|    nu                   | 18.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000857      |
|    reward_explained_... | 0.577          |
|    reward_value_loss    | 0.0184         |
|    std                  | 0.213          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
------------------------------------------
| eval/                   |              |
|    best_mean_reward     | 5e+03        |
|    mean_ep_length       | 500          |
|    mean_reward          | 3.04e+03     |
| infos/                  |              |
|    cost                 | 0            |
|    distance_from_origin | 5.33         |
|    forward_reward       | 0.117        |
|    reward_contact       | 0            |
|    reward_ctrl          | -0.972       |
|    reward_forward       | 0.117        |
|    reward_survive       | 1            |
|    x_position           | 0.444        |
|    x_velocity           | 0.117        |
|    y_position           | 5.28         |
|    y_velocity           | 0.197        |
| rollout/                |              |
|    adjusted_reward      | 6            |
|    ep_len_mean          | 500          |
|    ep_rew_mean          | 3.06e+03     |
| time/                   |              |
|    fps                  | 787          |
|    iterations           | 485          |
|    time_elapsed         | 6303         |
|    total_timesteps      | 4966400      |
| torque/                 |              |
|    greater_than_0.25    | 10239        |
|    greater_than_0.3     | 10211        |
|    greater_than_0.5     | 9381         |
|    mean_motor0          | 0.33935466   |
|    mean_motor1          | 0.34122223   |
|    mean_motor2          | 0.48882222   |
|    mean_motor3          | 0.30728883   |
|    mean_motor4          | 0.40437597   |
|    mean_motor5          | 0.34000635   |
|    mean_motor6          | 0.3699206    |
|    mean_motor7          | 0.48949176   |
| train/                  |              |
|    approx_kl            | 0.03887988   |
|    average_cost         | 0.0          |
|    clip_fraction        | 0.0971       |
|    clip_range           | 0.4          |
|    cost_explained_va... | 0.853        |
|    cost_value_loss      | 2.7e-06      |
|    early_stop_epoch     | 20           |
|    entropy_loss         | 1.17         |
|    learning_rate        | 3e-05        |
|    loss                 | 0.0121       |
|    mean_cost_advantages | 5.329867e-05 |
|    mean_reward_advan... | 0.002376269  |
|    n_updates            | 9680         |
|    nu                   | 18.8         |
|    nu_loss              | -0           |
|    policy_gradient_loss | -0.000725    |
|    reward_explained_... | 0.644        |
|    reward_value_loss    | 0.016        |
|    std                  | 0.213        |
|    total_cost           | 0.0          |
------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 2.93e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.52           |
|    forward_reward       | 0.109          |
|    reward_contact       | 0              |
|    reward_ctrl          | -0.936         |
|    reward_forward       | 0.109          |
|    reward_survive       | 1              |
|    x_position           | -0.0743        |
|    x_velocity           | 0.109          |
|    y_position           | 4.48           |
|    y_velocity           | 0.325          |
| rollout/                |                |
|    adjusted_reward      | 5.85           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 3.04e+03       |
| time/                   |                |
|    fps                  | 787            |
|    iterations           | 486            |
|    time_elapsed         | 6317           |
|    total_timesteps      | 4976640        |
| torque/                 |                |
|    greater_than_0.25    | 10233          |
|    greater_than_0.3     | 10213          |
|    greater_than_0.5     | 9342           |
|    mean_motor0          | 0.33842698     |
|    mean_motor1          | 0.35324094     |
|    mean_motor2          | 0.47524446     |
|    mean_motor3          | 0.32290164     |
|    mean_motor4          | 0.39694148     |
|    mean_motor5          | 0.3389849      |
|    mean_motor6          | 0.355074       |
|    mean_motor7          | 0.48713365     |
| train/                  |                |
|    approx_kl            | 0.04055957     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.102          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.868          |
|    cost_value_loss      | 1.22e-06       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 1.17           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.0122         |
|    mean_cost_advantages | -9.1738635e-05 |
|    mean_reward_advan... | -0.0094113145  |
|    n_updates            | 9700           |
|    nu                   | 18.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000777      |
|    reward_explained_... | 0.491          |
|    reward_value_loss    | 0.0196         |
|    std                  | 0.213          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
-------------------------------------------
| eval/                   |               |
|    best_mean_reward     | 5e+03         |
|    mean_ep_length       | 500           |
|    mean_reward          | 2.96e+03      |
| infos/                  |               |
|    cost                 | 0             |
|    distance_from_origin | 6.25          |
|    forward_reward       | 0.0838        |
|    reward_contact       | 0             |
|    reward_ctrl          | -0.84         |
|    reward_forward       | 0.0838        |
|    reward_survive       | 1             |
|    x_position           | 0.102         |
|    x_velocity           | 0.0838        |
|    y_position           | 6.23          |
|    y_velocity           | 0.226         |
| rollout/                |               |
|    adjusted_reward      | 5.65          |
|    ep_len_mean          | 500           |
|    ep_rew_mean          | 2.98e+03      |
| time/                   |               |
|    fps                  | 787           |
|    iterations           | 487           |
|    time_elapsed         | 6330          |
|    total_timesteps      | 4986880       |
| torque/                 |               |
|    greater_than_0.25    | 10235         |
|    greater_than_0.3     | 10218         |
|    greater_than_0.5     | 9323          |
|    mean_motor0          | 0.34837982    |
|    mean_motor1          | 0.3664376     |
|    mean_motor2          | 0.4603629     |
|    mean_motor3          | 0.32873255    |
|    mean_motor4          | 0.39926922    |
|    mean_motor5          | 0.33394524    |
|    mean_motor6          | 0.36502755    |
|    mean_motor7          | 0.488025      |
| train/                  |               |
|    approx_kl            | 0.0367432     |
|    average_cost         | 0.0           |
|    clip_fraction        | 0.102         |
|    clip_range           | 0.4           |
|    cost_explained_va... | 0.933         |
|    cost_value_loss      | 9.89e-07      |
|    early_stop_epoch     | 20            |
|    entropy_loss         | 1.19          |
|    learning_rate        | 3e-05         |
|    loss                 | 0.0074        |
|    mean_cost_advantages | -5.207041e-05 |
|    mean_reward_advan... | -0.014774901  |
|    n_updates            | 9720          |
|    nu                   | 18.8          |
|    nu_loss              | -0            |
|    policy_gradient_loss | -0.000747     |
|    reward_explained_... | 0.53          |
|    reward_value_loss    | 0.0203        |
|    std                  | 0.212         |
|    total_cost           | 0.0           |
-------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.02e+03       |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 6.96           |
|    forward_reward       | 0.0774         |
|    reward_contact       | 0              |
|    reward_ctrl          | -0.748         |
|    reward_forward       | 0.0774         |
|    reward_survive       | 1              |
|    x_position           | -0.134         |
|    x_velocity           | 0.0774         |
|    y_position           | 6.93           |
|    y_velocity           | 0.148          |
| rollout/                |                |
|    adjusted_reward      | 6.02           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.97e+03       |
| time/                   |                |
|    fps                  | 787            |
|    iterations           | 488            |
|    time_elapsed         | 6344           |
|    total_timesteps      | 4997120        |
| torque/                 |                |
|    greater_than_0.25    | 10236          |
|    greater_than_0.3     | 10215          |
|    greater_than_0.5     | 9334           |
|    mean_motor0          | 0.3227787      |
|    mean_motor1          | 0.35060853     |
|    mean_motor2          | 0.47666183     |
|    mean_motor3          | 0.33231157     |
|    mean_motor4          | 0.38670632     |
|    mean_motor5          | 0.33608073     |
|    mean_motor6          | 0.35635316     |
|    mean_motor7          | 0.48330912     |
| train/                  |                |
|    approx_kl            | 0.03236256     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.083          |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.956          |
|    cost_value_loss      | 1.5e-06        |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 1.2            |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00448        |
|    mean_cost_advantages | -0.00028327107 |
|    mean_reward_advan... | -0.016906075   |
|    n_updates            | 9740           |
|    nu                   | 18.8           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000571      |
|    reward_explained_... | 0.694          |
|    reward_value_loss    | 0.0193         |
|    std                  | 0.213          |
|    total_cost           | 0.0            |
--------------------------------------------
(8,)
--------------------------------------------
| eval/                   |                |
|    best_mean_reward     | 5e+03          |
|    mean_ep_length       | 500            |
|    mean_reward          | 3.1e+03        |
| infos/                  |                |
|    cost                 | 0              |
|    distance_from_origin | 4.28           |
|    forward_reward       | 0.303          |
|    reward_contact       | 0              |
|    reward_ctrl          | -1.05          |
|    reward_forward       | 0.303          |
|    reward_survive       | 1              |
|    x_position           | -0.271         |
|    x_velocity           | 0.303          |
|    y_position           | 4.23           |
|    y_velocity           | 0.501          |
| rollout/                |                |
|    adjusted_reward      | 5.97           |
|    ep_len_mean          | 500            |
|    ep_rew_mean          | 2.94e+03       |
| time/                   |                |
|    fps                  | 787            |
|    iterations           | 489            |
|    time_elapsed         | 6357           |
|    total_timesteps      | 5007360        |
| torque/                 |                |
|    greater_than_0.25    | 10234          |
|    greater_than_0.3     | 10220          |
|    greater_than_0.5     | 9244           |
|    mean_motor0          | 0.31816667     |
|    mean_motor1          | 0.36007497     |
|    mean_motor2          | 0.4762438      |
|    mean_motor3          | 0.3265471      |
|    mean_motor4          | 0.38632926     |
|    mean_motor5          | 0.3304113      |
|    mean_motor6          | 0.35677797     |
|    mean_motor7          | 0.47509766     |
| train/                  |                |
|    approx_kl            | 0.04081524     |
|    average_cost         | 0.0            |
|    clip_fraction        | 0.0973         |
|    clip_range           | 0.4            |
|    cost_explained_va... | 0.827          |
|    cost_value_loss      | 8.97e-07       |
|    early_stop_epoch     | 20             |
|    entropy_loss         | 1.21           |
|    learning_rate        | 3e-05          |
|    loss                 | 0.00225        |
|    mean_cost_advantages | -0.00013719479 |
|    mean_reward_advan... | -0.00508302    |
|    n_updates            | 9760           |
|    nu                   | 18.9           |
|    nu_loss              | -0             |
|    policy_gradient_loss | -0.000734      |
|    reward_explained_... | 0.522          |
|    reward_value_loss    | 0.0175         |
|    std                  | 0.212          |
|    total_cost           | 0.0            |
--------------------------------------------
Saving video to  /home/vm5/rl_codebase/icrl/wandb/run-20210108_164545-alci5o6e/files/final_policy-step-0-to-step-1500.mp4
Mean reward: 2854.624692 +/- 87.634814.
/home/vm5/rl_codebase/.env/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
[32;1mTime taken: 106.62 minutes[0m
